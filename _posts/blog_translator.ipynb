{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarianMTModel were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-de-en and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Blog-Beiträge automatisch übersetzen <<eol>>', '[:de:](https://github.com/joatom/ai_curious/blob/master/_posts/blog_translator_de.md) <--> [:us:](https://joatom.github.io/ai_curious/markdown/2020/12/26/blog-translator.html) <<eol>>', ' <<eol>>', '> Achtung! Dieser Text wurde automatisch übersetzt! <<eol>>', ' <<eol>>', 'Da ich in meinem ersten [Blog-Beitrag](https://datamuni.com/@joatom/a-handful-of-bricks-from-sql-to-pandas) soviele Fehler auf Englisch gemacht habe, schreibe ich diesen Beitrag auf Deutsch und lasse ihn automatisch übersetzen. <<eol>>', ' <<eol>>', 'Zur Übersetzung verwende ich das populäre NLP-Framework von [huggingface.co](https://huggingface.co/transformers/index.html). Auf ihrer Webseite ist ein einfaches [Beispiel](https://huggingface.co/transformers/model_doc/marian.html) um eine Übersetzungsanwendung zu implementieren. Und diese werde ich verwenden. <<eol>>', ' <<eol>>', 'Wie erwartet, hat es nicht auf Anhieb funktioniert die Markdown-Syntax bei der Übersetzung immer korrekt zu übernehmen. <<eol>>', 'Also habe ich zu Beginn und hinterher noch ein paar Anpassungen vornehmen müssen. <<eol>>', ' <<eol>>', 'Den Code (inkl. Vor- und Nachbearbeitung), den ich für die Übersetzung der Markdown-Dateien verwendet habe, findet ihr [hier](github.com/joatom/blog_ressources.git). <<eol>>', 'Aber da es ja nur ein paar Zeilen Code sind, können wir sie uns auch eben hier anschauen: <<eol>>', ' <<eol>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', '<<code_block>>', ' <<eol>>', 'Da das meine erster NLP-Anwendung ist, habe ich es bei diesem *Hello World*-Code belassen. Sicherlich gibt es geschickter Wege, um die Markdown-Syntax in *tokenizer* abzubilden. Vielleicht schreibe ich ein Follow up, wenn ich es rausgefunden habe. <<eol>>', ' <<eol>>', 'Übrigens, die Übersetzung hat mich ebenfallls dazu gebracht meinen deutschen Schreibstil anzupassen. <<eol>>', 'Beispielsweise funktioniert Sarkasmus nach der Übersetzung nicht so gut, also hab ich es vermieden. <<eol>>', 'Außerdem kommt es auch häufig auf die richtige Wortwahl an (z.B. gibt es kein Markdown-Befehl, jedoch gibt es Markdown-Syntax).']\n",
      "['# Automatically translate blog posts ', '[:de:](https://github.com/joatom/ai_curious/blob/master/_posts/blog_translator_en.md) <--> [:us:](https://joatom.github.io/ai_curious/markdown/2020/12/26/blog-translator.html) ', '', '> Attention! This text has been automatically translated! ', '', 'Since I made so many mistakes in my first [Blog post](https://datamuni.com/@joatom/a-handful-of-bricks-from-sql-to-pandas), I write this post in German and have it automatically translated. ', '', 'For translation I use the popular NLP framework of [huggingface.co](https://huggingface.co/transformers/index.html). On their website is a simple [example](https://huggingface.co/transformers/model_doc/marian.html) to implement a translation application and I will use it. ', '', 'As expected, the Markdown syntax does not immediately work correctly when translating. ', 'So I had to make some adjustments at the beginning and afterwards. ', '', 'The code (including pre- and post-processing) that I used to translate the markdown files can be found [here](github.com/joatom/blog_ressources.git). ', \"But since it's just a few lines of code, we can also look at it here: \", '', '```python ', 'from transformers import MarianMTModel, MarianTokenizer ', ' ', '# load pretrained model and tokenizer ', \"model_name = 'Helsinki-NLP/opus-mt-de-en' \", 'tokenizer = MarianTokenizer.from_pretrained(model_name) ', 'model = MarianMTModel.from_pretrained(model_name) ', ' ', '# load german block post ', 'f_in = open(\"blog_translator_de.md\", \"r\") ', 'src_text = f_in.readlines() ', 'f_in.close() ', ' ', '# preprocessing ', '## line break (\\\\n) results to \"I don\\'t know.\"  We make it more specific: ', \"src_text = [s.replace('\\\\n',' ') for s in src_text] \", ' ', '## remove code block ', 'code = [] ', 'inside_code_block = False ', 'for i, line in enumerate(src_text): ', \"    if line.startswith('```') and not inside_code_block: \", '        # entering codeblock ', '        inside_code_block = True ', '        code += [line] ', \"        src_text[i] = '<<code_block>>' \", \"    elif inside_code_block and not line.startswith('```'): \", '        code += [line] ', \"        src_text[i] = '<<code_block>>' \", \"    elif inside_code_block and line.startswith('```'): \", '        # leaving code block ', '        code += [line] ', \"        src_text[i] = '<<code_block>>' \", '        inside_code_block = False ', ' ', '# translate ', 'translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\")) ', 'tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] ', ' ', '# postprocessing ', '## replace code_blog tags with code ', 'for i, line in enumerate(tgt_text): ', \"    if line == '<<code_block>>': \", '        tgt_text[i] = code.pop(0) ', ' ', '## remove the eol (but keep empty list entries / lines) ', \"tgt_text = [s.replace('', '',) for s in tgt_text] \", '## remove space between ]( to get the md link syntax right ', \"tgt_text = [s.replace('](', '](',) for s in tgt_text] \", ' ', '# write english blog post ', \"with open('2020-12-26-blog-translator.md', 'w') as f_out: \", '    for line in tgt_text: ', '        f_out.write(\"%s\\\\n\" % line) ', 'f_out.close() ', '``` ', '', \"Since this is my first NLP application, I left it with this *Hello World* code. Surely there are clever ways to map the markdown syntax in *tokenizer*. Maybe I'll write a follow up when I find out. \", '', 'By the way, the translation just made me adapt my German writing style. ', \"For example, sarcasm doesn't work so well after translation, so I avoided it. \", 'In addition, it often depends on the correct choice of words (e.g. there is no markdown command, but there is a markdown syntax).']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# load pretrained model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# load german block post\n",
    "f_in = open(\"blog_translator_de.md\", \"r\")\n",
    "src_text = f_in.readlines()\n",
    "f_in.close()\n",
    "\n",
    "# preprocessing\n",
    "## line break (\\n) results to \"I don't know.\"  We make it more specific:\n",
    "src_text = [s.replace('\\n',' <<eol>>') for s in src_text]\n",
    "\n",
    "## remove code block\n",
    "code = []\n",
    "inside_code_block = False\n",
    "for i, line in enumerate(src_text):\n",
    "    if line.startswith('```') and not inside_code_block:\n",
    "        # entering codeblock\n",
    "        inside_code_block = True\n",
    "        code += [line]\n",
    "        src_text[i] = '<<code_block>>'\n",
    "    elif inside_code_block and not line.startswith('```'):\n",
    "        code += [line]\n",
    "        src_text[i] = '<<code_block>>'\n",
    "    elif inside_code_block and line.startswith('```'):\n",
    "        # leaving code block\n",
    "        code += [line]\n",
    "        src_text[i] = '<<code_block>>'\n",
    "        inside_code_block = False\n",
    "\n",
    "# translate\n",
    "translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# postprocessing\n",
    "## replace code_blog tags with code\n",
    "for i, line in enumerate(tgt_text):\n",
    "    if line == '<<code_block>>':\n",
    "        tgt_text[i] = code.pop(0)\n",
    "\n",
    "## remove the eol (but keep empty list entries / lines)\n",
    "tgt_text = [s.replace('<<eol>>', '',) for s in tgt_text]\n",
    "## remove space between ] ( to get the md link syntax right\n",
    "tgt_text = [s.replace('] (', '](',) for s in tgt_text]\n",
    "\n",
    "# write english blog post\n",
    "with open('2020-12-26-blog-translator.md', 'w') as f_out:\n",
    "    for line in tgt_text:\n",
    "        f_out.write(\"%s\\n\" % line)\n",
    "f_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sdf1', ['sdf2', 'sdf3'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['sdf1','sdf2']\n",
    "a += ['sdf3']\n",
    "\n",
    "a.pop(0) ,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
