{
  
    
        "post0": {
            "title": "Sterbefälle nach Altersgruppen und Bundesländern in Deutschland von 2016 bis 2021",
            "content": ":de: :us: [kaggle version] . In diesem Blog-Post möchte ich herausfinden, wie leicht sich interaktive Grafiken in Notebooks erstellen lassen. Als Framework verwende ich hier Altair, da es sich in meinem Blog leicht integrieren lässt [FP20]. . Um den Beitrag auch inhaltlich interessant zu gestallten, werte ich die Sterbefallzahlen von Deutschland der letzten fünf Jahre aus. Dabei Grenze ich die Zeiträume vor und während Covid19 voneinander ab. . Als Fragestellung definiere ich: . Wie verändern sich die Sterbefallzahlen je Altersgruppe und Monat? | Wie verändern sich die Sterbefallzahlen je Bundesland? | . Im ersten Teil wird die Datenherkunft und -verarbeitung beschrieben. Anschließend werden rund um die obigen Fragestellungen Grafiken aufgebaut. Zum Schluss folgt ein technisches Fazit zum Framework. . Important: Die Sterbefallzahlen sind kein Indikator für die Gefährlichkeit von Covid19. Sie bilden lediglich rückwirkend betrachtet die Anzahl verstorbener Menschen ab. . Datenherkunft . Für die Auswertung von Sterbefällen in Deutschland werden die aktuellen Sterbefalldaten vom Statistischen Bundesamt [SB21] herangezogen. Die Daten beinhalten unter anderem Aufstellungen der Todesfälle nach Altersgruppen oder Bundesländern. In dieser Analyse werden die monatlichen Sterbefallzahlen für den Zeitraum März 2016 bis Februar 2021 herangezogen. Die aktuellsten Daten liegen derzeit nur bis Februar 2021 vor und beinhalten einen Schätzanteil, der in der Datenquelle im Reiter &quot;Hinweise&quot; erklärt ist. Neben den Sterbefallzahlen werden zusätzlich Daten über die Bevölkerungsdichte der Bundesländer verarbeitet [SB20], wobei sich diese Zahlen auf den Stichtag 31.12.2019 beziehen. . Verarbeitung der Daten . Preprocessing . Der Auswertungszeitraum wird beschränkt auf März 2016 bis Februar 2021. Der Zeitraum, in dem Corona in Deutschland sehr aktive war, wird hier vereinfacht auf März 2020 (als Covid19 in Deutschland die ersten größeren gesellschaftlichen Veränderungen auslöste) bis Februar 2021 (orange) festgelegt. Der vor-Covid19-Zeitraum wird auf März 2016 bis Februar 2020 festgelegt (blau). Somit umfasst der Covid19-Zeitraum exakt ein Jahr und der vor-Covid19-Zeitraum exakt 4 Jahre. Somit bleiben beide Zeiträume ohne gravierende saisonale Abweichungen vergleichbar. Die Aufteilung der Zeiträume ist im nachstehenden Diagramm verdeutlicht. . Datenaggregation . Bei den Berechnungen werden die Werte über einen Zeitraum über das Arithmetische Mittel aggregiert. Je nach Auswertung geschieht dies über den ganzen Zeitraum oder je Monat. Das erste und dritte Quartil, der aggregierten Daten, werden gegebenenfalls als Schattierung in den Diagrammen mit abgebildet. In einigen Abbildung werden die berechneten Punkte interpoliert um die Lesbarkeit zu erhöhen. . . Tip: Bei interaktiven Grafiken befinden sich in der rechten oberen Ecke Steuerungselemente, wie Dropdown-Boxen. Mit dem Mausrad lässt sich zoomen und per Doppelklick lässt sich eine Grafik zurücksetzen. . Todesf&#228;lle nach Alter . Im nachstehenden Diagramm wird die durchschnittliche Anzahl an Todesfällen pro Monat je Altersgruppe abgebildet. Eine Altersgruppe umfasst fünf Jahre. Der Punkt Alter 55 umfasst z.B. alle Todesfälle im Alter zwischen 50 und 55. Todesfälle der über 100-jährigen werden im Punkt 100 abgebildet. . Die Werte sind jeweils über den vor-Covid19-Zeitraum (blau) und den Covid19-Zeitraum (orange) aggregiert. Im Diagramm lassen sich die Monate per Dropdown-Box auswählen. . Nachfolgend sind die durchschnittlichen Todesfälle nach Alter für jeden einzelnen Monat aufgelistet. Die Auflistung beginnt mit dem Monat März. . Beobachtung . In den Altersgruppen unter 55 Jahren kam es in diesen Zeiträumen zu keiner Übersterblichkeit. In den Altersgruppen ab 80 Jahren nahm die Übersterblichkeit massiv zu. . Todesf&#228;lle nach Bundesland . Im nächsten Diagramm lassen sich für die Altersgruppen 0-65 und 65+ die Todesfälle für die beiden Zeiträume je Bundesland auswerten. Per Dropdown-Box kann zwischen den Kennzahlen Todesfälle oder Todesfälle je 100.000 Einwohner gewählt werden. . Beobachtung . Nordrhein-Westfalen (NW) hat als Einwohner-stärkstes Land die meisten Todesfälle. In jedem Bundesland sind Anstiege der Todeszahlen im Covid19-Zeitraum zu erkennen. Allerdings sind lediglich minimale Anstiege in Hessen (HE) und Bayern (BY) für die Altersgruppen unter 65 Jahren zu erkennen. In den anderen Bundesländern gibt es keinen merklichen Anstieg in dieser Altersgruppe. . Bei der Darstellung der Todesfällen je Einwohner gibt es in der Altersgruppe unter 65 kleinere Schwankungen. Der Anstieg an Todesfällen in Nordrhein-Westfalen (NW) ist etwas geringer als in Bayern (BY) und in etwa so hoch wie in Baden-Württemberg (BW). Den stärksten Anstieg verzeichnet hier Sachsen (SN) und Brandenburg (BB). . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Todesf&#228;lle nach Bundesland und Bev&#246;lkerungsdichte . Das letzte Diagramm setzt die Bevölkerungsdichte der Bundesländer mit deren Todesfällen je 100.000 Einwohner in Bezug. . Beobachtung . Die Stadtstaaten Hamburg (HH) und Bremen (HB) weisen trotz ihrer hohen Einwohnerdichte einen nur moderaten Anstieg der Sterbefälle aus. Wohingegen Berlin einen hohen Anstieg verzeichnet. Schleswig-Holstein (SH) hat den geringsten Anstieg zu verzeichnen. . Der Korrelationskoeffizient zwischen der Bevölkerungsdichte und Anstieg der Todesfälle beträgt: . 0.0232 . Eine Korrelation ist für diesen Vergleich nicht festzustellen. . . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Technisches Fazit . Mit Altair lassen sich einfach Grafiken gestallten und interaktiv in Notebooks mit einbinden. Die Webseite beinhaltet eine Vielzahl von Beispielen [AA1]. Einige Fragestellungen werden auch im Github-Issue-Tracker [AA2] beantwortet. . Ich habe etwas Zeit mit JavaScript-Debugging verbracht, als ich die Bindings falsch verwendet habe. Es gab aber schlussendlich zu jedem Problem eine Lösung [AA2] oder Workarounds wie etwa bei der Legende im Bardiagramm [AG18]. Wie erwartet braucht es etwas Übung um ein neues Framework wie gewollt einsetzen zu können. . Die Integration [FP20] in diesen Blog funktioniert einwandfrei. Ich werde das Framework bestimmt auch an anderer Stelle verwenden. . Quellverzeichniss . Datenquellen . Die hier verwendeten Daten stammen vom Statistischen Bundesamtes und unterliegen der Lizenz &quot;dl-de/by-2-0&quot;. Der Lizenztext findet sich unter www.govdata.de/dl-de/by-2-0. Die Daten wurden zum Zwecke der Analyse ausschließlich innerhalb dieses Notebooks durch Ausführung des angegebenen Programmcodes modifiziert. . [SB21] Statistisches Bundesamt (Destatis), 2021 (erschienen am 30. März 2021), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, abgerufen 03.04.2021, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (erschienen am 2. September 2020), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, abgerufen am 03.04.2021, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Sonstige Quellen . A lot of the coding is derived from various examples of the Altair homepage and great examples in the coresponding github issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (erschienen am 06. Oktober 2018), Focus: generating an interactive legend in Altair, abgerufen 05.04.2021, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (erschienen am 20. Februar 2020), Fastpages Notebook Blog Post, abgerufen 05.04.2021, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany_de.html",
            "relUrl": "/eda/2021/04/06/mortality-germany_de.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Deaths by age group and states in Germany from 2016 to 2021",
            "content": ":de: :us: [kaggle version] . In this blog post, I want to find out how easy it is to create interactive charts in notebooks. As a framework, I will use Altair, as it can be easily integrated into my blog [FP20]. . In order to make the contribution interesting, I evaluate the death rates from Germany over the last five years. I construct two periods - the first period before and the second one during Covid19. . We will analyze these questions: . How do deaths per age group and month change? | How do deaths per state change? | . The first part describes the origin and processing of the data. Then, charts are built around the above questions. Finally, a technical conclusion to the framework follows. . Important: The death rates are not an indicator of the danger of Covid19. They only reflect the number of people who died retrospectively. . Source of data . For the analysis of deaths in Germany, the current death data from the Statistisches Bundesamt (Federal Statistical Office) [SB21] are used. The data include the number of deaths by age group or state on a monthly basis. In this blog post, the monthly death figures for the period March 2016 to February 2021 are examined. The most recent data are currently only available until February 2021 and include some estimations, which are explained in the data source in the tab &quot;Hinweise&quot;. In addition to the death figures, data on the population density of the states are processed [SB20]. These figures refere to the reference date 31.12.2019. . Procession of the data . Preprocessing . The evaluation period is limited to March 2016 until February 2021. The period in which Corona was heavily active in Germany is simplified here to March 2020 (when Covid19 triggered the first major social changes in Germany) until February 2021 (orange). The pre-Covid19 period is set to March 2016 until February 2020 (blue). Thus, the Covid19 period covers exactly one year and the pre-Covid19 period exactly four years. Thus, both periods remain comparable without serious seasonal deviations. The split of the periods is illustrated in the diagram below. . Aggregation of data . In the calculations, the values are averaged over a period of time. Depending on the evaluation, this happens over the whole period or per month. The first and third quartile of the aggregated data may also be shown as shading in the diagrams. In some charts, the calculated points are interpolated to increase readability. . . Tip: For some of the charts there are control elements in the upper right corner, such as drop-down boxes. The mouse wheel can be used to zoom and a chart can be reset by double-clicking. . Deaths by age . The diagram below shows the average number of deaths per month per age group. One age group covers five years. The point age 55 includes, for example, all deaths between 50 and 55. Deaths of those over 100 years of age are shown in point 100. . The values are aggregated over the pre-Covid19 period (blue) and the Covid19 period (orange). In the diagram, the months can be selected via drop-down box. . Below are the average deaths by age for each month. The list starts with the month of March. . Observation . In the age groups under 55, there was no excess mortality during these periods. In the age groups from 80 years and older, mortality increased massively. . Deaths by state . In the next diagram, the age groups 0-65 and 65+ can be evaluated for the two periods grouped by state. By drop-down box, you can choose between the key figures Deaths or Deaths per 100,000 inhabitants. . Observation . North Rhine-Westphalia (NW) has the largest number of deaths, since it is the state with highest population. In each state, there are increases in death rates in the Covid19 period. However, only minimal increases in Hesse (HE) and Bavaria (BY) can be seen for the age groups under the age of 65. In the other states, there is no noticeable increase in this age group. . There are smaller variations looking at the number of deaths per 100.000 inhabitants in the age group below 65. The increase in deaths in North Rhine-Westphalia (NW) is slightly lower than in Bavaria (BY) and roughly as high as in Baden-Württemberg (BW). Saxony (SN) and Brandenburg (BB) have the strongest increases. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Deaths by state and population density . The last diagram refers to the population density of the states and the amount of deaths per 100,000 inhabitants. . Observation . The city-states Hamburg (HH) and Bremen (HB) show a moderate increase in deaths despite their high population density. Berlin (BE) instead has a high increase in death rate. Schleswig-Holstein (SH) recorded the lowest increase. . The correlation coefficient of population density and increase in mortality is: . 0.0232 . Hence, There is no correlation for this comparison. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Technical conclusion . With Altair you can easily setup interactive charts and integrate them into notebooks. Their website contains a variety of examples [AA1]. Some questions are also answered in their Github Issue tracker [AA2]. . I spent some time debugging JavaScript when I used the bindings incorrectly. However, there was finally a solution to every problem [AA2] or workarounds such as for the legend in the bar diagram [AG18]. As expected, a bit of practice is neccessary to get used to a new framework. . The integration [FP20] in this blog works perfectly. I will certainly use the framework elsewhere. . References . Data sources . The data used here are from the &quot;Statistisches Bundesamt&quot; (Federal Statistical Office) and are subject to the license &quot;dl-de/by-2-0&quot;. The license text can be found at www.govdata.de/dl-de/by-2-0. The data were modified exclusively within this notebook by executing the specified program code for the purpose of analysis. . [SB21] Statistisches Bundesamt (Destatis), 2021 (published 2012/03/30), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, visited 2021/04/03, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (published 2020/09/02), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, visited 2021/04/03, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Other references . A lot of the coding is derived from various examples of the Altair homepage and from great examples in the coresponding Github Issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (published 2018/10/06), Focus: generating an interactive legend in Altair, visited 2021/04/05, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (published 2020/02/20), Fastpages Notebook Blog Post, visited 2021/04/05, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany.html",
            "relUrl": "/eda/2021/04/06/mortality-germany.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Automatically translate blog posts",
            "content": ":de: :us: . Attention! This text has been automatically translated! . Since I made so many mistakes in my first Blog post, I write this post in German and have it automatically translated. . For translation I use the popular NLP framework of huggingface.co. On their website is a simple example to implement a translation application and I will use it. . As expected, the Markdown syntax does not immediately work correctly when translating. So I had to make some adjustments at the beginning and afterwards. . The code (including pre- and post-processing) I used for the translation of the markdown files can be found here. But since it’s just a few lines of code, we can also look at it here: . from transformers import MarianMTModel, MarianTokenizer # load pretrained model and tokenizer model_name = &#39;Helsinki-NLP/opus-mt-de-en&#39; tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) # load german block post f_in = open(&quot;blog_translator_de.md&quot;, &quot;r&quot;) src_text = f_in.readlines() f_in.close() # preprocessing ## line break ( n) results to &quot;I don&#39;t know.&quot; We make it more specific: src_text = [s.replace(&#39; n&#39;,&#39; &#39;) for s in src_text] ## remove code block code = [] inside_code_block = False for i, line in enumerate(src_text): if line.startswith(&#39;&#39;) and not inside_code_block: # entering codeblock inside_code_block = True code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and not line.startswith(&#39;&#39;): code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and line.startswith(&#39;&#39;): # leaving code block code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; inside_code_block = False # translate translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=&quot;pt&quot;)) tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] # postprocessing ## replace code_blog tags with code for i, line in enumerate(tgt_text): if line == &#39;&lt;&lt;code_block&gt;&gt;&#39;: tgt_text[i] = code.pop(0) ## remove the eol (but keep empty list entries / lines) tgt_text = [s.replace(&#39;&#39;, &#39;&#39;,) for s in tgt_text] ## remove space between ]( to get the md link syntax right tgt_text = [s.replace(&#39;](&#39;, &#39;](&#39;,) for s in tgt_text] # write english blog post with open(&#39;2020-12-26-blog-translator.md&#39;, &#39;w&#39;) as f_out: for line in tgt_text: f_out.write(&quot;%s n&quot; % line) f_out.close() . Since this is my first NLP application, I left it with this Hello World code. Surely there are clever ways to map the markdown syntax in tokenizer. Maybe I’ll write a follow up when I find out. . By the way, the translation just made me adapt my German writing style. For example, sarcasm doesn’t work so well after translation, so I avoided it. Also, it often depends on the correct choice of words (e.g. there is no markdown command, but there is markdown syntax). «eol&gt; . Best regards . Johannes &amp; the Robot .",
            "url": "https://joatom.github.io/ai_curious/nlp/2020/12/26/blog-translator.html",
            "relUrl": "/nlp/2020/12/26/blog-translator.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A handful of bricks - from SQL to Pandas",
            "content": "Original article published at datamuni.com. . SQL vs. and Pandas . I love SQL. It’s been around for decades to arrange and analyse data. Data is kept in tables which are stored in a relational structure. Consistancy and data integraty is kept in mind when designing a relational data model. However, when it comes to machine learning other data structures such as matrices and tensors become important to feat the underlying algorithms and make data processing more efficient. That’s where Pandas steps in. From a SQL developer perspective it is the library to close the gap between your data storage and the ml frameworks. . This blog post shows how to translate some common and some advanced techniques from SQL to Pandas step-by-step. I didn’t just want to write a plain cheat sheet (actually Pandas has a good one to get started: Comparison SQL (Ref. 1)). Rather I want to unwind some concepts that might be helpful for a SQL developer who now and then deals with Pandas. . The coding examples are built upon a Lego Dataset (Ref. 2), that contains a couple of tables with data about various lego sets. . To follow along I’ve provided a notebook (Res. 1) on kaggle, where you can play with the blog examples either using SQLite or Bigquery. You can also checkout a docker container (Res. 2) to play on your home machine. . Missing bricks . First listen to this imaginary dialogue that guides us throug the coding: . :hatched_chick: I miss all red bricks of the Lego Pizzeria. I definetly need a new one. . :penguin: Don’t worry. We can try to solve this with data. That will be fun. :-) . :hatched_chick: (!@#%&amp;) You’re kidding, right? . Now that we have a mission we are ready to code and figuere out how to deal with missing bricks. First we inspect the tables. They are organized as shown in the relational diagram (Fig. 1). . . Fig. 1: Data model (source: Lego dataset (Ref. 2)) . There are colors, parts, sets and inventories. We should start by searching for the Pizzeria in the sets table using the set number (41311). . . Fig. 2: Lego Box with set number . A simple Filter (The behaviour of brackets.) . A simple like-filter on the sets table will return the set info. . SELECT * FROM sets s WHERE s.set_num like &#39;41311%&#39; . There are several ways to apply a filter in Pandas. The most SQL-like code utilizes the query-function which basicaly substitutes the where clause. . df_sets.query(&quot;set_num.str.contains(&#39;41311&#39;)&quot;, engine=&#39;python&#39;) .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Since the query function expects a String as input parameter we loose syntax highlighting and syntax checking for our filter expression. . Therefore a more commonly used expression consists of the bracket notation (The behaviour of the bracket notation of a class in python is implementated in the class function __getitem__) . See how we can apply an equals filter using brackets. . df_sets.query(&quot;set_num == &#39;41311-1&#39;&quot;) # or df_sets[df_sets.set_num == &#39;41311-1&#39;] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . There is a lot going on in this expression. . Let’s take it apart. . df_sets[&#39;set_num&#39;] returns a single column (a Pandas.Series object). A Pandas DataFrame is basically a collection of Series. Additionaly there is a row index (often just called index) and a column index (columnnames). Think of a column store database. . . Fig. 3: Elements of a DataFrame . Applying a boolean condition (== &#39;41311-1&#39;) to a Series of the DataFrame (df_sets[&#39;set_num&#39;]) will result in a boolean collection of the size of the column. . bool_coll = df_sets[&#39;set_num&#39;] == &#39;41311-1&#39; # only look at the position 3580 - 3590 of the collection bool_coll[3580:3590] . 3580 False 3581 False 3582 True 3583 False 3584 False 3585 False 3586 False 3587 False 3588 False 3589 False Name: set_num, dtype: bool . The boolean collection now gets past to the DataFrame and filters the rows: . df_sets[bool_coll] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] . Depending on what type of object we pass to the square brackets the outcome result in very different behaviors. . We have already seen in the example above that if we pass a boolean collection with the size of number of rows, the collection is been handled as a row filter. . But if we pass a column name or a list of column names to the brackets instead, the given columns are selected like in the SELECT clause of an SQL statement. . SELECT name, year FROM lego.sets; . =&gt; . df_sets[[&#39;name&#39;, &#39;year&#39;]] . Row filter and column selection can be combined like this: . SELECT s.name, s.year FROM lego.sets s WHERE s.set_num = &#39;41311-1&#39;; . =&gt; . df_temp = df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] df_temp[[&#39;name&#39;,&#39;year&#39;]] # or simply: df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;][[&#39;name&#39;,&#39;year&#39;]] .   name year . 3582 | Heartlake Pizzeria | 2017 | . Indexing (What actually is an index?) . Another way to access a row in Pandas is by using the row index. With the loc function (and brackets) we select the Pizzeria and another arbitrary set. We use the row numbers to filter the rows. . df_sets.loc[[236, 3582]] .   set_num name year theme_id num_parts . 236 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . If we inspect the DataFrame closely we realize that it doesn’t realy look like a simple table but rather like a cross table. . The first column on the left is a row index and the table header is the column index. In the center the values of the columns are displayed (see Fig. 3). . If we think of the values as a matrix the rows are dimension 0 and columns are dimension 1. The dimension is often used in DataFrame functions as axis parameter. E.g. dropping columns can be done using dimensional information: . -- EXCEPT in SELECT clause only works with BIGQUERY SELECT s.* EXCEPT s.year FROM lego.sets s; . ==&gt; . df_sets.drop([&#39;year&#39;], axis = 1).head(5) .   set_num name theme_id num_parts . 0 | 00-1 | Weetabix Castle | 414 | 471 | . 1 | 0011-2 | Town Mini-Figures | 84 | 12 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 199 | 2 | . 3 | 0012-1 | Space Mini-Figures | 143 | 12 | . 4 | 0013-1 | Space Mini-Figures | 143 | 12 | . The indexes can be accessed with the index and columns variable. axes contains both. . df_sets.index # row index df_sets.columns # column index df_sets.axes # both . [RangeIndex(start=0, stop=11673, step=1), Index([&#39;set_num&#39;, &#39;name&#39;, &#39;year&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;], dtype=&#39;object&#39;)] . The row index doesn’t necessarely be the row number. We can also convert a column into a row index. . df_sets.set_index(&#39;set_num&#39;).head() .   name year theme_id num_parts . set_num |   |   |   |   | . 00-1 | Weetabix Castle | 1970 | 414 | 471 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | . It is also possible to define hierarchicle indicies for multi dimensional representation. . df_sets.set_index([&#39;year&#39;, &#39;set_num&#39;]).sort_index(axis=0).head() # axis = 0 =&gt; row index .     name theme_id num_parts . year | set_num |   |   |   | . 150 | 700.1.1-1 | Individual 2 x 4 Bricks | 371 | 10 | .   | 700.1.2-1 | Individual 2 x 2 Bricks | 371 | 9 | .   | 700.A-1 | Automatic Binding Bricks Small Brick Set (Lego… | 366 | 24 | .   | 700.B.1-1 | Individual 1 x 4 x 2 Window (without glass) | 371 | 7 | .   | 700.B.2-1 | Individual 1 x 2 x 3 Window (without glass) | 371 | 7 | . Sometimes it is usefull to reset the index, hence reset the row numbers. . df_sets.loc[[236, 3582]].reset_index(drop = True) # set drop = False to keep the old index as new column .   set_num name year theme_id num_parts . 0 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 1 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Now we get a sence what is meant by an index in Pandas in contrast to SQL. . Indices in SQL are hidden data structures (in form of e.g. b-trees or hash-tables). They are built to access data more quickly, to avoid full table scans when appropriate or to mantain consistancies when used with constraints. . An index in Pandas can rather be seen as a dimensional access to the data values. They can be distingueshed between row and column indices. . Joins (Why merge doesn’t mean upsert.) . :hatched_chick: What are we gonna do now about my missing parts? . :penguin: We don’t have all the information we need, yet. We need to join the other tables. . Though there is a function called join to join DataFrames I always use the merge function. This can be a bit confusing, when you are used to Oracle where merge means upsert/updelete rather then combining two tables. . When combining two DataFrames with the merge function in Pandas we have to define the relationship more explicitly. If you are used to SQL thats what you want. . In contrast the join function implicitly combines the DataFrames by their index or column names. It also enables multiply DataFrame joins in one statement as long as the join columns are matchable by name. . SELECT * FROM sets s INNER JOIN inventories i ON s.set_num = i.set_num -- USING (set_num) LIMIT 5 . set_num name year theme_id num_parts id version set_num_1 . 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | 00-1 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | 0011-2 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | 0011-3 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | 0012-1 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | 0013-1 | . These Pandas statements all do the same: . df_sets.merge(df_inventories, how = &#39;inner&#39;, on = &#39;set_num&#39;).head(5) #or if columns are matching df_sets.merge(df_inventories, on = &#39;set_num&#39;).head(5) #or explicitly defined columns df_sets.merge(df_inventories, how = &#39;inner&#39;, left_on = &#39;set_num&#39;, right_on = &#39;set_num&#39;).head(5) .   set_num name year theme_id num_parts id version . 0 | 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | . 1 | 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | . 3 | 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | . 4 | 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | . To see witch parts are needed for the Pizzeria we combine some tables. We look for the inventory of the set and gather all parts. Then we get color and part category information. . We end up with an inventory list: . SELECT s.set_num, s.name set_name, p.part_num, p.name part_name, ip.quantity, c.name color, pc.name part_cat FROM sets s, inventories i, inventory_parts ip, parts p, colors c, part_categories pc WHERE s.set_num = i.set_num AND i.id = ip.inventory_id AND ip.part_num = p.part_num AND ip.color_id = c.id AND p.part_cat_id = pc.id AND s.set_num in (&#39;41311-1&#39;) AND i.version = 1 AND ip.is_spare = &#39;f&#39; ORDER BY p.name, s.set_num, c.name LIMIT 10 . set_num set_name part_num part_name quantity color part_cat . 41311-1 | Heartlake Pizzeria | 25269pr03 | 1/4 CIRCLE TILE 1X1 with Pizza Print | 4 | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 32807 | BRICK 1X1X1 1/3, W/ ARCH | 4 | Red | Other | . 41311-1 | Heartlake Pizzeria | 6190 | Bar 1 x 3 (Radio Handle, Phone Handset) | 1 | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 30374 | Bar 4L (Lightsaber Blade / Wand) | 1 | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 99207 | Bracket 1 x 2 - 2 x 2 Inverted | 1 | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 2453b | Brick 1 x 1 x 5 with Solid Stud | 4 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 4 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 3 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 1 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 3245b | Brick 1 x 2 x 2 with Inside Axle Holder | 2 | White | Bricks | . Lets create a general inventory_list as view and make the statement more readable and comparable with ANSI-syntax (separating filters/predicates from join conditions). . DROP VIEW IF EXISTS inventory_list; CREATE VIEW inventory_list AS SELECT s.set_num, s.name set_name, s.theme_id, s.num_parts, p.part_num, ip.quantity, p.name part_name, c.name color, pc.name part_cat FROM sets s INNER JOIN inventories i USING (set_num) INNER JOIN inventory_parts ip ON (i.id = ip.inventory_id) INNER JOIN parts p USING (part_num) INNER JOIN colors c ON (ip.color_id = c.id) INNER JOIN part_categories pc ON (p.part_cat_id = pc.id) WHERE i.version = 1 AND ip.is_spare = &#39;f&#39;; . Now we translate the view to Pandas. We see how the structure relates to sql. The parameter how defines the type of join (here: inner), on represents the USING clause whereas left_on and right_on stand for the SQL ON condition. . In SQL usually an optimizer defines based in rules or statistics the execution plan (the order in which the tables are accessed, combined and filtered). I’m not sure if Pandas follows a similar approache. To be safe, I assume the order and early column dropping might matter for performance and memory management. . df_inventory_list = df_sets[[&#39;set_num&#39;, &#39;name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;]] .merge( df_inventories[df_inventories[&#39;version&#39;] == 1][[&#39;id&#39;, &#39;set_num&#39;]], how = &#39;inner&#39;, on = &#39;set_num&#39; ) .merge( df_inventory_parts[df_inventory_parts[&#39;is_spare&#39;] == &#39;f&#39;][[&#39;inventory_id&#39;, &#39;part_num&#39;, &#39;color_id&#39;, &#39;quantity&#39;]], how = &#39;inner&#39;, left_on = &#39;id&#39;, right_on = &#39;inventory_id&#39; ) .merge( df_parts[[&#39;part_num&#39;, &#39;name&#39;, &#39;part_cat_id&#39;]], how = &#39;inner&#39;, on = &#39;part_num&#39; ) .merge( df_colors[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;color_id&#39;, right_on = &#39;id&#39; ) .merge( df_part_categories[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;part_cat_id&#39;, right_on = &#39;id&#39; ) # remove some columns and use index as row number (reset_index) df_inventory_list = df_inventory_list.drop([&#39;id_x&#39;, &#39;inventory_id&#39;, &#39;color_id&#39;, &#39;part_cat_id&#39;, &#39;id_y&#39;, &#39;id&#39;], axis = 1).reset_index(drop = True) # rename columns df_inventory_list.columns = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;, &#39;part_num&#39;, &#39;quantity&#39;, &#39;part_name&#39;, &#39;color&#39;, &#39;part_cat&#39;] . Lots of code here. So we better check if our Pandas code matches the results of our SQL code. . Select the inventory list for our example, write it to a DataFrame (df_test_from_sql) and compare the results. . df_test_from_sql &lt;&lt; SELECT il.* FROM inventory_list il WHERE il.set_num in (&#39;41311-1&#39;) ORDER BY il.part_name, il.set_num, il.color LIMIT 10; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # test df_test_from_df = df_inventory_list[df_inventory_list[&#39;set_num&#39;].isin([&#39;41311-1&#39;])].sort_values(by=[&#39;part_name&#39;, &#39;set_num&#39;, &#39;color&#39;]).head(10) df_test_from_df .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 507161 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 543292 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 266022 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 273113 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 306863 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 47206 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 50211 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 45716 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 16485 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 22890 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # assert equals if not USE_BIGQUERY: df_test_from_sql = df_test_from_sql.DataFrame() pd._testing.assert_frame_equal(df_test_from_sql, df_test_from_df.reset_index(drop = True)) . The results are equal as expected. . Since we are only interested in the red bricks we create a list of those missing parts. . SELECT * FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND part_cat like &#39;%Brick%&#39; AND color = &#39;Red&#39; ORDER BY il.color, il.part_name, il.set_num; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . We need to watchout for the brackets when combining filters in DataFrames. . df_missing_parts = df_inventory_list[(df_inventory_list[&#39;set_num&#39;] == &#39;41311-1&#39;) &amp; (df_inventory_list[&#39;part_cat&#39;].str.contains(&quot;Brick&quot;)) &amp; (df_inventory_list[&#39;color&#39;] == &#39;Red&#39;) ].sort_values(by=[&#39;color&#39;, &#39;part_name&#39;, &#39;set_num&#39;]).reset_index(drop = True) df_missing_parts .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 0 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 1 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . :penguin: There we go, we are missing one 2x2 brick and tw0 2x2 double convex. . :hatched_chick: Yup, that’s the roof of the fireplace. I knew that before. . Conditional Joins and Aggregation (Almost done!) . Next we search for sets that contain the missing parts. The quantity of the parts in the found sets must be greater or equal the quantity of the missing parts. . In SQL it is done with an conditional join il.quantity &gt;= mp.quantity. . sets_with_missing_parts &lt;&lt; -- A list of missing parts WITH missing_parts AS ( SELECT il.set_name, il.part_num, il.part_name, il.color, il.quantity FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND il.part_cat like &#39;%Brick%&#39; AND il.color = &#39;Red&#39; ) -- Looking for set that contains as much of the missing parts as needed SELECT mp.set_name as searching_for_set, il.set_num, il.set_name, il.part_name, -- total number of parts per set il.num_parts, -- how many of the missing parts were found in the set COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set FROM inventory_list il INNER JOIN missing_parts mp ON (il.part_num = mp.part_num AND il.color = mp.color -- searching for a set that contains at least as much parts as there are missing AND il.quantity &gt;= mp.quantity ) -- don&#39;t search in the Pizzeria set WHERE il.set_num &lt;&gt; &#39;41311-1&#39; -- prioritize sets with all the missing parts and as few parts as possible ORDER BY matches_per_set DESC, il.num_parts, il.set_num, il.part_name LIMIT 16 . Conditional Join . There is no intuitive way to do a conditional join on DataFrames. The easiest I’ve seen so far is a two step solution. As substitution for the SQL WITH-clause we can reuse df_missing_parts. . # 1. merge on the equal conditions df_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = &#39;inner&#39;, on = [&#39;part_num&#39;, &#39;color&#39;], suffixes = (&#39;_found&#39;, &#39;_missing&#39;)) # 2. apply filter for the qreater equals condition df_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts[&#39;quantity_found&#39;] &gt;= df_sets_with_missing_parts[&#39;quantity_missing&#39;]] # select columns cols = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;part_name&#39;, &#39;num_parts&#39;] df_sets_with_missing_parts = df_sets_with_missing_parts[[&#39;set_name_missing&#39;] + [c + &#39;_found&#39; for c in cols]] df_sets_with_missing_parts.columns = [&#39;searching_for_set&#39;] + cols . Aggregation . In the next step the aggregation of the analytic function . COUNT(*) OVER (PARTITION BY il.set_num) matches_per_set . needs to be calculated. Hence the number of not-NaN values will be counted per SET_NUM group and assigned to each row in a new column (matches_per_set). . But before translating the analytic function, let’s have a look at a regular aggregation, first. Say, we simply want to count the entries per set_num on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this: . SELECT s.set_num, COUNT(*) AS matches_per_set SUM(s.num_parts) AS total_num_parts FROM ... WHERE ... GROUP BY s.set_num; . All selected columns must either be aggregated by a function (COUNT, SUM) or defined as a group (GROUP BY). The result is a two column list with the group set_num and the aggregations matches_per_set and total_num_part. . Now see how the counting is done with Pandas. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).count() .sort_values(&#39;set_num&#39;, ascending = False) # for sum and count: # df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).agg([&#39;count&#39;, &#39;sum&#39;]) . ||searching_for_set|set_name|part_name|num_parts| |-|-|-|-|-| |set_num||||| |llca8-1|1|1|1|1| |llca21-1|1|1|1|1| |fruit1-1|1|1|1|1| |MMMB026-1|1|1|1|1| |MMMB003-1|1|1|1|1| |…|…|…|…|…| |10021-1|1|1|1|1| |088-1|1|1|1|1| |080-1|2|2|2|2| |066-1|1|1|1|1| |00-4|1|1|1|1| 468 rows × 4 columns . Wow, that’s different! The aggregation function is applied to every column independently and the group is set as row index. But it is also possible to define the aggregation function for each column explicitly like in SQL: . df_sets_with_missing_parts.groupby([&#39;set_num&#39;], as_index = False) .agg(matches_per_set = pd.NamedAgg(column = &quot;set_num&quot;, aggfunc = &quot;count&quot;), total_num_parts = pd.NamedAgg(column = &quot;num_parts&quot;, aggfunc = &quot;sum&quot;)) . ||set_num|matches_per_set|total_num_parts| |-|-|-|-| |0|00-4|1|126| |1|066-1|1|407| |2|080-1|2|1420| |3|088-1|1|615| |4|10021-1|1|974| |…|…|…|…| |463|MMMB003-1|1|15| |464|MMMB026-1|1|43| |465|fruit1-1|1|8| |466|llca21-1|1|42| |467|llca8-1|1|58| 468 rows × 3 columns . This looks more familiar. With the as_index argument the group becomes a column (rather than a row index). . So, now we return to our initial task translating the COUNT(*) OVER(PARTITION BY) clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like . df_sets_with_missing_parts.merge(my_agg_df, on = &#39;set_num&#39;) . A more common why is to use the transform() function: . # add aggregatiom df_sets_with_missing_parts[&#39;matches_per_set&#39;] = df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;].transform(&#39;count&#39;) df_sets_with_missing_parts.head(5) .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 00-4 | Weetabix Promotional Windmill | Slope 45° 2 x 2 | 126 | 1 | . 1 | Heartlake Pizzeria | 066-1 | Basic Building Set | Slope 45° 2 x 2 | 407 | 1 | . 2 | Heartlake Pizzeria | 080-1 | Basic Building Set with Train | Slope 45° 2 x 2 | 710 | 2 | . 3 | Heartlake Pizzeria | 088-1 | Super Set | Slope 45° 2 x 2 | 615 | 1 | . 4 | Heartlake Pizzeria | 10021-1 | U.S.S. Constellation | Slope 45° 2 x 2 | 974 | 1 | . Let’s elaborate the magic that’s happening. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;] . returns a GroupByDataFrame which contains the group names (from set_num) and all row/column indicies and values related to the groups. Here only one column [&#39;part_name&#39;] is selected. In the next step transform applies the given function (count) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4. . . Fig. 4: Aggregation with transform . Now that we have gathered all the data we arange the results so that they can be compared to the SQL data: . # sort and pick top 16 df_sets_with_missing_parts = df_sets_with_missing_parts.sort_values([&#39;matches_per_set&#39;, &#39;num_parts&#39;, &#39;set_num&#39;, &#39;part_name&#39;], ascending = [False, True, True, True]).reset_index(drop = True).head(16) df_sets_with_missing_parts .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 1 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 2 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 3 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 4 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 | 58 | 2 | . 5 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 Double Convex | 58 | 2 | . 6 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 | 59 | 2 | . 7 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 8 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 | 59 | 2 | . 9 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 10 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 | 60 | 2 | . 11 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 Double Convex | 60 | 2 | . 12 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 | 76 | 2 | . 13 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 Double Convex | 76 | 2 | . 14 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 | 92 | 2 | . 15 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 Double Convex | 92 | 2 | . # assert equals if not USE_BIGQUERY: sets_with_missing_parts = sets_with_missing_parts.DataFrame() pd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts) . The results are matching! . :penguin: We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-) . :hatched_chick: (#@§?!*#) Are you sure your data is usefull for anything? . Recursion (Lost in trees?) . We solved the red brick problem. But since we have the data already open, let’s have a closer look at the Fire Engine, set number 336-1. . SELECT s.name AS set_name, s.year, t.id, t.name AS theme_name, t.parent_id FROM sets s, themes t WHERE t.id = s.theme_id AND s.set_num = &#39;336-1&#39;; . set_name year id theme_name parent_id . Fire Engine | 1968 | 376 | Fire | 373.0 | . The fire engine is quiet old (from 1968) and it belongs to the theme Fire. The themes table also includes as column called parent_id. This suggests that themes is a hierarchical structure. We can check this with an recursive WITH-clause in SQL. (*BQ: recursive WITH is not implemented in BIGQUERY. . WITH RECURSIVE hier(name, parent_id, level) AS ( -- init recursion SELECT t.name, t.parent_id, 0 AS level FROM themes t WHERE id = 376 UNION ALL -- recursive call SELECT t.name, t.parent_id, h.level +1 FROM themes t, hier h WHERE t.id = h.parent_id ) SELECT COUNT(1) OVER() - level AS level, name as theme, GROUP_CONCAT(name,&#39; --&gt; &#39;) over(order by level desc) path FROM hier ORDER BY level; . level theme path . 1 | Classic | Classic | . 2 | Vehicle | Classic –&gt; Vehicle | . 3 | Fire | Classic –&gt; Vehicle –&gt; Fire | . OK, that looks like a reasonable hierarchie. The path column includes the parents and grant parents of a theme. What if we want to reverse the order of the path. Unfortunately GROUP_CONCAT in SQLite doesn’t allow us to specify a sort order in the aggregation. It’s possible to add custom aggregation function in some databases. In SQLite we can compile application defined function or in Oracle we can define customized aggregation function even at runtime as types. . Quiet some steps need to be taken to make the database use costumized aggregation efficently, so we can use them like regulare aggregation and windowing function. In Oracle for instance we have to define: . initial values: total := 0; n := 0; . | calculation per iteration step: total := total + this_step_value; n := n + 1; . | deletion per iteration for windowing: total := total - removed_step_value; n := n - 1; . | merging for parallel execution: total := total_worker1 + total_worker2; n := n_worker1 + n_worker2; . | termination: my_avg := total / nullif(n-1, 0) . | Now, how are we gonna do this in Pandas? We start by traversing the hierarchie. . fire_engine_info = df_themes[df_themes[&#39;id&#39;] == 376].copy() fire_engine_info[&#39;level&#39;] = 0 parent_id = fire_engine_info.parent_id.values[0] lvl = 0 while not np.isnan(parent_id) and lvl &lt; 10: lvl+= 1 new_info = df_themes[df_themes[&#39;id&#39;] == parent_id].copy() new_info[&#39;level&#39;] = lvl parent_id = new_info.parent_id.values[0] fire_engine_info = fire_engine_info.append(new_info) fire_engine_info[&#39;grp&#39;]=0 fire_engine_info .   id name parent_id level grp . 375 | 376 | Fire | 373.0 | 0 | 0 | . 372 | 373 | Vehicle | 365.0 | 1 | 0 | . 364 | 365 | Classic | NaN | 2 | 0 | . On the one hand this is pretty need, since we can do what ever we want in a manually coded loop. On the other hand I doubt that it is very efficent when we have to deal with lots of data. But to be fair, Recursive WITH isn’t that fast either in SQL. . Finaly we consider how to do customized aggregation. We could do it in the loop above or we can rather use the library’s transform or apply functions. . We define a custom aggregation function cat_sorted and then use the apply function like this: . def cat_sorted(ser, df, val_col = None, sort_col = None): u=&#39; --&gt; &#39;.join(df[df.id.isin(ser)].sort_values(sort_col)[val_col].values) return [u] . fire_engine_info.apply(lambda x: cat_sorted(x, fire_engine_info, &#39;name&#39;, &#39;level&#39;)) .   id name parent_id level grp . 0 | Fire –&gt; Vehicle –&gt; Classic |   | Vehicle –&gt; Classic |   |   | . We can also apply rolling or windowing behaviour. . Note that a rolling representation or windowing on string values is not possible because Pandas only allows numeric values for those action. . fire_engine_info.rolling(10,min_periods=1)[&#39;level&#39;].apply(lambda x: sum(10**x), raw = False) . 375 1.0 372 11.0 364 111.0 Name: level, dtype: float64 . Now, we not only understand the numbers on the lego package but also have a better understandig of Pandas. . Summary (Got it!) . SQL stays my favourite language to access structured data arranged over many tables. Pandas shines when data already is gathered together and easily accessable (e.g. as csv file). There are alternatives to Pandas to build ml pipelines, such as Dask or CUDF. But learning Pandas is a good foundation to learn more of them. . Resources . To play with the examples: . Res. 1 Kaggle notebook: https://www.kaggle.com/joatom/a-handful-of-bricks-from-sql-to-pandas | Res. 2 Docker container: https://github.com/joatom/blog-resources/tree/main/handful_bricks | . References . Ref. 1 Pandas SQL comparison: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html | Ref. 2 The Lego dataset: https://www.kaggle.com/rtatman/lego-database | .",
            "url": "https://joatom.github.io/ai_curious/sql/pandas/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "relUrl": "/sql/pandas/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "date": " • Dec 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! . I’m Johannes and this is my hobby blog. The content is limited to topics I’m curious in my spare time. Don’t expect to find here anything related to my job. . I’m happy when you leave a comment on blog posts you like. . Cheers, . Johannes . . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joatom.github.io/ai_curious/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joatom.github.io/ai_curious/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}