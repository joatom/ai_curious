{
  
    
        "post0": {
            "title": "I bought a fitbit, now I can watch my models train",
            "content": "During the Covid winter I hardly had any reason to leave the house. It was clear that I actively had to look after my mental and physical well-being. So, I decided to buy a smart watch and take on the 10000 steps per day challenge. Henceforth I spent much more time outside in the sunlight moving my body. . When I registered my new Fitbit I instantly got attracted by the For Developers link. As you might guess my thoughts started spinning like - Aha, they are providing a SDK! I got to try this out at some point and build an app for the watch. - I wanted this app to be related to ML or at least to data somehow. My first thought was - Would it be possible to use the green heart-rate sensor light as an OCR? - Nope, to complicated for a fun project. - Then I went on with the registration of the watch on the webpage. . Some month later, on a usual Sunday, I lay resting on the couch after lunch while the kids and my wife were cleaning the table and kitchen (I did the cooking ;-)). A little bored, I thought of my ML-model that was training on Hotel Room classification for a couple of hours upstairs in the office. I wanted to know how it was preceding. Sneaking upstairs would result in half an hour in front of the computer, followed by some trouble with my wife :unamused:. - May by I should eventually register at Neptune.ai or wandb.ai, then I could preview my trainings from the couch on my cell phone!? … Or may be I now have a new fun project for my new watch :smile: :bulb:! - . User story . Those were the requirements that finally got implemented: . Statistics (for metrics and losses) need to be captured during the training, so that I can see if training improves. | Progress of training needs to be captured and made available to the watch instantly, so that I can estimate how long the training will take to finish. | Stats &amp; progs should be processable in pytorch and fastai trainings, so that I can use my preferred ML libraries. | The watch plots the metrics history of the current training as line chart. so that I can quickly see if training improves on which metric. | The metrics values should be plotted, so I can easily compare with former trainings. | Progress on epochs and batches (train and valid) are plotted, so I can easily estimate how long the remaining training steps will last. | . As side by-product the training progress can also be displayed on a browser. This feature was build in parallel for debugging purpose. . Architecture . API-Server . The requirements led to the architecture shown in the diagram. In the center of the application is an API-Server to coordinate the training and the watch. The Training, Watch and Web are client applications connected to the API-Server’s Watchtrain-Topic. The Topic contains a connection pool for the Training client (data Producer) and another connection pool for the Web and the Watch clients (data Consumer). . . The initial idea was to setup a classical Consumer/Producer (Pub/Sub) pattern. But it ended up a bit different. The Topic holds the data in an object rather than a queue-like state and also does some data processing. The Producer and Consumer can still subscribe at any time, but they are also strongly connected via Websockets. I took the chance to play around with websockets, since it is also available on the watch. . For each client type there is an Agent that processes the data and messages that are send from the clients. The stats and progress data is saved in the topic. The topic generates the metric chart that is send to the Consumers, since I couldn’t find a charts library in the watch SDK. . The Topic-Consumer-Producer-Agent “pattern” with the connection pool handler is set up in a generic way so it’s easy to develop other applications in the same manner and run them on the API-Server. . As API-Server I used FastApi which is easy to start with as shown on the tutorial site or in this video. . The communications between the components is done with JSON. Messages start with an action-field followed by the training_id and a more or less complex payload. Depending of the action value different functionalities are triggered, such as sending the metric image to the client or converting batch information into a progress bar. . Training . Fastai . The easiest way to implement the train logging is by using the Fastai Callback infrastructure. So I built a WebsocketLogger which gets past to the training like this: . learn = cnn_learner(dls, resnet18, pretrained = False, metrics=[accuracy, Recall(average=&#39;macro&#39;), Precision(average=&#39;macro&#39;)]) learn.unfreeze learn.fit_one_cycle(10, lr_max = 5e-3, cbs=[WebsocketLogger(&#39;ws://myapiserver:8555/ws/watchtrain/producer/12345&#39;)]) . Starting out by looking at the source code of the fastai build-in CSVLoggers and ProgressCallback I learned how to track train data (metrics, epoch and batch progress). A bit challenging was the integration of the websocket client. I preferred a permanent connection rather than many one time (open-send-close) connections. Otherwise a simple REST call would have been more suitable. It is also very important that training must not break when the websocket connection is lost or the API-Server isn’t available anymore. . That’s how it is implemented using the websocket-client library: . def __init__(self, conn, ...): self.conn = conn ... self.heartbeat = False self._ws_connect() ... ... # gets called when a websocket is opened def _on_ws_open(self,ws): # ws connection is now ready =&gt; unlock self.ws_ready_lock.release() self.heartbeat = True def _ws_connect(self): self.heartbeat = False # aquire lock until websocket is ready to use self.ws_ready_lock = threading.Lock() self.ws_ready_lock.acquire() print(&#39;Connecting websocket ...&#39;) self.ws = websocket.WebSocketApp(self.conn, on_open = self._on_ws_open, on_message = self._on_ws_message, on_error = self._on_ws_error, on_close = self._on_ws_close) # run websocket in background thread.start_new_thread(self.ws.run_forever, ()) # wait for websocket to be initialized, # if connection is not possible (e.g. APIServer is down) resume after 3 sec, but heartbeat stays FALSE self.ws_ready_lock.acquire(timeout = 3) print(&#39;... websocket connected.&#39;) . The WebSocketApp runs as a local websocket-handler in the background. The Locks are used to make sure the connection gets properly established before the first messages are send. The heartbeat is introduced to keep the training running even if the websocket connection is broken and could not be reconnected via WebSocketApp. . If there is no heartbeat anymore _ws_connect() is called again after any epoch. If the API-Server is still not reachable the training continuous after a 3 second waiting time. . Pytorch . I skipped the pytorch implementation until I need it. But it is straight forward. Start a WebSocketApp thread in the background. Send the data from inside of the training/validation/inference-loop. . Watch . . The layout is held pretty simple as shown in the picture. There is a progress bar for the epochs and one for the mini batches (train and valid). In the center is the chart of the metrics. And at the bottom are the latest metric values. The cell phone that belongs to the watch establish a websocket connection to the API-Server and puts EventListeners for incoming messages into place. The incoming messages are uploaded to the watch were they can be displayed. . Lessons learned . FastAPI . FastAPI is a well-documented and easy to use framework. In the beginning I set it up with HTTPS. There is a tutorial on how to setup FastAPI with Traefik. But since I wanted to run the server at home I had to invest some evenings to figure out, how to set it up by myself. I used mkcert for SSL creation. A docker file to setup an FastAPI-Server at home can now be found here. At the end when I got it working I decided to not use HTTPS for reasons described below, :man_shrugging:. . Websockets . The different components communicate instantly. The data is pushed to the watch, which is the preferred behavior on the receiving site. With the websocket on the training site it is a bit more complicated to be fail safe and pickup communication when the connection is broken for a longer period of time. I might switch this part to a simple REST-post in a later version. But this way it was a fun exersice nevertheless. . Fitbit SDK . The Fitbit SDK is nice. They provide an online IDE which can easily be connected to your devices. The SDK is documented with a few examples. They also host helpful forum. . I had a bit of a hard time when I tried to load and display the Metrics chart image to the watch. I had to figure out that there are two types of jpeg, progressive and basic. And only one worked. It also was hard to figure out that the the image needs to have a certain size to be displayed. But that’s part of the normal learning path with a new technology. . And than, there was this one thing that really upset me (But as fare as I read in forums it is not the Fitbit SDKs fault!). Android doesn’t allow regular HTTP connection through apps. That’s why I setup the API-Server with HTTPS. But since I generated the certificate on my own, it wasn’t a trusted source and therefore Android didn’t accept it. Then I found some post that showed how to access HTTP from a local net, but only for IP range 192.168.0.x. That meant either building a Reverse Proxy or changing the Subnet of my network. And then finally I needed to deal with the docker net-addresse where the API-Server is running. As suspected, one evening I freaked out - ?#@!, I just want to send a JSON to my cell phone! 30 years of web-development and all we ended up is JavaScript and SSL-certs @!# - That was a good time to go to bed, put the project aside for a few days and celebrate that most of the time I’m into data instead of GUI :grin:. . Besides that I really enjoyed it to build a nice app for my Fitbit. . Conclusion . It was a fun project! Websockets, API, ML, App on a watch, it all fits together. I still have a thousand ideas for improvements and features. But for now I leave it as it is. . Implementation . The code for this project can be found in this repository: https://github.com/joatom/watchtrain . References . Here is a list of my inspirations, templates and useful content listed by topic. . Fitbit SDK . Online IDE (there also is a CLI version for VScode) | Tutorials | . FastAPI . FastAPI’s tutorials | FastAPI and websockets | Docker setup | FastAPI with Traefik | Video Tutorial for using FastAPI to serve ML models | . Training . Fastai’s Datablock tutorial | CSV and Progress logger | . Others . Uvicorn HTTPS options | Mkcert installation guide | .",
            "url": "https://joatom.github.io/ai_curious/api/websockets/ml%20logging/fastai%20callbacks/2021/06/19/watchtrain.html",
            "relUrl": "/api/websockets/ml%20logging/fastai%20callbacks/2021/06/19/watchtrain.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Identifying Hotels",
            "content": "“Recognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked.” (Stylianou et al., 2019). . As part of the Eight Workshop on Fine-Grained Visual Categorization a kaggle competition was launched to support investigations by advancing models to identify hotels from images. . This post contains some parts of my contribution to the Hotel-ID to Combat Human Trafficking 2021 - FGVC8 kaggle competition. . The challenge . The competition contained 97000+ images of hotel rooms from 7700! different hotels around the world. The objective was to identify the hotels of 13000 images from the hidden test set. The metric of the competition was Mean Average Precision of the top 5 picks (MAP@5). My solution scored 14th place out of 92 teams with a 0.6164 MAP@5 on the private leaderboard. . My solution contained six CNN models with various configurations. More technical details and why I ended up with rather simple models is described in a kaggle discussion topic. . Here I post the training and inference of one of the six models as well as the ensemble inference code. . Training . The final training was done on the entire train dataset. I didn’t choose a cross validation strategy to safe training time. To keep variance low nonetheless I relied on the usual regularization strategies, such as dropout and augmentation and in particular on test time augmentation during inference. . To refine the model a validation set can be created by setting the debug flag as described in the notebook. . Inference . Inference ensemble . References . Stylianou, Abby and Xuan, Hong and Shende, Maya and Brandt, Jonathan and Souvenir, Richard and Pless, Robert (2019). Hotels-50K: A Global Hotel Recognition Dataset. The AAAI Conference on Artificial Intelligence (AAAI) .",
            "url": "https://joatom.github.io/ai_curious/vision/classification/2021/05/28/hotel.html",
            "relUrl": "/vision/classification/2021/05/28/hotel.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Bending the space with nonlinearity",
            "content": "I’m trying out a new kaggle feature which allows us to include kaggle notebooks in our personal blogs. This articel was initialy published as kaggle notebook in November 2020. .",
            "url": "https://joatom.github.io/ai_curious/basics/2021/04/14/nonlinearity.html",
            "relUrl": "/basics/2021/04/14/nonlinearity.html",
            "date": " • Apr 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Sterbefälle nach Altersgruppen und Bundesländern in Deutschland von 2016 bis 2021",
            "content": ":de: :us: [kaggle version] . In diesem Blog-Post möchte ich herausfinden, wie leicht sich interaktive Grafiken in Notebooks erstellen lassen. Als Framework verwende ich hier Altair, da es sich in meinem Blog leicht integrieren lässt [FP20]. . Um den Beitrag auch inhaltlich interessant zu gestallten, werte ich die Sterbefallzahlen von Deutschland der letzten fünf Jahre aus. Dabei Grenze ich die Zeiträume vor und während Covid19 voneinander ab. . Als Fragestellung definiere ich: . Wie verändern sich die Sterbefallzahlen je Altersgruppe und Monat? | Wie verändern sich die Sterbefallzahlen je Bundesland? | . Im ersten Teil wird die Datenherkunft und -verarbeitung beschrieben. Anschließend werden rund um die obigen Fragestellungen Grafiken aufgebaut. Zum Schluss folgt ein technisches Fazit zum Framework. . Important: Die Sterbefallzahlen sind kein Indikator für die Gefährlichkeit von Covid19. Sie bilden lediglich rückwirkend betrachtet die Anzahl verstorbener Menschen ab. . Datenherkunft . Für die Auswertung von Sterbefällen in Deutschland werden die aktuellen Sterbefalldaten vom Statistischen Bundesamt [SB21] herangezogen. Die Daten beinhalten unter anderem Aufstellungen der Todesfälle nach Altersgruppen oder Bundesländern. In dieser Analyse werden die monatlichen Sterbefallzahlen für den Zeitraum März 2016 bis Februar 2021 herangezogen. Die aktuellsten Daten liegen derzeit nur bis Februar 2021 vor und beinhalten einen Schätzanteil, der in der Datenquelle im Reiter &quot;Hinweise&quot; erklärt ist. Neben den Sterbefallzahlen werden zusätzlich Daten über die Bevölkerungsdichte der Bundesländer verarbeitet [SB20], wobei sich diese Zahlen auf den Stichtag 31.12.2019 beziehen. . Verarbeitung der Daten . Preprocessing . Der Auswertungszeitraum wird beschränkt auf März 2016 bis Februar 2021. Der Zeitraum, in dem Corona in Deutschland sehr aktive war, wird hier vereinfacht auf März 2020 (als Covid19 in Deutschland die ersten größeren gesellschaftlichen Veränderungen auslöste) bis Februar 2021 (orange) festgelegt. Der vor-Covid19-Zeitraum wird auf März 2016 bis Februar 2020 festgelegt (blau). Somit umfasst der Covid19-Zeitraum exakt ein Jahr und der vor-Covid19-Zeitraum exakt 4 Jahre. Somit bleiben beide Zeiträume ohne gravierende saisonale Abweichungen vergleichbar. Die Aufteilung der Zeiträume ist im nachstehenden Diagramm verdeutlicht. . Datenaggregation . Bei den Berechnungen werden die Werte über einen Zeitraum über das Arithmetische Mittel aggregiert. Je nach Auswertung geschieht dies über den ganzen Zeitraum oder je Monat. Das erste und dritte Quartil, der aggregierten Daten, werden gegebenenfalls als Schattierung in den Diagrammen mit abgebildet. In einigen Abbildung werden die berechneten Punkte interpoliert um die Lesbarkeit zu erhöhen. . . Tip: Bei interaktiven Grafiken befinden sich in der rechten oberen Ecke Steuerungselemente, wie Dropdown-Boxen. Mit dem Mausrad lässt sich zoomen und per Doppelklick lässt sich eine Grafik zurücksetzen. . Todesf&#228;lle nach Alter . Im nachstehenden Diagramm wird die durchschnittliche Anzahl an Todesfällen pro Monat je Altersgruppe abgebildet. Eine Altersgruppe umfasst fünf Jahre. Der Punkt Alter 55 umfasst z.B. alle Todesfälle im Alter zwischen 50 und 55. Todesfälle der über 100-jährigen werden im Punkt 100 abgebildet. . Die Werte sind jeweils über den vor-Covid19-Zeitraum (blau) und den Covid19-Zeitraum (orange) aggregiert. Im Diagramm lassen sich die Monate per Dropdown-Box auswählen. . Nachfolgend sind die durchschnittlichen Todesfälle nach Alter für jeden einzelnen Monat aufgelistet. Die Auflistung beginnt mit dem Monat März. . Beobachtung . In den Altersgruppen unter 55 Jahren kam es in diesen Zeiträumen zu keiner Übersterblichkeit. In den Altersgruppen ab 80 Jahren nahm die Übersterblichkeit massiv zu. . Todesf&#228;lle nach Bundesland . Im nächsten Diagramm lassen sich für die Altersgruppen 0-65 und 65+ die Todesfälle für die beiden Zeiträume je Bundesland auswerten. Per Dropdown-Box kann zwischen den Kennzahlen Todesfälle oder Todesfälle je 100.000 Einwohner gewählt werden. . Beobachtung . Nordrhein-Westfalen (NW) hat als Einwohner-stärkstes Land die meisten Todesfälle. In jedem Bundesland sind Anstiege der Todeszahlen im Covid19-Zeitraum zu erkennen. Allerdings sind lediglich minimale Anstiege in Hessen (HE) und Bayern (BY) für die Altersgruppen unter 65 Jahren zu erkennen. In den anderen Bundesländern gibt es keinen merklichen Anstieg in dieser Altersgruppe. . Bei der Darstellung der Todesfällen je Einwohner gibt es in der Altersgruppe unter 65 kleinere Schwankungen. Der Anstieg an Todesfällen in Nordrhein-Westfalen (NW) ist etwas geringer als in Bayern (BY) und in etwa so hoch wie in Baden-Württemberg (BW). Den stärksten Anstieg verzeichnet hier Sachsen (SN) und Brandenburg (BB). . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Todesf&#228;lle nach Bundesland und Bev&#246;lkerungsdichte . Das letzte Diagramm setzt die Bevölkerungsdichte der Bundesländer mit deren Todesfällen je 100.000 Einwohner in Bezug. . Beobachtung . Die Stadtstaaten Hamburg (HH) und Bremen (HB) weisen trotz ihrer hohen Einwohnerdichte einen nur moderaten Anstieg der Sterbefälle aus. Wohingegen Berlin einen hohen Anstieg verzeichnet. Schleswig-Holstein (SH) hat den geringsten Anstieg zu verzeichnen. . Der Korrelationskoeffizient zwischen der Bevölkerungsdichte und Anstieg der Todesfälle beträgt: . 0.0232 . Eine Korrelation ist für diesen Vergleich nicht festzustellen. . . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Technisches Fazit . Mit Altair lassen sich einfach Grafiken gestallten und interaktiv in Notebooks mit einbinden. Die Webseite beinhaltet eine Vielzahl von Beispielen [AA1]. Einige Fragestellungen werden auch im Github-Issue-Tracker [AA2] beantwortet. . Ich habe etwas Zeit mit JavaScript-Debugging verbracht, als ich die Bindings falsch verwendet habe. Es gab aber schlussendlich zu jedem Problem eine Lösung [AA2] oder Workarounds wie etwa bei der Legende im Bardiagramm [AG18]. Wie erwartet braucht es etwas Übung um ein neues Framework wie gewollt einsetzen zu können. . Die Integration [FP20] in diesen Blog funktioniert einwandfrei. Ich werde das Framework bestimmt auch an anderer Stelle verwenden. . Quellverzeichniss . Datenquellen . Die hier verwendeten Daten stammen vom Statistischen Bundesamtes und unterliegen der Lizenz &quot;dl-de/by-2-0&quot;. Der Lizenztext findet sich unter www.govdata.de/dl-de/by-2-0. Die Daten wurden zum Zwecke der Analyse ausschließlich innerhalb dieses Notebooks durch Ausführung des angegebenen Programmcodes modifiziert. . [SB21] Statistisches Bundesamt (Destatis), 2021 (erschienen am 30. März 2021), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, abgerufen 03.04.2021, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (erschienen am 2. September 2020), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, abgerufen am 03.04.2021, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Sonstige Quellen . A lot of the coding is derived from various examples of the Altair homepage and great examples in the coresponding github issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (erschienen am 06. Oktober 2018), Focus: generating an interactive legend in Altair, abgerufen 05.04.2021, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (erschienen am 20. Februar 2020), Fastpages Notebook Blog Post, abgerufen 05.04.2021, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany_de.html",
            "relUrl": "/eda/2021/04/06/mortality-germany_de.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Deaths by age group and states in Germany from 2016 to 2021",
            "content": ":de: :us: [kaggle version] . In this blog post, I want to find out how easy it is to create interactive charts in notebooks. As a framework, I will use Altair, as it can be easily integrated into my blog [FP20]. . In order to make the contribution interesting, I evaluate the death rates from Germany over the last five years. I construct two periods - the first period before and the second one during Covid19. . We will analyze these questions: . How do deaths per age group and month change? | How do deaths per state change? | . The first part describes the origin and processing of the data. Then, charts are built around the above questions. Finally, a technical conclusion to the framework follows. . Important: The death rates are not an indicator of the danger of Covid19. They only reflect the number of people who died retrospectively. . Source of data . For the analysis of deaths in Germany, the current death data from the Statistisches Bundesamt (Federal Statistical Office) [SB21] are used. The data include the number of deaths by age group or state on a monthly basis. In this blog post, the monthly death figures for the period March 2016 to February 2021 are examined. The most recent data are currently only available until February 2021 and include some estimations, which are explained in the data source in the tab &quot;Hinweise&quot;. In addition to the death figures, data on the population density of the states are processed [SB20]. These figures refere to the reference date 31.12.2019. . Procession of the data . Preprocessing . The evaluation period is limited to March 2016 until February 2021. The period in which Corona was heavily active in Germany is simplified here to March 2020 (when Covid19 triggered the first major social changes in Germany) until February 2021 (orange). The pre-Covid19 period is set to March 2016 until February 2020 (blue). Thus, the Covid19 period covers exactly one year and the pre-Covid19 period exactly four years. Thus, both periods remain comparable without serious seasonal deviations. The split of the periods is illustrated in the diagram below. . Aggregation of data . In the calculations, the values are averaged over a period of time. Depending on the evaluation, this happens over the whole period or per month. The first and third quartile of the aggregated data may also be shown as shading in the diagrams. In some charts, the calculated points are interpolated to increase readability. . . Tip: For some of the charts there are control elements in the upper right corner, such as drop-down boxes. The mouse wheel can be used to zoom and a chart can be reset by double-clicking. . Deaths by age . The diagram below shows the average number of deaths per month per age group. One age group covers five years. The point age 55 includes, for example, all deaths between 50 and 55. Deaths of those over 100 years of age are shown in point 100. . The values are aggregated over the pre-Covid19 period (blue) and the Covid19 period (orange). In the diagram, the months can be selected via drop-down box. . Below are the average deaths by age for each month. The list starts with the month of March. . Observation . In the age groups under 55, there was no excess mortality during these periods. In the age groups from 80 years and older, mortality increased massively. . Deaths by state . In the next diagram, the age groups 0-65 and 65+ can be evaluated for the two periods grouped by state. By drop-down box, you can choose between the key figures Deaths or Deaths per 100,000 inhabitants. . Observation . North Rhine-Westphalia (NW) has the largest number of deaths, since it is the state with highest population. In each state, there are increases in death rates in the Covid19 period. However, only minimal increases in Hesse (HE) and Bavaria (BY) can be seen for the age groups under the age of 65. In the other states, there is no noticeable increase in this age group. . There are smaller variations looking at the number of deaths per 100.000 inhabitants in the age group below 65. The increase in deaths in North Rhine-Westphalia (NW) is slightly lower than in Bavaria (BY) and roughly as high as in Baden-Württemberg (BW). Saxony (SN) and Brandenburg (BB) have the strongest increases. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Deaths by state and population density . The last diagram refers to the population density of the states and the amount of deaths per 100,000 inhabitants. . Observation . The city-states Hamburg (HH) and Bremen (HB) show a moderate increase in deaths despite their high population density. Berlin (BE) instead has a high increase in death rate. Schleswig-Holstein (SH) recorded the lowest increase. . The correlation coefficient of population density and increase in mortality is: . 0.0232 . Hence, There is no correlation for this comparison. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Technical conclusion . With Altair you can easily setup interactive charts and integrate them into notebooks. Their website contains a variety of examples [AA1]. Some questions are also answered in their Github Issue tracker [AA2]. . I spent some time debugging JavaScript when I used the bindings incorrectly. However, there was finally a solution to every problem [AA2] or workarounds such as for the legend in the bar diagram [AG18]. As expected, a bit of practice is neccessary to get used to a new framework. . The integration [FP20] in this blog works perfectly. I will certainly use the framework elsewhere. . References . Data sources . The data used here are from the &quot;Statistisches Bundesamt&quot; (Federal Statistical Office) and are subject to the license &quot;dl-de/by-2-0&quot;. The license text can be found at www.govdata.de/dl-de/by-2-0. The data were modified exclusively within this notebook by executing the specified program code for the purpose of analysis. . [SB21] Statistisches Bundesamt (Destatis), 2021 (published 2012/03/30), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, visited 2021/04/03, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (published 2020/09/02), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, visited 2021/04/03, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Other references . A lot of the coding is derived from various examples of the Altair homepage and from great examples in the coresponding Github Issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (published 2018/10/06), Focus: generating an interactive legend in Altair, visited 2021/04/05, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (published 2020/02/20), Fastpages Notebook Blog Post, visited 2021/04/05, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany.html",
            "relUrl": "/eda/2021/04/06/mortality-germany.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Automatically translate blog posts",
            "content": ":de: :us: . Attention! This text has been automatically translated! . Since I made so many mistakes in my first Blog post, I write this post in German and have it automatically translated. . For translation I use the popular NLP framework of huggingface.co. On their website is a simple example to implement a translation application and I will use it. . As expected, the Markdown syntax does not immediately work correctly when translating. So I had to make some adjustments at the beginning and afterwards. . The code (including pre- and post-processing) I used for the translation of the markdown files can be found here. But since it’s just a few lines of code, we can also look at it here: . from transformers import MarianMTModel, MarianTokenizer # load pretrained model and tokenizer model_name = &#39;Helsinki-NLP/opus-mt-de-en&#39; tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) # load german block post f_in = open(&quot;blog_translator_de.md&quot;, &quot;r&quot;) src_text = f_in.readlines() f_in.close() # preprocessing ## line break ( n) results to &quot;I don&#39;t know.&quot; We make it more specific: src_text = [s.replace(&#39; n&#39;,&#39; &#39;) for s in src_text] ## remove code block code = [] inside_code_block = False for i, line in enumerate(src_text): if line.startswith(&#39;&#39;) and not inside_code_block: # entering codeblock inside_code_block = True code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and not line.startswith(&#39;&#39;): code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and line.startswith(&#39;&#39;): # leaving code block code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; inside_code_block = False # translate translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=&quot;pt&quot;)) tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] # postprocessing ## replace code_blog tags with code for i, line in enumerate(tgt_text): if line == &#39;&lt;&lt;code_block&gt;&gt;&#39;: tgt_text[i] = code.pop(0) ## remove the eol (but keep empty list entries / lines) tgt_text = [s.replace(&#39;&#39;, &#39;&#39;,) for s in tgt_text] ## remove space between ]( to get the md link syntax right tgt_text = [s.replace(&#39;](&#39;, &#39;](&#39;,) for s in tgt_text] # write english blog post with open(&#39;2020-12-26-blog-translator.md&#39;, &#39;w&#39;) as f_out: for line in tgt_text: f_out.write(&quot;%s n&quot; % line) f_out.close() . Since this is my first NLP application, I left it with this Hello World code. Surely there are clever ways to map the markdown syntax in tokenizer. Maybe I’ll write a follow up when I find out. . By the way, the translation just made me adapt my German writing style. For example, sarcasm doesn’t work so well after translation, so I avoided it. Also, it often depends on the correct choice of words (e.g. there is no markdown command, but there is markdown syntax). «eol&gt; . Best regards . Johannes &amp; the Robot .",
            "url": "https://joatom.github.io/ai_curious/nlp/2020/12/26/blog-translator.html",
            "relUrl": "/nlp/2020/12/26/blog-translator.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Blog-Beiträge automatisch übersetzen",
            "content": ":de: :us: . Achtung! Dieser Text wurde automatisch übersetzt! . Da ich in meinem ersten Blog-Beitrag soviele Fehler auf Englisch gemacht habe, schreibe ich diesen Beitrag auf Deutsch und lasse ihn automatisch übersetzen. . Zur Übersetzung verwende ich das populäre NLP-Framework von huggingface.co. Auf ihrer Webseite ist ein einfaches Beispiel um eine Übersetzungsanwendung zu implementieren. Und diese werde ich verwenden. . Wie erwartet, hat es nicht auf Anhieb funktioniert die Markdown-Syntax bei der Übersetzung immer korrekt zu übernehmen. Also habe ich zu Beginn und hinterher noch ein paar Anpassungen vornehmen müssen. . Den Code (inkl. Vor- und Nachbearbeitung), den ich für die Übersetzung der Markdown-Dateien verwendet habe, findet ihr hier. Aber da es ja nur ein paar Zeilen Code sind, können wir sie uns auch eben hier anschauen: . from transformers import MarianMTModel, MarianTokenizer # load pretrained model and tokenizer model_name = &#39;Helsinki-NLP/opus-mt-de-en&#39; tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) # load german block post f_in = open(&quot;blog_translator_de.md&quot;, &quot;r&quot;) src_text = f_in.readlines() f_in.close() # preprocessing ## line break ( n) results to &quot;I don&#39;t know.&quot; We make it more specific: src_text = [s.replace(&#39; n&#39;,&#39; &lt;&lt;eol&gt;&gt;&#39;) for s in src_text] ## remove code block code = [] inside_code_block = False for i, line in enumerate(src_text): if line.startswith(&#39;&#39;) and not inside_code_block: # entering codeblock inside_code_block = True code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and not line.startswith(&#39;&#39;): code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and line.startswith(&#39;&#39;): # leaving code block code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; inside_code_block = False # translate translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=&quot;pt&quot;)) tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] # postprocessing ## replace code_blog tags with code for i, line in enumerate(tgt_text): if line == &#39;&lt;&lt;code_block&gt;&gt;&#39;: tgt_text[i] = code.pop(0) ## remove the eol (but keep empty list entries / lines) tgt_text = [s.replace(&#39;&lt;&lt;eol&gt;&gt;&#39;, &#39;&#39;,) for s in tgt_text] ## remove space between ] ( to get the md link syntax right tgt_text = [s.replace(&#39;] (&#39;, &#39;](&#39;,) for s in tgt_text] # write english blog post with open(&#39;2020-12-26-blog-translator.md&#39;, &#39;w&#39;) as f_out: for line in tgt_text: f_out.write(&quot;%s n&quot; % line) f_out.close() . Da das meine erster NLP-Anwendung ist, habe ich es bei diesem Hello World-Code belassen. Sicherlich gibt es geschickter Wege, um die Markdown-Syntax in tokenizer abzubilden. Vielleicht schreibe ich ein Follow up, wenn ich es rausgefunden habe. . Übrigens, die Übersetzung hat mich ebenfallls dazu gebracht meinen deutschen Schreibstil anzupassen. Beispielsweise funktioniert Sarkasmus nach der Übersetzung nicht so gut, also hab ich es vermieden. Außerdem kommt es auch häufig auf die richtige Wortwahl an (z.B. gibt es kein Markdown-Befehl, jedoch gibt es Markdown-Syntax). . Viele Grüße . Johannes &amp; der Roboter .",
            "url": "https://joatom.github.io/ai_curious/nlp/2020/12/26/blog-translator-de.html",
            "relUrl": "/nlp/2020/12/26/blog-translator-de.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "A handful of bricks - from SQL to Pandas",
            "content": "Original article published at datamuni.com. . SQL vs. and Pandas . I love SQL. It’s been around for decades to arrange and analyse data. Data is kept in tables which are stored in a relational structure. Consistancy and data integraty is kept in mind when designing a relational data model. However, when it comes to machine learning other data structures such as matrices and tensors become important to feat the underlying algorithms and make data processing more efficient. That’s where Pandas steps in. From a SQL developer perspective it is the library to close the gap between your data storage and the ml frameworks. . This blog post shows how to translate some common and some advanced techniques from SQL to Pandas step-by-step. I didn’t just want to write a plain cheat sheet (actually Pandas has a good one to get started: Comparison SQL (Ref. 1)). Rather I want to unwind some concepts that might be helpful for a SQL developer who now and then deals with Pandas. . The coding examples are built upon a Lego Dataset (Ref. 2), that contains a couple of tables with data about various lego sets. . To follow along I’ve provided a notebook (Res. 1) on kaggle, where you can play with the blog examples either using SQLite or Bigquery. You can also checkout a docker container (Res. 2) to play on your home machine. . Missing bricks . First listen to this imaginary dialogue that guides us throug the coding: . :hatched_chick: I miss all red bricks of the Lego Pizzeria. I definetly need a new one. . :penguin: Don’t worry. We can try to solve this with data. That will be fun. :-) . :hatched_chick: (!@#%&amp;) You’re kidding, right? . Now that we have a mission we are ready to code and figuere out how to deal with missing bricks. First we inspect the tables. They are organized as shown in the relational diagram (Fig. 1). . . Fig. 1: Data model (source: Lego dataset (Ref. 2)) . There are colors, parts, sets and inventories. We should start by searching for the Pizzeria in the sets table using the set number (41311). . . Fig. 2: Lego Box with set number . A simple Filter (The behaviour of brackets.) . A simple like-filter on the sets table will return the set info. . SELECT * FROM sets s WHERE s.set_num like &#39;41311%&#39; . There are several ways to apply a filter in Pandas. The most SQL-like code utilizes the query-function which basicaly substitutes the where clause. . df_sets.query(&quot;set_num.str.contains(&#39;41311&#39;)&quot;, engine=&#39;python&#39;) .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Since the query function expects a String as input parameter we loose syntax highlighting and syntax checking for our filter expression. . Therefore a more commonly used expression consists of the bracket notation (The behaviour of the bracket notation of a class in python is implementated in the class function __getitem__) . See how we can apply an equals filter using brackets. . df_sets.query(&quot;set_num == &#39;41311-1&#39;&quot;) # or df_sets[df_sets.set_num == &#39;41311-1&#39;] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . There is a lot going on in this expression. . Let’s take it apart. . df_sets[&#39;set_num&#39;] returns a single column (a Pandas.Series object). A Pandas DataFrame is basically a collection of Series. Additionaly there is a row index (often just called index) and a column index (columnnames). Think of a column store database. . . Fig. 3: Elements of a DataFrame . Applying a boolean condition (== &#39;41311-1&#39;) to a Series of the DataFrame (df_sets[&#39;set_num&#39;]) will result in a boolean collection of the size of the column. . bool_coll = df_sets[&#39;set_num&#39;] == &#39;41311-1&#39; # only look at the position 3580 - 3590 of the collection bool_coll[3580:3590] . 3580 False 3581 False 3582 True 3583 False 3584 False 3585 False 3586 False 3587 False 3588 False 3589 False Name: set_num, dtype: bool . The boolean collection now gets past to the DataFrame and filters the rows: . df_sets[bool_coll] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] . Depending on what type of object we pass to the square brackets the outcome result in very different behaviors. . We have already seen in the example above that if we pass a boolean collection with the size of number of rows, the collection is been handled as a row filter. . But if we pass a column name or a list of column names to the brackets instead, the given columns are selected like in the SELECT clause of an SQL statement. . SELECT name, year FROM lego.sets; . =&gt; . df_sets[[&#39;name&#39;, &#39;year&#39;]] . Row filter and column selection can be combined like this: . SELECT s.name, s.year FROM lego.sets s WHERE s.set_num = &#39;41311-1&#39;; . =&gt; . df_temp = df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] df_temp[[&#39;name&#39;,&#39;year&#39;]] # or simply: df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;][[&#39;name&#39;,&#39;year&#39;]] .   name year . 3582 | Heartlake Pizzeria | 2017 | . Indexing (What actually is an index?) . Another way to access a row in Pandas is by using the row index. With the loc function (and brackets) we select the Pizzeria and another arbitrary set. We use the row numbers to filter the rows. . df_sets.loc[[236, 3582]] .   set_num name year theme_id num_parts . 236 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . If we inspect the DataFrame closely we realize that it doesn’t realy look like a simple table but rather like a cross table. . The first column on the left is a row index and the table header is the column index. In the center the values of the columns are displayed (see Fig. 3). . If we think of the values as a matrix the rows are dimension 0 and columns are dimension 1. The dimension is often used in DataFrame functions as axis parameter. E.g. dropping columns can be done using dimensional information: . -- EXCEPT in SELECT clause only works with BIGQUERY SELECT s.* EXCEPT s.year FROM lego.sets s; . ==&gt; . df_sets.drop([&#39;year&#39;], axis = 1).head(5) .   set_num name theme_id num_parts . 0 | 00-1 | Weetabix Castle | 414 | 471 | . 1 | 0011-2 | Town Mini-Figures | 84 | 12 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 199 | 2 | . 3 | 0012-1 | Space Mini-Figures | 143 | 12 | . 4 | 0013-1 | Space Mini-Figures | 143 | 12 | . The indexes can be accessed with the index and columns variable. axes contains both. . df_sets.index # row index df_sets.columns # column index df_sets.axes # both . [RangeIndex(start=0, stop=11673, step=1), Index([&#39;set_num&#39;, &#39;name&#39;, &#39;year&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;], dtype=&#39;object&#39;)] . The row index doesn’t necessarely be the row number. We can also convert a column into a row index. . df_sets.set_index(&#39;set_num&#39;).head() .   name year theme_id num_parts . set_num |   |   |   |   | . 00-1 | Weetabix Castle | 1970 | 414 | 471 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | . It is also possible to define hierarchicle indicies for multi dimensional representation. . df_sets.set_index([&#39;year&#39;, &#39;set_num&#39;]).sort_index(axis=0).head() # axis = 0 =&gt; row index .     name theme_id num_parts . year | set_num |   |   |   | . 150 | 700.1.1-1 | Individual 2 x 4 Bricks | 371 | 10 | .   | 700.1.2-1 | Individual 2 x 2 Bricks | 371 | 9 | .   | 700.A-1 | Automatic Binding Bricks Small Brick Set (Lego… | 366 | 24 | .   | 700.B.1-1 | Individual 1 x 4 x 2 Window (without glass) | 371 | 7 | .   | 700.B.2-1 | Individual 1 x 2 x 3 Window (without glass) | 371 | 7 | . Sometimes it is usefull to reset the index, hence reset the row numbers. . df_sets.loc[[236, 3582]].reset_index(drop = True) # set drop = False to keep the old index as new column .   set_num name year theme_id num_parts . 0 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 1 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Now we get a sence what is meant by an index in Pandas in contrast to SQL. . Indices in SQL are hidden data structures (in form of e.g. b-trees or hash-tables). They are built to access data more quickly, to avoid full table scans when appropriate or to mantain consistancies when used with constraints. . An index in Pandas can rather be seen as a dimensional access to the data values. They can be distingueshed between row and column indices. . Joins (Why merge doesn’t mean upsert.) . :hatched_chick: What are we gonna do now about my missing parts? . :penguin: We don’t have all the information we need, yet. We need to join the other tables. . Though there is a function called join to join DataFrames I always use the merge function. This can be a bit confusing, when you are used to Oracle where merge means upsert/updelete rather then combining two tables. . When combining two DataFrames with the merge function in Pandas we have to define the relationship more explicitly. If you are used to SQL thats what you want. . In contrast the join function implicitly combines the DataFrames by their index or column names. It also enables multiply DataFrame joins in one statement as long as the join columns are matchable by name. . SELECT * FROM sets s INNER JOIN inventories i ON s.set_num = i.set_num -- USING (set_num) LIMIT 5 . set_num name year theme_id num_parts id version set_num_1 . 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | 00-1 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | 0011-2 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | 0011-3 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | 0012-1 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | 0013-1 | . These Pandas statements all do the same: . df_sets.merge(df_inventories, how = &#39;inner&#39;, on = &#39;set_num&#39;).head(5) #or if columns are matching df_sets.merge(df_inventories, on = &#39;set_num&#39;).head(5) #or explicitly defined columns df_sets.merge(df_inventories, how = &#39;inner&#39;, left_on = &#39;set_num&#39;, right_on = &#39;set_num&#39;).head(5) .   set_num name year theme_id num_parts id version . 0 | 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | . 1 | 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | . 3 | 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | . 4 | 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | . To see witch parts are needed for the Pizzeria we combine some tables. We look for the inventory of the set and gather all parts. Then we get color and part category information. . We end up with an inventory list: . SELECT s.set_num, s.name set_name, p.part_num, p.name part_name, ip.quantity, c.name color, pc.name part_cat FROM sets s, inventories i, inventory_parts ip, parts p, colors c, part_categories pc WHERE s.set_num = i.set_num AND i.id = ip.inventory_id AND ip.part_num = p.part_num AND ip.color_id = c.id AND p.part_cat_id = pc.id AND s.set_num in (&#39;41311-1&#39;) AND i.version = 1 AND ip.is_spare = &#39;f&#39; ORDER BY p.name, s.set_num, c.name LIMIT 10 . set_num set_name part_num part_name quantity color part_cat . 41311-1 | Heartlake Pizzeria | 25269pr03 | 1/4 CIRCLE TILE 1X1 with Pizza Print | 4 | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 32807 | BRICK 1X1X1 1/3, W/ ARCH | 4 | Red | Other | . 41311-1 | Heartlake Pizzeria | 6190 | Bar 1 x 3 (Radio Handle, Phone Handset) | 1 | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 30374 | Bar 4L (Lightsaber Blade / Wand) | 1 | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 99207 | Bracket 1 x 2 - 2 x 2 Inverted | 1 | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 2453b | Brick 1 x 1 x 5 with Solid Stud | 4 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 4 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 3 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 1 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 3245b | Brick 1 x 2 x 2 with Inside Axle Holder | 2 | White | Bricks | . Lets create a general inventory_list as view and make the statement more readable and comparable with ANSI-syntax (separating filters/predicates from join conditions). . DROP VIEW IF EXISTS inventory_list; CREATE VIEW inventory_list AS SELECT s.set_num, s.name set_name, s.theme_id, s.num_parts, p.part_num, ip.quantity, p.name part_name, c.name color, pc.name part_cat FROM sets s INNER JOIN inventories i USING (set_num) INNER JOIN inventory_parts ip ON (i.id = ip.inventory_id) INNER JOIN parts p USING (part_num) INNER JOIN colors c ON (ip.color_id = c.id) INNER JOIN part_categories pc ON (p.part_cat_id = pc.id) WHERE i.version = 1 AND ip.is_spare = &#39;f&#39;; . Now we translate the view to Pandas. We see how the structure relates to sql. The parameter how defines the type of join (here: inner), on represents the USING clause whereas left_on and right_on stand for the SQL ON condition. . In SQL usually an optimizer defines based in rules or statistics the execution plan (the order in which the tables are accessed, combined and filtered). I’m not sure if Pandas follows a similar approache. To be safe, I assume the order and early column dropping might matter for performance and memory management. . df_inventory_list = df_sets[[&#39;set_num&#39;, &#39;name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;]] .merge( df_inventories[df_inventories[&#39;version&#39;] == 1][[&#39;id&#39;, &#39;set_num&#39;]], how = &#39;inner&#39;, on = &#39;set_num&#39; ) .merge( df_inventory_parts[df_inventory_parts[&#39;is_spare&#39;] == &#39;f&#39;][[&#39;inventory_id&#39;, &#39;part_num&#39;, &#39;color_id&#39;, &#39;quantity&#39;]], how = &#39;inner&#39;, left_on = &#39;id&#39;, right_on = &#39;inventory_id&#39; ) .merge( df_parts[[&#39;part_num&#39;, &#39;name&#39;, &#39;part_cat_id&#39;]], how = &#39;inner&#39;, on = &#39;part_num&#39; ) .merge( df_colors[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;color_id&#39;, right_on = &#39;id&#39; ) .merge( df_part_categories[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;part_cat_id&#39;, right_on = &#39;id&#39; ) # remove some columns and use index as row number (reset_index) df_inventory_list = df_inventory_list.drop([&#39;id_x&#39;, &#39;inventory_id&#39;, &#39;color_id&#39;, &#39;part_cat_id&#39;, &#39;id_y&#39;, &#39;id&#39;], axis = 1).reset_index(drop = True) # rename columns df_inventory_list.columns = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;, &#39;part_num&#39;, &#39;quantity&#39;, &#39;part_name&#39;, &#39;color&#39;, &#39;part_cat&#39;] . Lots of code here. So we better check if our Pandas code matches the results of our SQL code. . Select the inventory list for our example, write it to a DataFrame (df_test_from_sql) and compare the results. . df_test_from_sql &lt;&lt; SELECT il.* FROM inventory_list il WHERE il.set_num in (&#39;41311-1&#39;) ORDER BY il.part_name, il.set_num, il.color LIMIT 10; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # test df_test_from_df = df_inventory_list[df_inventory_list[&#39;set_num&#39;].isin([&#39;41311-1&#39;])].sort_values(by=[&#39;part_name&#39;, &#39;set_num&#39;, &#39;color&#39;]).head(10) df_test_from_df .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 507161 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 543292 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 266022 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 273113 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 306863 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 47206 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 50211 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 45716 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 16485 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 22890 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # assert equals if not USE_BIGQUERY: df_test_from_sql = df_test_from_sql.DataFrame() pd._testing.assert_frame_equal(df_test_from_sql, df_test_from_df.reset_index(drop = True)) . The results are equal as expected. . Since we are only interested in the red bricks we create a list of those missing parts. . SELECT * FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND part_cat like &#39;%Brick%&#39; AND color = &#39;Red&#39; ORDER BY il.color, il.part_name, il.set_num; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . We need to watchout for the brackets when combining filters in DataFrames. . df_missing_parts = df_inventory_list[(df_inventory_list[&#39;set_num&#39;] == &#39;41311-1&#39;) &amp; (df_inventory_list[&#39;part_cat&#39;].str.contains(&quot;Brick&quot;)) &amp; (df_inventory_list[&#39;color&#39;] == &#39;Red&#39;) ].sort_values(by=[&#39;color&#39;, &#39;part_name&#39;, &#39;set_num&#39;]).reset_index(drop = True) df_missing_parts .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 0 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 1 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . :penguin: There we go, we are missing one 2x2 brick and tw0 2x2 double convex. . :hatched_chick: Yup, that’s the roof of the fireplace. I knew that before. . Conditional Joins and Aggregation (Almost done!) . Next we search for sets that contain the missing parts. The quantity of the parts in the found sets must be greater or equal the quantity of the missing parts. . In SQL it is done with an conditional join il.quantity &gt;= mp.quantity. . sets_with_missing_parts &lt;&lt; -- A list of missing parts WITH missing_parts AS ( SELECT il.set_name, il.part_num, il.part_name, il.color, il.quantity FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND il.part_cat like &#39;%Brick%&#39; AND il.color = &#39;Red&#39; ) -- Looking for set that contains as much of the missing parts as needed SELECT mp.set_name as searching_for_set, il.set_num, il.set_name, il.part_name, -- total number of parts per set il.num_parts, -- how many of the missing parts were found in the set COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set FROM inventory_list il INNER JOIN missing_parts mp ON (il.part_num = mp.part_num AND il.color = mp.color -- searching for a set that contains at least as much parts as there are missing AND il.quantity &gt;= mp.quantity ) -- don&#39;t search in the Pizzeria set WHERE il.set_num &lt;&gt; &#39;41311-1&#39; -- prioritize sets with all the missing parts and as few parts as possible ORDER BY matches_per_set DESC, il.num_parts, il.set_num, il.part_name LIMIT 16 . Conditional Join . There is no intuitive way to do a conditional join on DataFrames. The easiest I’ve seen so far is a two step solution. As substitution for the SQL WITH-clause we can reuse df_missing_parts. . # 1. merge on the equal conditions df_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = &#39;inner&#39;, on = [&#39;part_num&#39;, &#39;color&#39;], suffixes = (&#39;_found&#39;, &#39;_missing&#39;)) # 2. apply filter for the qreater equals condition df_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts[&#39;quantity_found&#39;] &gt;= df_sets_with_missing_parts[&#39;quantity_missing&#39;]] # select columns cols = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;part_name&#39;, &#39;num_parts&#39;] df_sets_with_missing_parts = df_sets_with_missing_parts[[&#39;set_name_missing&#39;] + [c + &#39;_found&#39; for c in cols]] df_sets_with_missing_parts.columns = [&#39;searching_for_set&#39;] + cols . Aggregation . In the next step the aggregation of the analytic function . COUNT(*) OVER (PARTITION BY il.set_num) matches_per_set . needs to be calculated. Hence the number of not-NaN values will be counted per SET_NUM group and assigned to each row in a new column (matches_per_set). . But before translating the analytic function, let’s have a look at a regular aggregation, first. Say, we simply want to count the entries per set_num on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this: . SELECT s.set_num, COUNT(*) AS matches_per_set SUM(s.num_parts) AS total_num_parts FROM ... WHERE ... GROUP BY s.set_num; . All selected columns must either be aggregated by a function (COUNT, SUM) or defined as a group (GROUP BY). The result is a two column list with the group set_num and the aggregations matches_per_set and total_num_part. . Now see how the counting is done with Pandas. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).count() .sort_values(&#39;set_num&#39;, ascending = False) # for sum and count: # df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).agg([&#39;count&#39;, &#39;sum&#39;]) .   searching_for_set set_name part_name num_parts . set_num |   |   |   |   | . llca8-1 | 1 | 1 | 1 | 1 | . llca21-1 | 1 | 1 | 1 | 1 | . fruit1-1 | 1 | 1 | 1 | 1 | . MMMB026-1 | 1 | 1 | 1 | 1 | . MMMB003-1 | 1 | 1 | 1 | 1 | . … | … | … | … | … | . 10021-1 | 1 | 1 | 1 | 1 | . 088-1 | 1 | 1 | 1 | 1 | . 080-1 | 2 | 2 | 2 | 2 | . 066-1 | 1 | 1 | 1 | 1 | . 00-4 | 1 | 1 | 1 | 1 | . Wow, that’s different! The aggregation function is applied to every column independently and the group is set as row index. But it is also possible to define the aggregation function for each column explicitly like in SQL: . df_sets_with_missing_parts.groupby([&#39;set_num&#39;], as_index = False) .agg(matches_per_set = pd.NamedAgg(column = &quot;set_num&quot;, aggfunc = &quot;count&quot;), total_num_parts = pd.NamedAgg(column = &quot;num_parts&quot;, aggfunc = &quot;sum&quot;)) .   set_num matches_per_set total_num_parts . 0 | 00-4 | 1 | 126 | . 1 | 066-1 | 1 | 407 | . 2 | 080-1 | 2 | 1420 | . 3 | 088-1 | 1 | 615 | . 4 | 10021-1 | 1 | 974 | . … | … | … | … | . 463 | MMMB003-1 | 1 | 15 | . 464 | MMMB026-1 | 1 | 43 | . 465 | fruit1-1 | 1 | 8 | . 466 | llca21-1 | 1 | 42 | . 467 | llca8-1 | 1 | 58 | . This looks more familiar. With the as_index argument the group becomes a column (rather than a row index). . So, now we return to our initial task translating the COUNT(*) OVER(PARTITION BY) clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like . df_sets_with_missing_parts.merge(my_agg_df, on = &#39;set_num&#39;) . A more common why is to use the transform() function: . # add aggregatiom df_sets_with_missing_parts[&#39;matches_per_set&#39;] = df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;].transform(&#39;count&#39;) df_sets_with_missing_parts.head(5) .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 00-4 | Weetabix Promotional Windmill | Slope 45° 2 x 2 | 126 | 1 | . 1 | Heartlake Pizzeria | 066-1 | Basic Building Set | Slope 45° 2 x 2 | 407 | 1 | . 2 | Heartlake Pizzeria | 080-1 | Basic Building Set with Train | Slope 45° 2 x 2 | 710 | 2 | . 3 | Heartlake Pizzeria | 088-1 | Super Set | Slope 45° 2 x 2 | 615 | 1 | . 4 | Heartlake Pizzeria | 10021-1 | U.S.S. Constellation | Slope 45° 2 x 2 | 974 | 1 | . Let’s elaborate the magic that’s happening. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;] . returns a GroupByDataFrame which contains the group names (from set_num) and all row/column indicies and values related to the groups. Here only one column [&#39;part_name&#39;] is selected. In the next step transform applies the given function (count) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4. . . Fig. 4: Aggregation with transform . Now that we have gathered all the data we arange the results so that they can be compared to the SQL data: . # sort and pick top 16 df_sets_with_missing_parts = df_sets_with_missing_parts.sort_values([&#39;matches_per_set&#39;, &#39;num_parts&#39;, &#39;set_num&#39;, &#39;part_name&#39;], ascending = [False, True, True, True]).reset_index(drop = True).head(16) df_sets_with_missing_parts .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 1 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 2 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 3 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 4 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 | 58 | 2 | . 5 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 Double Convex | 58 | 2 | . 6 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 | 59 | 2 | . 7 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 8 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 | 59 | 2 | . 9 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 10 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 | 60 | 2 | . 11 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 Double Convex | 60 | 2 | . 12 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 | 76 | 2 | . 13 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 Double Convex | 76 | 2 | . 14 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 | 92 | 2 | . 15 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 Double Convex | 92 | 2 | . # assert equals if not USE_BIGQUERY: sets_with_missing_parts = sets_with_missing_parts.DataFrame() pd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts) . The results are matching! . :penguin: We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-) . :hatched_chick: (#@§?!*#) Are you sure your data is usefull for anything? . Recursion (Lost in trees?) . We solved the red brick problem. But since we have the data already open, let’s have a closer look at the Fire Engine, set number 336-1. . SELECT s.name AS set_name, s.year, t.id, t.name AS theme_name, t.parent_id FROM sets s, themes t WHERE t.id = s.theme_id AND s.set_num = &#39;336-1&#39;; . set_name year id theme_name parent_id . Fire Engine | 1968 | 376 | Fire | 373.0 | . The fire engine is quiet old (from 1968) and it belongs to the theme Fire. The themes table also includes as column called parent_id. This suggests that themes is a hierarchical structure. We can check this with an recursive WITH-clause in SQL. (*BQ: recursive WITH is not implemented in BIGQUERY. . WITH RECURSIVE hier(name, parent_id, level) AS ( -- init recursion SELECT t.name, t.parent_id, 0 AS level FROM themes t WHERE id = 376 UNION ALL -- recursive call SELECT t.name, t.parent_id, h.level +1 FROM themes t, hier h WHERE t.id = h.parent_id ) SELECT COUNT(1) OVER() - level AS level, name as theme, GROUP_CONCAT(name,&#39; --&gt; &#39;) over(order by level desc) path FROM hier ORDER BY level; . level theme path . 1 | Classic | Classic | . 2 | Vehicle | Classic –&gt; Vehicle | . 3 | Fire | Classic –&gt; Vehicle –&gt; Fire | . OK, that looks like a reasonable hierarchie. The path column includes the parents and grant parents of a theme. What if we want to reverse the order of the path. Unfortunately GROUP_CONCAT in SQLite doesn’t allow us to specify a sort order in the aggregation. It’s possible to add custom aggregation function in some databases. In SQLite we can compile application defined function or in Oracle we can define customized aggregation function even at runtime as types. . Quiet some steps need to be taken to make the database use costumized aggregation efficently, so we can use them like regulare aggregation and windowing function. In Oracle for instance we have to define: . initial values: total := 0; n := 0; . | calculation per iteration step: total := total + this_step_value; n := n + 1; . | deletion per iteration for windowing: total := total - removed_step_value; n := n - 1; . | merging for parallel execution: total := total_worker1 + total_worker2; n := n_worker1 + n_worker2; . | termination: my_avg := total / nullif(n-1, 0) . | Now, how are we gonna do this in Pandas? We start by traversing the hierarchie. . fire_engine_info = df_themes[df_themes[&#39;id&#39;] == 376].copy() fire_engine_info[&#39;level&#39;] = 0 parent_id = fire_engine_info.parent_id.values[0] lvl = 0 while not np.isnan(parent_id) and lvl &lt; 10: lvl+= 1 new_info = df_themes[df_themes[&#39;id&#39;] == parent_id].copy() new_info[&#39;level&#39;] = lvl parent_id = new_info.parent_id.values[0] fire_engine_info = fire_engine_info.append(new_info) fire_engine_info[&#39;grp&#39;]=0 fire_engine_info .   id name parent_id level grp . 375 | 376 | Fire | 373.0 | 0 | 0 | . 372 | 373 | Vehicle | 365.0 | 1 | 0 | . 364 | 365 | Classic | NaN | 2 | 0 | . On the one hand this is pretty need, since we can do what ever we want in a manually coded loop. On the other hand I doubt that it is very efficent when we have to deal with lots of data. But to be fair, Recursive WITH isn’t that fast either in SQL. . Finaly we consider how to do customized aggregation. We could do it in the loop above or we can rather use the library’s transform or apply functions. . We define a custom aggregation function cat_sorted and then use the apply function like this: . def cat_sorted(ser, df, val_col = None, sort_col = None): u=&#39; --&gt; &#39;.join(df[df.id.isin(ser)].sort_values(sort_col)[val_col].values) return [u] . fire_engine_info.apply(lambda x: cat_sorted(x, fire_engine_info, &#39;name&#39;, &#39;level&#39;)) .   id name parent_id level grp . 0 | Fire –&gt; Vehicle –&gt; Classic |   | Vehicle –&gt; Classic |   |   | . We can also apply rolling or windowing behaviour. . Note that a rolling representation or windowing on string values is not possible because Pandas only allows numeric values for those action. . fire_engine_info.rolling(10,min_periods=1)[&#39;level&#39;].apply(lambda x: sum(10**x), raw = False) . 375 1.0 372 11.0 364 111.0 Name: level, dtype: float64 . Now, we not only understand the numbers on the lego package but also have a better understandig of Pandas. . Summary (Got it!) . SQL stays my favourite language to access structured data arranged over many tables. Pandas shines when data already is gathered together and easily accessable (e.g. as csv file). There are alternatives to Pandas to build ml pipelines, such as Dask or CUDF. But learning Pandas is a good foundation to learn more of them. . Resources . To play with the examples: . Res. 1 Kaggle notebook: https://www.kaggle.com/joatom/a-handful-of-bricks-from-sql-to-pandas | Res. 2 Docker container: https://github.com/joatom/blog-resources/tree/main/handful_bricks | . References . Ref. 1 Pandas SQL comparison: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html | Ref. 2 The Lego dataset: https://www.kaggle.com/rtatman/lego-database | .",
            "url": "https://joatom.github.io/ai_curious/sql/pandas/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "relUrl": "/sql/pandas/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "date": " • Dec 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! . I’m Johannes and this is my hobby blog. The content is limited to topics I’m curious in my spare time. Don’t expect to find here anything related to my job. . I’m happy when you leave a comment on blog posts you like. . Cheers, . Johannes . . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joatom.github.io/ai_curious/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joatom.github.io/ai_curious/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}