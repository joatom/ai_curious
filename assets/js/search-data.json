{
  
    
        "post0": {
            "title": "Learning along",
            "content": ". It’s said, that the most effective way to learn on kaggle is to participate in a competition and afterwards wrangling the top solutions. At the current state (as of January 2022) of my ML skills this approach doesn’t work well for me. . Top notebook solutions are often complicated, contain implementations that are difficult to grasp or are made out of tons of ensembled models. Solutions in the discussion section contain broad overviews that are inspiring, yet hard to rebuild. | The solutions are often different from my own solution. It is difficult to compare them with my own code and to derive how to improve my own code. | The second best way to learn on kaggle is to follow the discussions and read the notebooks during a competition. This approach suites me a better, since there are often small insights to be discovered that are shared by others and that I can easily integrated in my solution. But filtering the valuable information can be hard because the notebook and discussion sections are often flooded with similar content. It is also easy to be distracted by too many different techniques and ideas. . I often enjoy the Playground competitions to focus on a few skills to improve. It is also less challenging to get a baseline implementation up running because the data is prepared to quickly get started. . In January 2022 there is a community competition hosted by Abhishek Thakur, that provides the opportunity for yet another learning approach. During the competition there are sessions on YouTube with two Grandmasters covering the topics EDA and Imputation. So we get high quality guidance for two important topics while working on the competition. I took the opportunity to learn along. .",
            "url": "https://joatom.github.io/ai_curious/tabular/learning/2022/01/31/learn_along.html",
            "relUrl": "/tabular/learning/2022/01/31/learn_along.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Notes about Sequence Modelling",
            "content": "Notes about Sequence Modelling . I lately participated in the Google Brain - Ventilator Pressure Prediction competition. I didn&#39;t score decent, but I still learned a lot. And some of it is worth to sum up, so I can easily look it up later. . The goal of the competition was to predict airway pressure of lungs that are ventilated in a clinician-intensive procedure. Given values of the input pressure (u_in) we had to predict the output pressure for a time frame of a few seconds. . &lt;AxesSubplot:xlabel=&#39;time_step&#39;, ylabel=&#39;Pressure&#39;&gt; . Since all u_in values for a time frame were given we can build a bidirectional sequence model. Unless in a typical time-series problem where the future points are unknown at a certain time step, we know the future and past input values. Therefore I decided not to mask the sequences while training. . A good model choice for sequencing tasks are LSTMs and Transformers. I built a model that combines both architectures. I also tried XGBoost with a lot of features (especially windowing, rolling, lead, lag features) engineering, But neural nets (NN) performed better, here. Though I kept some of the engineered features as embeddings for the NN model. . The competition metric was mean average error (MAE). Only those pressures were evaluated, that appear while filling the lungs with oxygen. . Feature engineering . Besides the given features, u_in, u_out, R, C and time_step I defined several features. They can by categorized as: . area (accumulation of u_in over time) from this notebook | one hot encoding of ventilator parameters R and C | statistical (mean, max, skewness, quartiles, rolling mean, ...) | shifted input pressure | input pressure performance over window | inverse features | . To reduce memory consumption I used a function from this notebook. . def gen_features(df, norm=False): # area feature from https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153 df[&#39;area&#39;] = df[&#39;time_step&#39;] * df[&#39;u_in&#39;] df[&#39;area_crv&#39;] = (1.5-df[&#39;time_step&#39;]) * df[&#39;u_in&#39;] df[&#39;area&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;area&#39;].cumsum() df[&#39;area_crv&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;area_crv&#39;].cumsum() df[&#39;area_inv&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;area&#39;].transform(&#39;max&#39;) - df[&#39;area&#39;] df[&#39;ts&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;id&#39;].rank().astype(&#39;int&#39;) df[&#39;R4&#39;] = 1/df[&#39;R&#39;]**4 df[&#39;R&#39;] = df[&#39;R&#39;].astype(&#39;str&#39;) df[&#39;C&#39;] = df[&#39;C&#39;].astype(&#39;str&#39;) df = pd.get_dummies(df) for in_out in [0,1]: #,1 for qs in [0.2, 0.25, 0.5, 0.9, 0.95]: df.loc[:, f&#39;u_in_{in_out}_q{str(qs*100)}&#39;] = 0 df.loc[df.u_out==in_out, f&#39;u_in_{in_out}_q{str(qs*100)}&#39;] = df[df.u_out==in_out].groupby(&#39;breath_id&#39;)[&#39;u_in&#39;].transform(&#39;quantile&#39;, q=0.2) for agg_type in [&#39;count&#39;, &#39;std&#39;, &#39;skew&#39;,&#39;mean&#39;, &#39;min&#39;, &#39;max&#39;, &#39;median&#39;, &#39;last&#39;, &#39;first&#39;]: df.loc[:,f&#39;u_out_{in_out}_{agg_type}&#39;] = 0 df.loc[df.u_out==in_out, f&#39;u_out_{in_out}_{agg_type}&#39;] = df[df.u_out==in_out].groupby(&#39;breath_id&#39;)[&#39;u_in&#39;].transform(agg_type) if norm: df.loc[:,f&#39;u_in&#39;] = (df.u_in - df[f&#39;u_out_{in_out}_mean&#39;]) / (df[f&#39;u_out_{in_out}_std&#39;]+1e-6) for s in range(1,8): df.loc[:,f&#39;shift_u_in_{s}&#39;] = 0 df.loc[:,f&#39;shift_u_in_{s}&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;u_in&#39;].shift(s) df.loc[:,f&#39;shift_u_in_m{s}&#39;] = 0 df.loc[:,f&#39;shift_u_in_m{s}&#39;] = df.groupby(&#39;breath_id&#39;)[&#39;u_in&#39;].shift(-s) df.loc[:,&#39;perf1&#39;] = (df.u_in / df.shift_u_in_1).clip(-2,2) df.loc[:,&#39;perf3&#39;] = (df.u_in / df.shift_u_in_3).clip(-2,2) df.loc[:,&#39;perf5&#39;] = (df.u_in / df.shift_u_in_5).clip(-2,2) df.loc[:,&#39;perf7&#39;] = (df.u_in / df.shift_u_in_7).clip(-2,2) df.loc[:,&#39;perf1&#39;] = df.perf1-1 df.loc[:,&#39;perf3&#39;] = df.perf3-1 df.loc[:,&#39;perf5&#39;] = df.perf5-1 df.loc[:,&#39;perf7&#39;] = df.perf7-1 df.loc[:,&#39;perf1inv&#39;] = (df.u_in / df.shift_u_in_m1).clip(-2,2) df.loc[:,&#39;perf3inv&#39;] = (df.u_in / df.shift_u_in_m3).clip(-2,2) df.loc[:,&#39;perf5inv&#39;] = (df.u_in / df.shift_u_in_m5).clip(-2,2) df.loc[:,&#39;perf7inv&#39;] = (df.u_in / df.shift_u_in_m7).clip(-2,2) df.loc[:,&#39;perf1inv&#39;] = df.perf1inv-1 df.loc[:,&#39;perf3inv&#39;] = df.perf3inv-1 df.loc[:,&#39;perf5inv&#39;] = df.perf5inv-1 df.loc[:,&#39;perf7inv&#39;] = df.perf7inv-1 df.loc[:,&#39;rol_mean5&#39;] = df.u_in.rolling(5).mean() return df . Scaler . The data was transformed with scikit&#39;s RobustScaler to reduce influence of outliers. . features = list(set(train.columns)-set([&#39;id&#39;,&#39;breath_id&#39;,&#39;pressure&#39;,&#39;kfold_2021&#39;,&#39;kfold&#39;])) features.sort() rs = RobustScaler().fit(train[features]) . Folds . I didn&#39;t do cross validation here, but instead trained the final model on the entire dataset. Nevertheless it&#39;s helpful to build kfolds for model evaluation. I build GroupKFold over breath_id to keep the entire time frame in the same fold. . Dataloader . Since the data is quite small (ca. 800 MB after memory reduction) I decided to load the entire train set in the Dataset object during construction (calling __init__()). In a first attempt I loaded the data as Pandas Dataframe. Then I figured out (from this notebook) that converting the Dataframe into an numpy array speeds up training significantly. The Dataframe is converted to an numpy array by the scaler. . Since the competition metric only evaluates the pressures where u_out==0 I also provide a mask tensor, which can later on be used feeding the loss and metric functions. . class VPPDataset(torch.utils.data.Dataset): def __init__(self,df, scaler, is_train = True, kfolds = [0], features = [&#39;R&#39;,&#39;C&#39;, &#39;time_step&#39;, &#39;u_in&#39;, &#39;u_out&#39;]): if is_train: # build a mask for metric and loss function self.mask = torch.FloatTensor(1 - df[df[&#39;kfold&#39;].isin(kfolds)].u_out.values.reshape(-1,80)) self.target = torch.FloatTensor(df[df[&#39;kfold&#39;].isin(kfolds)].pressure.values.reshape(-1,80)) # calling scaler also converts the dataframe in an numpy array, which results in speed up while training feature_values = scaler.transform(df[df[&#39;kfold&#39;].isin(kfolds)][features]) self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features))) else: self.mask = torch.FloatTensor(1 - df.u_out.values.reshape(-1,80)) feature_values = scaler.transform(df[features]) self.df = torch.FloatTensor(feature_values.reshape(-1,80,len(features))) self.target = None self.features = features self.is_train = is_train def __len__(self): return self.df.shape[0] def __getitem__(self, item): sample = self.df[item] mask = self.mask[item] if self.is_train: targets = self.target[item] else: targets = torch.zeros((1)) return torch.cat([sample, mask.view(80,1)],dim=1), targets #.float() . Model . My model combines a multi layered LSTM and a Transformer Encoder. Additionally I build an AutoEncoder by placing a Transformer Decoder on top of the Transformer encoder. The AutoEncoder predictions are used as auxiliary variables. . . Some further considerations: . I did not use drop out. The reason why it performence worse is discussed here. | LayerNorm can be used in sequential models but didn&#39;t improve my score. | . The model is influenced by these notebooks: . Transformer part | LSTM part | Parameter initialization | . # Transformer: https://pytorch.org/tutorials/beginner/transformer_tutorial.html # LSTM: https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm # Parameter init from: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization class VPPEncoder(nn.Module): def __init__(self, fin = 5, nhead = 8, nhid = 2048, nlayers = 6, seq_len=80, use_decoder = True): super(VPPEncoder, self).__init__() self.seq_len = seq_len self.use_decoder = use_decoder # number of input features self.fin = fin #self.tail = nn.Sequential( # nn.Linear(self.fin, nhid), # #nn.LayerNorm(nhid), # nn.SELU(), # nn.Linear(nhid, fin), # #nn.LayerNorm(nhid), # nn.SELU(), # #nn.Dropout(0.05), #) encoder_layers = nn.TransformerEncoderLayer(self.fin, nhead, nhid , activation= &#39;gelu&#39;) self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers) decoder_layers = nn.TransformerDecoderLayer(self.fin, nhead, nhid, activation= &#39;gelu&#39;) self.transformer_decoder = nn.TransformerDecoder(decoder_layers, nlayers) self.lstm_layer = nn.LSTM(fin, nhid, num_layers=3, bidirectional=True) # Head self.linear1 = nn.Linear(nhid*2+fin , seq_len*2) self.linear3 = nn.Linear(seq_len*2, 1) self._reinitialize() # from https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization def _reinitialize(self): &quot;&quot;&quot; Tensorflow/Keras-like initialization &quot;&quot;&quot; for name, p in self.named_parameters(): if &#39;lstm&#39; in name: if &#39;weight_ih&#39; in name: nn.init.xavier_uniform_(p.data) elif &#39;weight_hh&#39; in name: nn.init.orthogonal_(p.data) elif &#39;bias_ih&#39; in name: p.data.fill_(0) # Set forget-gate bias to 1 n = p.size(0) p.data[(n // 4):(n // 2)].fill_(1) elif &#39;bias_hh&#39; in name: p.data.fill_(0) elif &#39;fc&#39; in name: if &#39;weight&#39; in name: nn.init.xavier_uniform_(p.data,gain=3/4) elif &#39;bias&#39; in name: p.data.fill_(0) def forward(self, x): out = x[:,:,:-1] out = out.permute(1,0,2) out = self.transformer_encoder( out) out_l,_ = self.lstm_layer(out) if self.use_decoder: out = self.transformer_decoder(out, out) out_dec_diff = (out - x[:,:,:-1].permute(1,0,2)).abs().mean(dim=2) else: out_dec_diff = out*0 out = torch.cat([out, out_l], dim=2) # Head out = F.gelu(self.linear1(out.permute(1,0,2))) out = self.linear3(out) return out.view(-1, self.seq_len) , x[:,:,-1], out_dec_diff.view(-1, self.seq_len) . Metric and Loss Function . Masked MAE metric . The competition metric was Mean Absolute Error (MAE), but only for the time-steps where air flows into the lunge (approx. half of the timesteps). Hence, I masked the predictions (using the flag introduced in the Dataset) ignoring the unnecessary time-steps. The flags are passed through the model (val[1]) and is an output along with the predictions. . def vppMetric(val, target): flag = val[1] preds = val[0] loss = (preds*flag-target*flag).abs() loss= loss.sum()/flag.sum() return loss . Thee values produced by the AutoGenerater are additionally measured by the vppGenMetric. It uses MAE to evaluate how good the reconstruction of the input features values evolves. . def vppGenMetric(val, target): gen =val[2] flag = val[1] loss = (gen*flag).abs() loss= loss.sum()/flag.sum() return loss . Combined Loss function . The loss function is a combination of L1-derived-Loss (vppAutoLoss) for the predictions and the AutoEncoder-predictions. . Due to this discussion I did some experiments with variations of Huber and SmoothL1Loss. The later (vppAutoSmoothL1Loss) performed better. . def vppAutoLoss(val, target): gen =val[2] flag = val[1] preds = val[0] loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2 # loss= loss.sum()/flag.sum() return loss # Adapting https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss def vppAutoSmoothL1Loss(val, target): beta = 2 fct = 0.5 gen =val[2] flag = val[1] preds = val[0] loss = (preds*flag-target*flag).abs() + (gen*flag).abs()*0.2 loss = torch.where(loss &lt; beta, (fct*(loss**2))/beta, loss)#-fct*beta) # reduction mean**0.5 loss = loss.sum()/flag.sum() #()**0.5 return loss . Training . The training was done in mixed precision mode (to_fp16()) to speed up training. As optimizer I used QHAdam. The best single score I achieved with a 100 epoch fit_one_cycle (CosineAnnealing with warmup). I also tried more epochs with restart schedules fit_sgdr and changing loss functions. But the didn&#39;t do better. . learn = Learner(data, VPPEncoder(fin = len(features), nhead = 5, nhid = 128, nlayers = 6, seq_len=80, use_decoder = True), opt_func= QHAdam, loss_func = vppAutoLoss, #vppAutoSmoothL1Loss metrics=[vppMetric, vppGenMetric], cbs=[ShowGraphCallback()]).to_fp16() learn.fit_one_cycle(EPOCHS, 2e-3) . epoch train_loss valid_loss vppMetric vppGenMetric time . 0 | 2.204789 | 3.071832 | 2.759562 | 1.561353 | 02:33 | . 1 | 1.472645 | 1.427370 | 1.147190 | 1.400903 | 02:34 | . 2 | 1.130861 | 1.373204 | 1.108116 | 1.325440 | 02:32 | . 3 | 0.864211 | 0.880435 | 0.627816 | 1.263098 | 02:32 | . 4 | 0.788929 | 0.765516 | 0.516083 | 1.247169 | 02:37 | . Th CV score after 100 epochs is 0.227013 for vppMetric and 0.255794 for vppGenMetric. Leaderboard scores for this singele model training in the entire train-dataset were 0.2090 private LB and 0.2091 public LB. . References . Competition Home Page: https://www.kaggle.com/c/ventilator-pressure-prediction | Area features: https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153 | Memory reduction: https://www.kaggle.com/konradb/tabnet-end-to-end-starter | Dataloader from numpy: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization | Dropout discussion: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/276719 | Transformer model: https://pytorch.org/tutorials/beginner/transformer_tutorial.html | LSTM model: https://www.kaggle.com/theoviel/deep-learning-starter-simple-lstm | LSTM parameter initialization: https://www.kaggle.com/junkoda/pytorch-lstm-with-tensorflow-like-initialization | SmoothL1Loss discussion: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/277690 | .",
            "url": "https://joatom.github.io/ai_curious/timeseries/transformer/lstm/2021/11/22/vpp-seq.html",
            "relUrl": "/timeseries/transformer/lstm/2021/11/22/vpp-seq.html",
            "date": " • Nov 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Model allocation",
            "content": "This notebook was originally published on https://www.kaggle.com/joatom/model-allocation. . In this notebook I experiment with two ensembling strategies. . There are many ways to combine different models to improve predictions. A common technique for regression tasks is taking a weighted average of the model predictions (y_pred = (m1(x)*w1 + ... + mn(x)*wn) / n). Another common technique is building a meta model, that is trained on the models&#39; outputs. . The first chapter starts with a simple linear combination of two models. And we explore with an simple example, why ensembling actually works. These insights will lead, in the second chapter, to the first technique on how to choose weights for a linear ensemble by using residual variance. In the third chapter an alternative for the weight selection is examined. This second technique is inspired by portfolio theory (a theory to combine financial assets). In the fourth chapter the two techniques are applied and compared on the Tabular Playground Series (TPS) - Aug 2021 competition. Finaly cross validation (CV) and leaderboard (LB) Scores are listed in the fith chapter. . Note: For the ease of explanation we make some simplifying assumptions, such as equal distribution of the data, same distribution on unseen data, ... (just think of a vanilla world). . 1. Why ensembling works . Suppose there are two fitted regression models and they predict values like shown in the first chart. . To get a better intuition on how good the two models fit the ground truth, we plot the residuals y_true(x)-m(x). . If we had to choose one of the models, which one would we prefer? Model 2 does better on the first data point and perfect on the third, but it contains an outlier the 5th data point. . Let&#39;s look at the mean and the variance of the residuals. . Model #1. mean: 0.0714, var: 1.6020 Model #2. mean: 0.0000, var: 2.0000 . On the long run Model2 has an average residual of 0. Model 1 carries along a residual of 0.0714. So on average Model 2 seams to do better. . But Model 2 also has a higher variance. That implies we have a great chance to do a great prediction (e.g. x=3) but we also have high risk to screw the prediction (e.g. x=5). . Now we build a simple linear ensemble of the two models like ens = 0.5 * m1 + 0.5 m2. . The ensemble line is closer to the true values. It also looks smoother then m1 and m2. . In the residual chart we can see that the ensemble does a bit worse for x=3 compared to Model 2. But it also decreases the residuals for the outliers (points 1, 5, 7). . Let&#39;s check the stats: . Ensemble. mean: 0.0357, var: 0.2219 . We dramatically reduced the variance, hence reduced the risk/chance. The mean value is now in between Model 1 and Model 2. . Finally let&#39;s play around with the model weights in the ensemble and check how mean and variance change. . weight_m1 = np.linspace(0, 1, 30) ens_mean = np.zeros(30) ens_var = np.zeros(30) for i, w1 in enumerate(weight_m1): # build ensemble for different weights ens = m1*w1 + m2*(1-w1) ens_res = y_true - ens # keep track of mean and var of the differently weighted ensembles ens_mean[i] = ens_res.mean() ens_var[i] = ens_res.var() . With the previous 50:50 split the variance seems almost at the lowest point. So we only get a reduction of the mean below 0.0357 if we allow the ensemble to have more variance, hence take more risk. . 2. Weights by residual variance . Since the Model 1 and Model 2 are well fitted, their average residuals are pretty close to 0. So let&#39;s focus on reducing our variance to avoid surprises on later later predictions. . We now solve for the optimal weights that minimizes the variance of the residual of our ensemble with this function: . fun = lambda w: (y_true-np.matmul(w, preds)).var() . We also define a constraint so that the w.sum() == 0: . cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda w: w.sum()-1}) . If you want, you can also set bounds, so that the weights want be negative. . I don&#39;t. I like the idea of going short with a model. And negative weights really increase the results of TPS predictions in chapter 4. . bnds = ((0,None), (0,None), (0,None), (0,None), (0,None)) . Now, we are all set to retrieve the optimal weights. . preds = np.array([m1, m2]) # init weights w_init = np.ones(preds.shape[0])/preds.shape[0] # run optimization res = scipy.optimize.minimize(fun, w_init, method=&#39;SLSQP&#39;, constraints=cons) #,bounds=bnds # get optimal weights w_calc = res.x print(f&#39;Calculated weights: {w_calc}&#39;) . Calculated weights: [0.53150242 0.46849758] . Let&#39;s see how the calculated weights perform. . ens_ex1 = np.matmul(w_calc, preds) ens_ex1_res=y_true-ens_ex1 print(f&#39;Ensemble Ex1. mean: {ens_ex1_res.mean(): .4f}, var: {ens_ex1_res.var(): .4f}&#39;) . Ensemble Ex1. mean: 0.0380, var: 0.2157 . We con compare the results with the first ensemble 50:50 split. With the calculated weights we could further reduce the variance of the model (0.2219 -&gt; 0.2157). But unfortunately the mean increased a bit (0.0357 -&gt; 0.0380). . We see the trade off between mean and variance and have to decide if we prefer a more stable model or take some risk for better results. . 3. Portfolio theory for ensembling . In finance different assets are often combined in a portfolio. There are many criteria for the asset selection/allocation. One of them is by choosing a risk strategy. In 1952 the economist Harry Markowitz defined a Portfolio Selection strategy which built the foundation of many portfolio strategies to come. There is a great summary on Wikipidia, but the original paper can also be found with a google search. . So, what it is all about. Let&#39;s assume we are living in an easy, plain vanilla world. We want to build a portfolio that yields high return with low risk. That&#39;s not easy. If we only buy stocks of our favorite fruit grower, a rainy summer would result in a low return. Wouldn&#39;t it be smart to also buy stocks of a raincoat producer, just in case. But what if the summer was sunny, then we would have rather invested the entire money in fruits instead of raincoats. It&#39;s clearly a trade off. Either we lower the risk of loosing money in a rainy summer and invest in both (fruits and raincoats). Or we take the risk investing all money in fruits to maybe gain more money. And if we lower the risk, in which raincoat producer should we invest? The one with the bumpy stock price or the one with a steady, but slowly growing stock price. . Now, we already see the first similarities between our ensemble example above and the Portfolio Theory. Risk can be measured through variance and a good return of our ensemble is results in a low expected residual. . But there is even more in Portfolio Theory. It also takes dependencies between assets into account. If the summer is sunny the fruit price goes up and the raincoat price goes down, they are somewhat negative correlated. . Since we expect the average residual of our fitted models to be close to 0 and we build a linear model, we can expect our ensemble average residual also to be close to 0. Therefore, we focus on optimizing the portfolio variance, which can be boiled down to Var_p = w&#39;*Cov*w. The covariance measures the dependency between combined models and also considers the variance. . What data can we actually use? In the financial example returns are the increase or decrease of an asset price (p/p_t-1), hence we are looking on returns for a certain period of time. In ML we can take our out-of-fold (oof) predictions and calculate the residuals from the train targets to build a dataset. . Can we do this despite we are looking at a time-series in the financial example? Yes, in this basic portfolio theory we don&#39;t take time dependencies into account. But it&#39;s important to keep the same order for the different asset returns for correlation/covariance calculation. We want to compare the residual of model 1 and 2 for always the same data item. . The optimization function for the second ensemble technique is: . # Predictions of Model 1 and Model 2 preds = np.array([m1,m2]) # Residuals of Model 1 and Model 2 preds_res = np.array([m1_res, m2_res]) # handle residuals like asset returns R = np.array(preds_res.mean(axis=1)) # factor by which R is considered during optimization. turned off for our example q = 0 #-1 # covariance matrix of model residuals CM = np.cov(preds_res) # optimization function fun = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w) # constraint: weights must sum up to 1.0 cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda x: x.sum()-1}) . Run the optimization. . w_init = np.ones(preds.shape[0])/preds.shape[0] # run optimization res = scipy.optimize.minimize(fun, w_init, method=&#39;SLSQP&#39;, constraints=cons) #,bounds=bnds # get optimal weights w_calc = res.x print(f&#39;Calculated weights: {w_calc}&#39;) . Calculated weights: [0.53150242 0.46849758] . The weights are the same as in the first technique. That really surprised me. And I run a couple of examples with different models. But the weights were only slightly different between the two techniques. . 4. Ensembling TPS Aug 2021 . Now that we have to techniques to ensemble, let&#39;s try them on the TPS August 2021 data. . We do a 7 kfold split and calculate the residuals on the out-of-fold-predictions, that are used for validation. We train 7 regression models with different architecture so we get some diversity. . N_SPLITS = 7 SEED = 2021 PATH_INPUT = &#39;/home/kaggle/TPS-AUG-2021/input/&#39; . test = pd.read_csv(PATH_INPUT + &#39;test.csv&#39;) train = pd.read_csv(PATH_INPUT + &#39;train.csv&#39;).sample(frac=1.0, random_state = SEED).reset_index(drop=True) train[&#39;fold_crit&#39;] = train.loss train.loc[train.loss&gt;=39, &#39;fold_crit&#39;]=39 . target = &#39;loss&#39; fold_crit = &#39;fold_crit&#39; features = list(set(train.columns)-set([&#39;id&#39;,&#39;kfold&#39;,&#39;loss&#39;,&#39;fold_crit&#39;]+[target])) . skf = StratifiedKFold(n_splits = N_SPLITS, random_state = None, shuffle = False) train.kfold = -1 for f, (train_idx, valid_idx) in enumerate(skf.split(X = train, y = train[fold_crit].values)): train.loc[valid_idx,&#39;kfold&#39;] = f train.groupby(&#39;kfold&#39;)[target].count() . kfold 0.0 35715 1.0 35715 2.0 35714 3.0 35714 4.0 35714 5.0 35714 6.0 35714 Name: loss, dtype: int64 . models = { &#39;LinReg&#39;: LinearRegression(n_jobs=-1), &#39;HGB&#39;: HistGradientBoostingRegressor(), &#39;XGB&#39;: XGBRegressor(tree_method = &#39;gpu_hist&#39;, reg_lambda= 6, reg_alpha= 10, n_jobs=-1), &#39;KNN&#39;: KNeighborsRegressor(100, n_jobs=-1), &#39;BayesRidge&#39;: BayesianRidge(), &#39;ExtraTrees&#39;: ExtraTreesRegressor(max_depth=2, n_jobs=-1), &#39;Poisson&#39;: Pipeline(steps=[(&#39;scale&#39;, StandardScaler()), (&#39;pois&#39;, PoissonRegressor(max_iter=100))]) } . Fit models and save oof predictions. . for (m_name, m) in models.items(): print(f&#39;# Model:{m_name} n&#39;) train[m_name + &#39;_oof&#39;] = 0 test[m_name] = 0 y_oof = np.zeros(train.shape[0]) for f in range(N_SPLITS): train_df = train[train[&#39;kfold&#39;] != f] valid_df = train[train[&#39;kfold&#39;] == f] m.fit(train_df[features], train_df[target]) oof_preds = m.predict(valid_df[features]) y_oof[valid_df.index] = oof_preds print(f&#39;Fold {f} rmse: {mean_squared_error(valid_df[target], oof_preds, squared = False):0.5f}&#39;) test[m_name] += m.predict(test[features]) / N_SPLITS train[m_name + &#39;_oof&#39;] = y_oof print(f&quot; nTotal rmse: {mean_squared_error(train[target], train[m_name + &#39;_oof&#39;], squared = False):0.5f} n&quot;) oof_cols = [m_name + &#39;_oof&#39; for m_name in models.keys()] print(f&quot;# ALL Mean ensemble rmse: {mean_squared_error(train[target], train[oof_cols].mean(axis=1), squared = False):0.5f} n&quot;) . # Model:LinReg Fold 0 rmse: 7.89515 Fold 1 rmse: 7.90212 Fold 2 rmse: 7.90260 Fold 3 rmse: 7.89748 Fold 4 rmse: 7.89844 Fold 5 rmse: 7.89134 Fold 6 rmse: 7.89643 Total rmse: 7.89765 # Model:HGB Fold 0 rmse: 7.86447 Fold 1 rmse: 7.87374 Fold 2 rmse: 7.86688 Fold 3 rmse: 7.86255 Fold 4 rmse: 7.86822 Fold 5 rmse: 7.85785 Fold 6 rmse: 7.86566 Total rmse: 7.86563 # Model:XGB Fold 0 rmse: 7.91179 Fold 1 rmse: 7.92748 Fold 2 rmse: 7.92141 Fold 3 rmse: 7.91901 Fold 4 rmse: 7.91125 Fold 5 rmse: 7.90286 Fold 6 rmse: 7.92340 Total rmse: 7.91675 # Model:KNN Fold 0 rmse: 7.97845 Fold 1 rmse: 7.97709 Fold 2 rmse: 7.98165 Fold 3 rmse: 7.97895 Fold 4 rmse: 7.97781 Fold 5 rmse: 7.97798 Fold 6 rmse: 7.98711 Total rmse: 7.97986 # Model:BayesRidge Fold 0 rmse: 7.89649 Fold 1 rmse: 7.90576 Fold 2 rmse: 7.90349 Fold 3 rmse: 7.90007 Fold 4 rmse: 7.90121 Fold 5 rmse: 7.89455 Fold 6 rmse: 7.89928 Total rmse: 7.90012 # Model:ExtraTrees Fold 0 rmse: 7.93239 Fold 1 rmse: 7.93247 Fold 2 rmse: 7.92993 Fold 3 rmse: 7.93121 Fold 4 rmse: 7.93129 Fold 5 rmse: 7.93247 Fold 6 rmse: 7.93364 Total rmse: 7.93191 # Model:Poisson Fold 0 rmse: 7.89597 Fold 1 rmse: 7.90240 Fold 2 rmse: 7.90233 Fold 3 rmse: 7.89682 Fold 4 rmse: 7.89873 Fold 5 rmse: 7.89241 Fold 6 rmse: 7.89701 Total rmse: 7.89795 # ALL Mean ensemble rmse: 7.88061 . Let&#39;s a look at the correlation heatmap. . oof_cols = [m_name + &#39;_oof&#39; for m_name in models.keys()] oofs = train[oof_cols] oof_diffs = oofs.copy() for c in oof_cols: oof_diffs[c] = oofs[c]-train[target] oof_diffs[c] = oof_diffs[c]#**2 sns.heatmap(oof_diffs.corr()) . &lt;AxesSubplot:&gt; . XGB and KNN are most diverse, so I export a 50:50 ensemble. I&#39;ll also export an equally weighted ensemble of all models and HGB only because it is the best single model. . CV: ALL equaly weighted: 7.880605075536334 CV: XGB only: 7.916746570344035 CV: HGB only: 7.865625158180185 CV: XGB and LinReg (50:50): 7.872064005057903 CV: XGB and KNN (50:50): 7.893210466099108 . Next we inspect the variance and mean of the residuals. Means are close to 0, as expected. . oof_diffs.var(), oof_diffs.mean() . (LinReg_oof 62.373163 HGB_oof 61.868296 XGB_oof 62.675125 KNN_oof 63.678431 BayesRidge_oof 62.412188 ExtraTrees_oof 62.915511 Poisson_oof 62.377910 dtype: float64, LinReg_oof -0.000055 HGB_oof -0.003314 XGB_oof 0.001392 KNN_oof -0.005395 BayesRidge_oof -0.000024 ExtraTrees_oof -0.000136 Poisson_oof -0.000084 dtype: float64) . These are the histograms of the residuals: . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;LinReg_oof&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;HGB_oof&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;XGB_oof&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;KNN_oof&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;BayesRidge_oof&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;ExtraTrees_oof&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Poisson_oof&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . Finally, we apply the two techniques to calculate the ensembling weights . R = oof_diffs.mean().values CM = oof_diffs.cov().values q=0 # Var technique fun_ex1 = lambda w: (train[target]-np.matmul(oofs.values, w)).var() # Cov technique fun_ex2 = lambda w: np.matmul(np.matmul(w.T,CM),w) - q * np.matmul(R,w) cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda x: x.sum()-1}) bnds = ((0,None), (0,None), (0,None), (0,None), (0,None)) . w_init = np.ones((len(models)))/len(models) res = scipy.optimize.minimize(fun_ex1, w_init, method=&#39;SLSQP&#39;, constraints=cons) #,bounds=bnds w_calc = res.x . CV: Ex1 calc weights: 7.85594426240217 . w_init = np.ones((len(models)))/len(models) res = scipy.optimize.minimize(fun_ex2, w_init, method=&#39;SLSQP&#39;, constraints=cons) #,bounds=bnds w_calc = res.x . CV: Ex2 calc weights: 7.855944262936231 . 5. Results . The competition metric is root mean squared error (RMSE). These are the scores of the different ensembles: . Ensemble CV public LB . HGB only | 7.86563 | 7.90117 | . All weights eq. | 7.88061 | 7.92183 | . XGB and KNN (50:50) | 7.89321 | 7.91603 | . Ex1 (Var) | 7.85594 | 7.88876 | . Ex2 (Cov) | 7.85594 | 7.88876 | . References . Modern Portfolio Theory: https://en.wikipedia.org/wiki/Modern_portfolio_theory | TPS August 2021 Competition: https://www.kaggle.com/c/tabular-playground-series-aug-2021/overview | . Ressources . Original notebook: https://www.kaggle.com/joatom/model-allocation | TPS data: https://www.kaggle.com/c/tabular-playground-series-aug-2021/data | .",
            "url": "https://joatom.github.io/ai_curious/ensembling/2021/08/15/model-allocation.html",
            "relUrl": "/ensembling/2021/08/15/model-allocation.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Streaming ML training progress to a smart watch",
            "content": ":mountain_snow: . During the Covid winter I hardly had any reason to leave the house. It was clear that I actively had to look after my mental and physical well-being. So, I decided to buy a smart watch (Fitbit Versa 3) and take on the 10000 steps per day challenge. Henceforth I spent much more time outside and in the sunlight moving my body. . :partly_sunny: . When I registered my new Fitbit I instantly got attracted by the For Developers link. As you might guess my thoughts started spinning like - Aha, they are providing a SDK! I got to try this out at some point and build an app for the watch. - I wanted this app to be related to ML or at least to data somehow. My first thought was - Would it be possible to use the green heart-rate sensor light as an OCR? - Nope, to complicated for a fun project. - Then I went on with the registration of the watch on the webpage. . :sun_behind_small_cloud: . Some month later, on a usual Sunday, I lay resting on the couch after lunch while the kids and my wife were cleaning the table and kitchen (I did the cooking ;-)). A little bored, I thought of my ML-model that was training on Hotel Room classification for a couple of hours upstairs in the office. I wanted to know how it was preceding. Sneaking upstairs would result in half an hour in front of the computer, followed by some trouble with my wife :unamused:. - May by I should eventually register at Neptune.ai or wandb.ai, then I could preview my trainings from the couch on my cell phone!? … Or may be I now have a new fun project for my new watch :smile: :bulb:! - . User story . Those were the requirements that finally got implemented: . Statistics (for metrics and losses) need to be captured during the training, so that I can see if training improves. | Progress of training needs to be captured and made available to the watch instantly, so that I can estimate how long the training will take to finish. | Stats &amp; progs should be processable in pytorch and fastai trainings, so that I can use my preferred ML libraries. | The watch plots the metrics history of the current training as line chart. so that I can quickly see if training improves on which metric. | The metrics values should be plotted, so I can easily compare with former trainings. | Progress on epochs and batches (train and valid) are plotted, so I can easily estimate how long the remaining training steps will last. | . As by-product the training progress can also be displayed on a browser. This feature was build in parallel for debugging purpose. . Architecture . API-Server . The requirements led to the architecture shown in the diagram. In the center of the application is an API-Server to coordinate the training and the watch. The Training, Watch and Web are client applications connected to the API-Server’s Watchtrain-Topic. The Topic contains a connection pool for the Training client (data Producer) and another connection pool for the Web and the Watch clients (data Consumer). . . The initial idea was to setup a classical Consumer/Producer (Pub/Sub) pattern. But it ended up a bit different. The Topic holds the data in an object rather than a queue-like state and also does some data processing. The Producer and Consumer can still subscribe at any time, but they are also strongly connected via Websockets. I took the chance to play around with websockets, since it is also available on the watch. . For each client type there is an Agent that processes the data and messages that are send from the clients. The stats and progress data is saved in the topic. The topic generates the metric chart that is send to the Consumers, since I couldn’t find a charts library in the watch SDK. . The Topic-Consumer-Producer-Agent “pattern” with the connection pool handler is set up in a generic way so it’s easy to develop other applications in the same manner and run them on the API-Server. . As API-Server I used FastApi which is easy to start with as shown on the tutorial site or in this video. . The communications between the components is done with JSON. Messages start with an action-field followed by the training_id and a more or less complex payload. Depending of the action value different functionalities are triggered, such as sending the metric image to the client or converting batch information into a progress bar. . Training . Fastai . The easiest way to implement the train logging is by using the Fastai Callback infrastructure. So I built a WebsocketLogger which gets past to the training like this: . learn = cnn_learner(dls, resnet18, pretrained = False, metrics=[accuracy, Recall(average=&#39;macro&#39;), Precision(average=&#39;macro&#39;)]) learn.unfreeze learn.fit_one_cycle(10, lr_max = 5e-3, cbs=[WebsocketLogger(&#39;ws://myapiserver:8555/ws/watchtrain/producer/12345&#39;)]) . Starting out by looking at the source code of the fastai build-in CSVLoggers and ProgressCallback I learned how to track train data (metrics, epoch and batch progress). A bit challenging was the integration of the websocket client. I preferred a permanent connection rather than many one time (open-send-close) connections. Otherwise a simple REST call would have been more suitable. It is also very important that training must not break when the websocket connection is lost or the API-Server isn’t available anymore. . That’s how it is implemented using the websocket-client library: . def __init__(self, conn, ...): self.conn = conn ... self.heartbeat = False self._ws_connect() ... ... # gets called when a websocket is opened def _on_ws_open(self,ws): # ws connection is now ready =&gt; unlock self.ws_ready_lock.release() self.heartbeat = True def _ws_connect(self): self.heartbeat = False # aquire lock until websocket is ready to use self.ws_ready_lock = threading.Lock() self.ws_ready_lock.acquire() print(&#39;Connecting websocket ...&#39;) self.ws = websocket.WebSocketApp(self.conn, on_open = self._on_ws_open, on_message = self._on_ws_message, on_error = self._on_ws_error, on_close = self._on_ws_close) # run websocket in background thread.start_new_thread(self.ws.run_forever, ()) # wait for websocket to be initialized, # if connection is not possible (e.g. APIServer is down) resume after 3 sec, but heartbeat stays FALSE self.ws_ready_lock.acquire(timeout = 3) print(&#39;... websocket connected.&#39;) . The WebSocketApp runs as a local websocket-handler in the background. The Locks are used to make sure the connection gets properly established before the first messages are send. The heartbeat is introduced to keep the training running even if the websocket connection is broken and could not be reconnected via WebSocketApp. . If there is no heartbeat anymore _ws_connect() is called again after any epoch. If the API-Server is still not reachable the training continuous after a 3 second waiting time. . Pytorch . I skipped the pytorch implementation until I need it. But it is straight forward. Start a WebSocketApp thread in the background. Send the data from inside of the training/validation/inference-loop. . Watch . . The layout is held pretty simple as shown in the picture. There is a progress bar for the epochs and one for the mini batches (train and valid). In the center is the chart of the metrics. And at the bottom are the latest metric values. The cell phone that belongs to the watch establish a websocket connection to the API-Server and puts EventListeners for incoming messages into place. The incoming messages are uploaded to the watch were they can be displayed. . Lessons learned . FastAPI . FastAPI is a well-documented and easy to use framework. In the beginning I set it up with HTTPS. There is a tutorial on how to setup FastAPI with Traefik. But since I wanted to run the server at home I had to invest some evenings to figure out, how to set it up by myself. I used mkcert for SSL creation. A docker file to setup an FastAPI-Server at home can now be found here. At the end when I got it working I decided to not use HTTPS for reasons described below, :man_shrugging:. . Websockets . The different components communicate instantly. The data is pushed to the watch, which is the preferred behavior on the receiving site. With the websocket on the training site it is a bit more complicated to be fail safe and pickup communication when the connection is broken for a longer period of time. I might switch this part to a simple REST-post in a later version. But this way it was a fun exersice nevertheless. . Fitbit SDK . The Fitbit SDK is nice. They provide an online IDE which can easily be connected to your devices. The SDK is documented with a few examples. They also host helpful forum. . I had a bit of a hard time when I tried to load and display the Metrics chart image to the watch. I had to figure out that there are two types of jpeg, progressive and basic. And only one worked. It also was hard to figure out that the the image needs to have a certain size to be displayed. But that’s part of the normal learning path with a new technology. . And than, there was this one thing that really upset me (But as fare as I read in forums it is not the Fitbit SDKs fault!). Android doesn’t allow regular HTTP connection through apps. That’s why I setup the API-Server with HTTPS. But since I generated the certificate on my own, it wasn’t a trusted source and therefore Android didn’t accept it. Then I found some post that showed how to access HTTP from a local net, but only for IP range 192.168.0.x. That meant either building a Reverse Proxy or changing the Subnet of my network. And then finally I needed to deal with the docker net-addresse where the API-Server is running. As suspected, one evening I freaked out - ?#@!, I just want to send a JSON to my cell phone! 30 years of web-development and all we ended up is JavaScript and SSL-certs @!# - That was a good time to go to bed, put the project aside for a few days and celebrate that most of the time I’m into data instead of GUI :grin:. . Besides that I really enjoyed it to build a nice app for my Fitbit. . Conclusion . It was a fun project! Websockets, API, ML, App on a watch, it all fits together. I still have a thousand ideas for improvements and features. But for now I leave it as it is. . Implementation . The code for this project can be found in this repository: https://github.com/joatom/watchtrain . References . Here is a list of my inspirations, templates and useful content listed by topic. . Fitbit SDK . Online IDE (there also is a CLI version for VScode) | Tutorials | . FastAPI . FastAPI’s tutorials | FastAPI and websockets | Docker setup | FastAPI with Traefik | Video Tutorial for using FastAPI to serve ML models | . Training . Fastai’s Datablock tutorial | CSV and Progress logger | . Others . Uvicorn HTTPS options | Mkcert installation guide | .",
            "url": "https://joatom.github.io/ai_curious/api/websockets/ml%20logging/fastai%20callbacks/2021/06/19/watchtrain.html",
            "relUrl": "/api/websockets/ml%20logging/fastai%20callbacks/2021/06/19/watchtrain.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Identifying Hotels",
            "content": "“Recognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked.” (Stylianou et al., 2019). . As part of the Eight Workshop on Fine-Grained Visual Categorization a kaggle competition was launched to support investigations by advancing models to identify hotels from images. . This post contains some parts of my contribution to the Hotel-ID to Combat Human Trafficking 2021 - FGVC8 kaggle competition. . The challenge . The competition contained 97000+ images of hotel rooms from 7700! different hotels around the world. The objective was to identify the hotels of 13000 images from the hidden test set. The metric of the competition was Mean Average Precision of the top 5 picks (MAP@5). My solution scored 14th place out of 92 teams with a 0.6164 MAP@5 on the private leaderboard. . My solution contained six CNN models with various configurations. More technical details and why I ended up with rather simple models is described in a kaggle discussion topic. . Here I post the training and inference of one of the six models as well as the ensemble inference code. . Training . The final training was done on the entire train dataset. I didn’t choose a cross validation strategy to safe training time. To keep variance low nonetheless I relied on the usual regularization strategies, such as dropout and augmentation and in particular on test time augmentation during inference. . To refine the model a validation set can be created by setting the debug flag as described in the notebook. . Inference . Inference ensemble . References . Stylianou, Abby and Xuan, Hong and Shende, Maya and Brandt, Jonathan and Souvenir, Richard and Pless, Robert (2019). Hotels-50K: A Global Hotel Recognition Dataset. The AAAI Conference on Artificial Intelligence (AAAI) .",
            "url": "https://joatom.github.io/ai_curious/vision/classification/2021/05/28/hotel.html",
            "relUrl": "/vision/classification/2021/05/28/hotel.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Bending the space with nonlinearity",
            "content": "I’m trying out a new kaggle feature which allows us to include kaggle notebooks in our personal blogs. This articel was initialy published as kaggle notebook in November 2020. .",
            "url": "https://joatom.github.io/ai_curious/basics/2021/04/14/nonlinearity.html",
            "relUrl": "/basics/2021/04/14/nonlinearity.html",
            "date": " • Apr 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Sterbefälle nach Altersgruppen und Bundesländern in Deutschland von 2016 bis 2021",
            "content": ":de: :us: [kaggle version] . . Warning: (Update vom 22.11.2021) Vorweg ist hier eine persönliche Anmerkung, um Missverständnisse zu vermeiden: Ich bin geimpft! Die STIKO-Impfempfehlungen waren bislang immer schlüssig, daher ist auch meine älteste Tochter geimpft und sobald eine entsprechned Zulassung vorhanden ist, werden wir auch unsere jüngere Kinder impfen lassen. Nun, berücksichtige bei den nachstehenden Zahlen, dass diese aus dem Frühjahr 2021 stammen, also bevor die Delta-Variant in Deutschland weit verbreitet war und zu einer Zeit als Kinder mehr Einschränkungen hatten als Erwachsene. . In diesem Blog-Post möchte ich herausfinden, wie leicht sich interaktive Grafiken in Notebooks erstellen lassen. Als Framework verwende ich hier Altair, da es sich in meinem Blog leicht integrieren lässt [FP20]. . Um den Beitrag auch inhaltlich interessant zu gestallten, werte ich die Sterbefallzahlen von Deutschland der letzten fünf Jahre aus. Dabei Grenze ich die Zeiträume vor und während Covid19 voneinander ab. . Als Fragestellung definiere ich: . Wie verändern sich die Sterbefallzahlen je Altersgruppe und Monat? | Wie verändern sich die Sterbefallzahlen je Bundesland? | . Im ersten Teil wird die Datenherkunft und -verarbeitung beschrieben. Anschließend werden rund um die obigen Fragestellungen Grafiken aufgebaut. Zum Schluss folgt ein technisches Fazit zum Framework. . Important: Die Sterbefallzahlen sind kein Indikator für die Gefährlichkeit von Covid19. Sie bilden lediglich rückwirkend betrachtet die Anzahl verstorbener Menschen ab. . Datenherkunft . Für die Auswertung von Sterbefällen in Deutschland werden die aktuellen Sterbefalldaten vom Statistischen Bundesamt [SB21] herangezogen. Die Daten beinhalten unter anderem Aufstellungen der Todesfälle nach Altersgruppen oder Bundesländern. In dieser Analyse werden die monatlichen Sterbefallzahlen für den Zeitraum März 2016 bis Februar 2021 herangezogen. Die aktuellsten Daten liegen derzeit nur bis Februar 2021 vor und beinhalten einen Schätzanteil, der in der Datenquelle im Reiter &quot;Hinweise&quot; erklärt ist. Neben den Sterbefallzahlen werden zusätzlich Daten über die Bevölkerungsdichte der Bundesländer verarbeitet [SB20], wobei sich diese Zahlen auf den Stichtag 31.12.2019 beziehen. . Verarbeitung der Daten . Preprocessing . Der Auswertungszeitraum wird beschränkt auf März 2016 bis Februar 2021. Der Zeitraum, in dem Corona in Deutschland sehr aktive war, wird hier vereinfacht auf März 2020 (als Covid19 in Deutschland die ersten größeren gesellschaftlichen Veränderungen auslöste) bis Februar 2021 (orange) festgelegt. Der vor-Covid19-Zeitraum wird auf März 2016 bis Februar 2020 festgelegt (blau). Somit umfasst der Covid19-Zeitraum exakt ein Jahr und der vor-Covid19-Zeitraum exakt 4 Jahre. Somit bleiben beide Zeiträume ohne gravierende saisonale Abweichungen vergleichbar. Die Aufteilung der Zeiträume ist im nachstehenden Diagramm verdeutlicht. . Datenaggregation . Bei den Berechnungen werden die Werte über einen Zeitraum über das Arithmetische Mittel aggregiert. Je nach Auswertung geschieht dies über den ganzen Zeitraum oder je Monat. Das erste und dritte Quartil, der aggregierten Daten, werden gegebenenfalls als Schattierung in den Diagrammen mit abgebildet. In einigen Abbildung werden die berechneten Punkte interpoliert um die Lesbarkeit zu erhöhen. . . Tip: Bei interaktiven Grafiken befinden sich in der rechten oberen Ecke Steuerungselemente, wie Dropdown-Boxen. Mit dem Mausrad lässt sich zoomen und per Doppelklick lässt sich eine Grafik zurücksetzen. . Todesf&#228;lle nach Alter . Im nachstehenden Diagramm wird die durchschnittliche Anzahl an Todesfällen pro Monat je Altersgruppe abgebildet. Eine Altersgruppe umfasst fünf Jahre. Der Punkt Alter 55 umfasst z.B. alle Todesfälle im Alter zwischen 50 und 55. Todesfälle der über 100-jährigen werden im Punkt 100 abgebildet. . Die Werte sind jeweils über den vor-Covid19-Zeitraum (blau) und den Covid19-Zeitraum (orange) aggregiert. Im Diagramm lassen sich die Monate per Dropdown-Box auswählen. . Nachfolgend sind die durchschnittlichen Todesfälle nach Alter für jeden einzelnen Monat aufgelistet. Die Auflistung beginnt mit dem Monat März. . Beobachtung . In den Altersgruppen unter 55 Jahren kam es in diesen Zeiträumen zu keiner Übersterblichkeit. In den Altersgruppen ab 80 Jahren nahm die Übersterblichkeit massiv zu. . Todesf&#228;lle nach Bundesland . Im nächsten Diagramm lassen sich für die Altersgruppen 0-65 und 65+ die Todesfälle für die beiden Zeiträume je Bundesland auswerten. Per Dropdown-Box kann zwischen den Kennzahlen Todesfälle oder Todesfälle je 100.000 Einwohner gewählt werden. . Beobachtung . Nordrhein-Westfalen (NW) hat als Einwohner-stärkstes Land die meisten Todesfälle. In jedem Bundesland sind Anstiege der Todeszahlen im Covid19-Zeitraum zu erkennen. Allerdings sind lediglich minimale Anstiege in Hessen (HE) und Bayern (BY) für die Altersgruppen unter 65 Jahren zu erkennen. In den anderen Bundesländern gibt es keinen merklichen Anstieg in dieser Altersgruppe. . Bei der Darstellung der Todesfällen je Einwohner gibt es in der Altersgruppe unter 65 kleinere Schwankungen. Der Anstieg an Todesfällen in Nordrhein-Westfalen (NW) ist etwas geringer als in Bayern (BY) und in etwa so hoch wie in Baden-Württemberg (BW). Den stärksten Anstieg verzeichnet hier Sachsen (SN) und Brandenburg (BB). . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Todesf&#228;lle nach Bundesland und Bev&#246;lkerungsdichte . Das letzte Diagramm setzt die Bevölkerungsdichte der Bundesländer mit deren Todesfällen je 100.000 Einwohner in Bezug. . Beobachtung . Die Stadtstaaten Hamburg (HH) und Bremen (HB) weisen trotz ihrer hohen Einwohnerdichte einen nur moderaten Anstieg der Sterbefälle aus. Wohingegen Berlin einen hohen Anstieg verzeichnet. Schleswig-Holstein (SH) hat den geringsten Anstieg zu verzeichnen. . Der Korrelationskoeffizient zwischen der Bevölkerungsdichte und Anstieg der Todesfälle beträgt: . 0.0232 . Eine Korrelation ist für diesen Vergleich nicht festzustellen. . . Note: Hier werden nur Sterbefallzahlen betrachtet. Es werden keine anderen Aspekte (wie z.B. demografische Aspekte) berücksichtigt. . Technisches Fazit . Mit Altair lassen sich einfach Grafiken gestallten und interaktiv in Notebooks mit einbinden. Die Webseite beinhaltet eine Vielzahl von Beispielen [AA1]. Einige Fragestellungen werden auch im Github-Issue-Tracker [AA2] beantwortet. . Ich habe etwas Zeit mit JavaScript-Debugging verbracht, als ich die Bindings falsch verwendet habe. Es gab aber schlussendlich zu jedem Problem eine Lösung [AA2] oder Workarounds wie etwa bei der Legende im Bardiagramm [AG18]. Wie erwartet braucht es etwas Übung um ein neues Framework wie gewollt einsetzen zu können. . Die Integration [FP20] in diesen Blog funktioniert einwandfrei. Ich werde das Framework bestimmt auch an anderer Stelle verwenden. . Quellverzeichniss . Datenquellen . Die hier verwendeten Daten stammen vom Statistischen Bundesamtes und unterliegen der Lizenz &quot;dl-de/by-2-0&quot;. Der Lizenztext findet sich unter www.govdata.de/dl-de/by-2-0. Die Daten wurden zum Zwecke der Analyse ausschließlich innerhalb dieses Notebooks durch Ausführung des angegebenen Programmcodes modifiziert. . [SB21] Statistisches Bundesamt (Destatis), 2021 (erschienen am 30. März 2021), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, abgerufen 03.04.2021, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (erschienen am 2. September 2020), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, abgerufen am 03.04.2021, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Sonstige Quellen . A lot of the coding is derived from various examples of the Altair homepage and great examples in the coresponding github issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (erschienen am 06. Oktober 2018), Focus: generating an interactive legend in Altair, abgerufen 05.04.2021, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (erschienen am 20. Februar 2020), Fastpages Notebook Blog Post, abgerufen 05.04.2021, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany_de.html",
            "relUrl": "/eda/2021/04/06/mortality-germany_de.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Deaths by age group and states in Germany from 2016 to 2021",
            "content": ":de: :us: [kaggle version] . . Warning: (Updated on 2021/11/22) First of all here are some clarifying personal notes: I&#8217;m vaccinated! The guidelines of of the STIKO (official German board for vaccination recommendation) have always been plausible. Therefore my eldest daughter got vaccinated, too. And our younger children will also get their vaccination as soon as a recommendation exists. So, if you look at the numbers below, keep in mind, those were the numbers from Spring 2021, before Delta hit Germany and during a time where kids had more restrictions than adults. . In this blog post, I want to find out how easy it is to create interactive charts in notebooks. As a framework, I will use Altair, as it can be easily integrated into my blog [FP20]. . In order to make the contribution interesting, I evaluate the death rates from Germany over the last five years. I construct two periods - the first period before and the second one during Covid19. . We will analyze these questions: . How do deaths per age group and month change? | How do deaths per state change? | . The first part describes the origin and processing of the data. Then, charts are built around the above questions. Finally, a technical conclusion to the framework follows. . Important: The death rates are not an indicator of the danger of Covid19. They only reflect the number of people who died retrospectively. . Source of data . For the analysis of deaths in Germany, the current death data from the Statistisches Bundesamt (Federal Statistical Office) [SB21] are used. The data include the number of deaths by age group or state on a monthly basis. In this blog post, the monthly death figures for the period March 2016 to February 2021 are examined. The most recent data are currently only available until February 2021 and include some estimations, which are explained in the data source in the tab &quot;Hinweise&quot;. In addition to the death figures, data on the population density of the states are processed [SB20]. These figures refere to the reference date 31.12.2019. . Procession of the data . Preprocessing . The evaluation period is limited to March 2016 until February 2021. The period in which Corona was heavily active in Germany is simplified here to March 2020 (when Covid19 triggered the first major social changes in Germany) until February 2021 (orange). The pre-Covid19 period is set to March 2016 until February 2020 (blue). Thus, the Covid19 period covers exactly one year and the pre-Covid19 period exactly four years. Thus, both periods remain comparable without serious seasonal deviations. The split of the periods is illustrated in the diagram below. . Aggregation of data . In the calculations, the values are averaged over a period of time. Depending on the evaluation, this happens over the whole period or per month. The first and third quartile of the aggregated data may also be shown as shading in the diagrams. In some charts, the calculated points are interpolated to increase readability. . . Tip: For some of the charts there are control elements in the upper right corner, such as drop-down boxes. The mouse wheel can be used to zoom and a chart can be reset by double-clicking. . Deaths by age . The diagram below shows the average number of deaths per month per age group. One age group covers five years. The point age 55 includes, for example, all deaths between 50 and 55. Deaths of those over 100 years of age are shown in point 100. . The values are aggregated over the pre-Covid19 period (blue) and the Covid19 period (orange). In the diagram, the months can be selected via drop-down box. . Below are the average deaths by age for each month. The list starts with the month of March. . Observation . In the age groups under 55, there was no excess mortality during these periods. In the age groups from 80 years and older, mortality increased massively. . Deaths by state . In the next diagram, the age groups 0-65 and 65+ can be evaluated for the two periods grouped by state. By drop-down box, you can choose between the key figures Deaths or Deaths per 100,000 inhabitants. . Observation . North Rhine-Westphalia (NW) has the largest number of deaths, since it is the state with highest population. In each state, there are increases in death rates in the Covid19 period. However, only minimal increases in Hesse (HE) and Bavaria (BY) can be seen for the age groups under the age of 65. In the other states, there is no noticeable increase in this age group. . There are smaller variations looking at the number of deaths per 100.000 inhabitants in the age group below 65. The increase in deaths in North Rhine-Westphalia (NW) is slightly lower than in Bavaria (BY) and roughly as high as in Baden-Württemberg (BW). Saxony (SN) and Brandenburg (BB) have the strongest increases. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Deaths by state and population density . The last diagram refers to the population density of the states and the amount of deaths per 100,000 inhabitants. . Observation . The city-states Hamburg (HH) and Bremen (HB) show a moderate increase in deaths despite their high population density. Berlin (BE) instead has a high increase in death rate. Schleswig-Holstein (SH) recorded the lowest increase. . The correlation coefficient of population density and increase in mortality is: . 0.0232 . Hence, There is no correlation for this comparison. . Note: Only deaths are evaluated. No other aspects (such as demographic aspects) are taken into account. . Technical conclusion . With Altair you can easily setup interactive charts and integrate them into notebooks. Their website contains a variety of examples [AA1]. Some questions are also answered in their Github Issue tracker [AA2]. . I spent some time debugging JavaScript when I used the bindings incorrectly. However, there was finally a solution to every problem [AA2] or workarounds such as for the legend in the bar diagram [AG18]. As expected, a bit of practice is neccessary to get used to a new framework. . The integration [FP20] in this blog works perfectly. I will certainly use the framework elsewhere. . References . Data sources . The data used here are from the &quot;Statistisches Bundesamt&quot; (Federal Statistical Office) and are subject to the license &quot;dl-de/by-2-0&quot;. The license text can be found at www.govdata.de/dl-de/by-2-0. The data were modified exclusively within this notebook by executing the specified program code for the purpose of analysis. . [SB21] Statistisches Bundesamt (Destatis), 2021 (published 2012/03/30), Sterbefälle - Fallzahlen nach Tagen, Wochen, Monaten, Altersgruppen, Geschlecht und Bundesländern für Deutschland 2016 - 2021, visited 2021/04/03, https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Sterbefaelle-Lebenserwartung/Tabellen/sonderauswertung-sterbefaelle.xlsx?__blob=publicationFile | [SB20] Statistisches Bundesamt (Destatis), 2020 (published 2020/09/02), Bundesländer mit Hauptstädten nach Fläche, Bevölkerung und Bevölkerungsdichte am 31.12.2019, visited 2021/04/03, https://www.destatis.de/DE/Themen/Laender-Regionen/Regionales/Gemeindeverzeichnis/Administrativ/02-bundeslaender.xlsx?__blob=publicationFile | . Other references . A lot of the coding is derived from various examples of the Altair homepage and from great examples in the coresponding Github Issue tracker answered by https://github.com/jakevdp. . [AA1] https://altair-viz.github.io/gallery/index.html | [AA2] https://github.com/altair-viz/altair/issues/ | [AG18] A. Gordon, 2018 (published 2018/10/06), Focus: generating an interactive legend in Altair, visited 2021/04/05, https://medium.com/dataexplorations/focus-generating-an-interactive-legend-in-altair-9a92b5714c55 | [FP20] fastpages.fast.ai, 2020 (published 2020/02/20), Fastpages Notebook Blog Post, visited 2021/04/05, https://fastpages.fast.ai/jupyter/2020/02/20/test.html | .",
            "url": "https://joatom.github.io/ai_curious/eda/2021/04/06/mortality-germany.html",
            "relUrl": "/eda/2021/04/06/mortality-germany.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Automatically translate blog posts",
            "content": ":de: :us: . Attention! This text has been automatically translated! . Since I made so many mistakes in my first Blog post, I write this post in German and have it automatically translated. . For translation I use the popular NLP framework of huggingface.co. On their website is a simple example to implement a translation application and I will use it. . As expected, the Markdown syntax does not immediately work correctly when translating. So I had to make some adjustments at the beginning and afterwards. . The code (including pre- and post-processing) I used for the translation of the markdown files can be found here. But since it’s just a few lines of code, we can also look at it here: . from transformers import MarianMTModel, MarianTokenizer # load pretrained model and tokenizer model_name = &#39;Helsinki-NLP/opus-mt-de-en&#39; tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) # load german block post f_in = open(&quot;blog_translator_de.md&quot;, &quot;r&quot;) src_text = f_in.readlines() f_in.close() # preprocessing ## line break ( n) results to &quot;I don&#39;t know.&quot; We make it more specific: src_text = [s.replace(&#39; n&#39;,&#39; &#39;) for s in src_text] ## remove code block code = [] inside_code_block = False for i, line in enumerate(src_text): if line.startswith(&#39;&#39;) and not inside_code_block: # entering codeblock inside_code_block = True code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and not line.startswith(&#39;&#39;): code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and line.startswith(&#39;&#39;): # leaving code block code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; inside_code_block = False # translate translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=&quot;pt&quot;)) tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] # postprocessing ## replace code_blog tags with code for i, line in enumerate(tgt_text): if line == &#39;&lt;&lt;code_block&gt;&gt;&#39;: tgt_text[i] = code.pop(0) ## remove the eol (but keep empty list entries / lines) tgt_text = [s.replace(&#39;&#39;, &#39;&#39;,) for s in tgt_text] ## remove space between ]( to get the md link syntax right tgt_text = [s.replace(&#39;](&#39;, &#39;](&#39;,) for s in tgt_text] # write english blog post with open(&#39;2020-12-26-blog-translator.md&#39;, &#39;w&#39;) as f_out: for line in tgt_text: f_out.write(&quot;%s n&quot; % line) f_out.close() . Since this is my first NLP application, I left it with this Hello World code. Surely there are clever ways to map the markdown syntax in tokenizer. Maybe I’ll write a follow up when I find out. . By the way, the translation just made me adapt my German writing style. For example, sarcasm doesn’t work so well after translation, so I avoided it. Also, it often depends on the correct choice of words (e.g. there is no markdown command, but there is markdown syntax). «eol&gt; . Best regards . Johannes &amp; the Robot .",
            "url": "https://joatom.github.io/ai_curious/nlp/2020/12/26/blog-translator.html",
            "relUrl": "/nlp/2020/12/26/blog-translator.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Blog-Beiträge automatisch übersetzen",
            "content": ":de: :us: . Achtung! Dieser Text wurde automatisch übersetzt! . Da ich in meinem ersten Blog-Beitrag soviele Fehler auf Englisch gemacht habe, schreibe ich diesen Beitrag auf Deutsch und lasse ihn automatisch übersetzen. . Zur Übersetzung verwende ich das populäre NLP-Framework von huggingface.co. Auf ihrer Webseite ist ein einfaches Beispiel um eine Übersetzungsanwendung zu implementieren. Und diese werde ich verwenden. . Wie erwartet, hat es nicht auf Anhieb funktioniert die Markdown-Syntax bei der Übersetzung immer korrekt zu übernehmen. Also habe ich zu Beginn und hinterher noch ein paar Anpassungen vornehmen müssen. . Den Code (inkl. Vor- und Nachbearbeitung), den ich für die Übersetzung der Markdown-Dateien verwendet habe, findet ihr hier. Aber da es ja nur ein paar Zeilen Code sind, können wir sie uns auch eben hier anschauen: . from transformers import MarianMTModel, MarianTokenizer # load pretrained model and tokenizer model_name = &#39;Helsinki-NLP/opus-mt-de-en&#39; tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name) # load german block post f_in = open(&quot;blog_translator_de.md&quot;, &quot;r&quot;) src_text = f_in.readlines() f_in.close() # preprocessing ## line break ( n) results to &quot;I don&#39;t know.&quot; We make it more specific: src_text = [s.replace(&#39; n&#39;,&#39; &lt;&lt;eol&gt;&gt;&#39;) for s in src_text] ## remove code block code = [] inside_code_block = False for i, line in enumerate(src_text): if line.startswith(&#39;&#39;) and not inside_code_block: # entering codeblock inside_code_block = True code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and not line.startswith(&#39;&#39;): code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; elif inside_code_block and line.startswith(&#39;&#39;): # leaving code block code += [line] src_text[i] = &#39;&lt;&lt;code_block&gt;&gt;&#39; inside_code_block = False # translate translated = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=&quot;pt&quot;)) tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated] # postprocessing ## replace code_blog tags with code for i, line in enumerate(tgt_text): if line == &#39;&lt;&lt;code_block&gt;&gt;&#39;: tgt_text[i] = code.pop(0) ## remove the eol (but keep empty list entries / lines) tgt_text = [s.replace(&#39;&lt;&lt;eol&gt;&gt;&#39;, &#39;&#39;,) for s in tgt_text] ## remove space between ] ( to get the md link syntax right tgt_text = [s.replace(&#39;] (&#39;, &#39;](&#39;,) for s in tgt_text] # write english blog post with open(&#39;2020-12-26-blog-translator.md&#39;, &#39;w&#39;) as f_out: for line in tgt_text: f_out.write(&quot;%s n&quot; % line) f_out.close() . Da das meine erster NLP-Anwendung ist, habe ich es bei diesem Hello World-Code belassen. Sicherlich gibt es geschickter Wege, um die Markdown-Syntax in tokenizer abzubilden. Vielleicht schreibe ich ein Follow up, wenn ich es rausgefunden habe. . Übrigens, die Übersetzung hat mich ebenfallls dazu gebracht meinen deutschen Schreibstil anzupassen. Beispielsweise funktioniert Sarkasmus nach der Übersetzung nicht so gut, also hab ich es vermieden. Außerdem kommt es auch häufig auf die richtige Wortwahl an (z.B. gibt es kein Markdown-Befehl, jedoch gibt es Markdown-Syntax). . Viele Grüße . Johannes &amp; der Roboter .",
            "url": "https://joatom.github.io/ai_curious/nlp/2020/12/26/blog-translator-de.html",
            "relUrl": "/nlp/2020/12/26/blog-translator-de.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "A handful of bricks - from SQL to Pandas",
            "content": "Original article published at datamuni.com. . SQL vs. and Pandas . I love SQL. It’s been around for decades to arrange and analyse data. Data is kept in tables which are stored in a relational structure. Consistancy and data integraty is kept in mind when designing a relational data model. However, when it comes to machine learning other data structures such as matrices and tensors become important to feat the underlying algorithms and make data processing more efficient. That’s where Pandas steps in. From a SQL developer perspective it is the library to close the gap between your data storage and the ml frameworks. . This blog post shows how to translate some common and some advanced techniques from SQL to Pandas step-by-step. I didn’t just want to write a plain cheat sheet (actually Pandas has a good one to get started: Comparison SQL (Ref. 1)). Rather I want to unwind some concepts that might be helpful for a SQL developer who now and then deals with Pandas. . The coding examples are built upon a Lego Dataset (Ref. 2), that contains a couple of tables with data about various lego sets. . To follow along I’ve provided a notebook (Res. 1) on kaggle, where you can play with the blog examples either using SQLite or Bigquery. You can also checkout a docker container (Res. 2) to play on your home machine. . Missing bricks . First listen to this imaginary dialogue that guides us throug the coding: . :hatched_chick: I miss all red bricks of the Lego Pizzeria. I definetly need a new one. . :penguin: Don’t worry. We can try to solve this with data. That will be fun. :-) . :hatched_chick: (!@#%&amp;) You’re kidding, right? . Now that we have a mission we are ready to code and figuere out how to deal with missing bricks. First we inspect the tables. They are organized as shown in the relational diagram (Fig. 1). . . Fig. 1: Data model (source: Lego dataset (Ref. 2)) . There are colors, parts, sets and inventories. We should start by searching for the Pizzeria in the sets table using the set number (41311). . . Fig. 2: Lego Box with set number . A simple Filter (The behaviour of brackets.) . A simple like-filter on the sets table will return the set info. . SELECT * FROM sets s WHERE s.set_num like &#39;41311%&#39; . There are several ways to apply a filter in Pandas. The most SQL-like code utilizes the query-function which basicaly substitutes the where clause. . df_sets.query(&quot;set_num.str.contains(&#39;41311&#39;)&quot;, engine=&#39;python&#39;) .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Since the query function expects a String as input parameter we loose syntax highlighting and syntax checking for our filter expression. . Therefore a more commonly used expression consists of the bracket notation (The behaviour of the bracket notation of a class in python is implementated in the class function __getitem__) . See how we can apply an equals filter using brackets. . df_sets.query(&quot;set_num == &#39;41311-1&#39;&quot;) # or df_sets[df_sets.set_num == &#39;41311-1&#39;] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] .   set_num name year theme_id num_parts . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . There is a lot going on in this expression. . Let’s take it apart. . df_sets[&#39;set_num&#39;] returns a single column (a Pandas.Series object). A Pandas DataFrame is basically a collection of Series. Additionaly there is a row index (often just called index) and a column index (columnnames). Think of a column store database. . . Fig. 3: Elements of a DataFrame . Applying a boolean condition (== &#39;41311-1&#39;) to a Series of the DataFrame (df_sets[&#39;set_num&#39;]) will result in a boolean collection of the size of the column. . bool_coll = df_sets[&#39;set_num&#39;] == &#39;41311-1&#39; # only look at the position 3580 - 3590 of the collection bool_coll[3580:3590] . 3580 False 3581 False 3582 True 3583 False 3584 False 3585 False 3586 False 3587 False 3588 False 3589 False Name: set_num, dtype: bool . The boolean collection now gets past to the DataFrame and filters the rows: . df_sets[bool_coll] # or df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] . Depending on what type of object we pass to the square brackets the outcome result in very different behaviors. . We have already seen in the example above that if we pass a boolean collection with the size of number of rows, the collection is been handled as a row filter. . But if we pass a column name or a list of column names to the brackets instead, the given columns are selected like in the SELECT clause of an SQL statement. . SELECT name, year FROM lego.sets; . =&gt; . df_sets[[&#39;name&#39;, &#39;year&#39;]] . Row filter and column selection can be combined like this: . SELECT s.name, s.year FROM lego.sets s WHERE s.set_num = &#39;41311-1&#39;; . =&gt; . df_temp = df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;] df_temp[[&#39;name&#39;,&#39;year&#39;]] # or simply: df_sets[df_sets[&#39;set_num&#39;] == &#39;41311-1&#39;][[&#39;name&#39;,&#39;year&#39;]] .   name year . 3582 | Heartlake Pizzeria | 2017 | . Indexing (What actually is an index?) . Another way to access a row in Pandas is by using the row index. With the loc function (and brackets) we select the Pizzeria and another arbitrary set. We use the row numbers to filter the rows. . df_sets.loc[[236, 3582]] .   set_num name year theme_id num_parts . 236 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 3582 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . If we inspect the DataFrame closely we realize that it doesn’t realy look like a simple table but rather like a cross table. . The first column on the left is a row index and the table header is the column index. In the center the values of the columns are displayed (see Fig. 3). . If we think of the values as a matrix the rows are dimension 0 and columns are dimension 1. The dimension is often used in DataFrame functions as axis parameter. E.g. dropping columns can be done using dimensional information: . -- EXCEPT in SELECT clause only works with BIGQUERY SELECT s.* EXCEPT s.year FROM lego.sets s; . ==&gt; . df_sets.drop([&#39;year&#39;], axis = 1).head(5) .   set_num name theme_id num_parts . 0 | 00-1 | Weetabix Castle | 414 | 471 | . 1 | 0011-2 | Town Mini-Figures | 84 | 12 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 199 | 2 | . 3 | 0012-1 | Space Mini-Figures | 143 | 12 | . 4 | 0013-1 | Space Mini-Figures | 143 | 12 | . The indexes can be accessed with the index and columns variable. axes contains both. . df_sets.index # row index df_sets.columns # column index df_sets.axes # both . [RangeIndex(start=0, stop=11673, step=1), Index([&#39;set_num&#39;, &#39;name&#39;, &#39;year&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;], dtype=&#39;object&#39;)] . The row index doesn’t necessarely be the row number. We can also convert a column into a row index. . df_sets.set_index(&#39;set_num&#39;).head() .   name year theme_id num_parts . set_num |   |   |   |   | . 00-1 | Weetabix Castle | 1970 | 414 | 471 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | . It is also possible to define hierarchicle indicies for multi dimensional representation. . df_sets.set_index([&#39;year&#39;, &#39;set_num&#39;]).sort_index(axis=0).head() # axis = 0 =&gt; row index .     name theme_id num_parts . year | set_num |   |   |   | . 150 | 700.1.1-1 | Individual 2 x 4 Bricks | 371 | 10 | .   | 700.1.2-1 | Individual 2 x 2 Bricks | 371 | 9 | .   | 700.A-1 | Automatic Binding Bricks Small Brick Set (Lego… | 366 | 24 | .   | 700.B.1-1 | Individual 1 x 4 x 2 Window (without glass) | 371 | 7 | .   | 700.B.2-1 | Individual 1 x 2 x 3 Window (without glass) | 371 | 7 | . Sometimes it is usefull to reset the index, hence reset the row numbers. . df_sets.loc[[236, 3582]].reset_index(drop = True) # set drop = False to keep the old index as new column .   set_num name year theme_id num_parts . 0 | 10255-1 | Assembly Square | 2017 | 155 | 4009 | . 1 | 41311-1 | Heartlake Pizzeria | 2017 | 494 | 287 | . Now we get a sence what is meant by an index in Pandas in contrast to SQL. . Indices in SQL are hidden data structures (in form of e.g. b-trees or hash-tables). They are built to access data more quickly, to avoid full table scans when appropriate or to mantain consistancies when used with constraints. . An index in Pandas can rather be seen as a dimensional access to the data values. They can be distingueshed between row and column indices. . Joins (Why merge doesn’t mean upsert.) . :hatched_chick: What are we gonna do now about my missing parts? . :penguin: We don’t have all the information we need, yet. We need to join the other tables. . Though there is a function called join to join DataFrames I always use the merge function. This can be a bit confusing, when you are used to Oracle where merge means upsert/updelete rather then combining two tables. . When combining two DataFrames with the merge function in Pandas we have to define the relationship more explicitly. If you are used to SQL thats what you want. . In contrast the join function implicitly combines the DataFrames by their index or column names. It also enables multiply DataFrame joins in one statement as long as the join columns are matchable by name. . SELECT * FROM sets s INNER JOIN inventories i ON s.set_num = i.set_num -- USING (set_num) LIMIT 5 . set_num name year theme_id num_parts id version set_num_1 . 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | 00-1 | . 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | 0011-2 | . 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | 0011-3 | . 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | 0012-1 | . 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | 0013-1 | . These Pandas statements all do the same: . df_sets.merge(df_inventories, how = &#39;inner&#39;, on = &#39;set_num&#39;).head(5) #or if columns are matching df_sets.merge(df_inventories, on = &#39;set_num&#39;).head(5) #or explicitly defined columns df_sets.merge(df_inventories, how = &#39;inner&#39;, left_on = &#39;set_num&#39;, right_on = &#39;set_num&#39;).head(5) .   set_num name year theme_id num_parts id version . 0 | 00-1 | Weetabix Castle | 1970 | 414 | 471 | 5574 | 1 | . 1 | 0011-2 | Town Mini-Figures | 1978 | 84 | 12 | 5087 | 1 | . 2 | 0011-3 | Castle 2 for 1 Bonus Offer | 1987 | 199 | 2 | 2216 | 1 | . 3 | 0012-1 | Space Mini-Figures | 1979 | 143 | 12 | 1414 | 1 | . 4 | 0013-1 | Space Mini-Figures | 1979 | 143 | 12 | 4609 | 1 | . To see witch parts are needed for the Pizzeria we combine some tables. We look for the inventory of the set and gather all parts. Then we get color and part category information. . We end up with an inventory list: . SELECT s.set_num, s.name set_name, p.part_num, p.name part_name, ip.quantity, c.name color, pc.name part_cat FROM sets s, inventories i, inventory_parts ip, parts p, colors c, part_categories pc WHERE s.set_num = i.set_num AND i.id = ip.inventory_id AND ip.part_num = p.part_num AND ip.color_id = c.id AND p.part_cat_id = pc.id AND s.set_num in (&#39;41311-1&#39;) AND i.version = 1 AND ip.is_spare = &#39;f&#39; ORDER BY p.name, s.set_num, c.name LIMIT 10 . set_num set_name part_num part_name quantity color part_cat . 41311-1 | Heartlake Pizzeria | 25269pr03 | 1/4 CIRCLE TILE 1X1 with Pizza Print | 4 | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 32807 | BRICK 1X1X1 1/3, W/ ARCH | 4 | Red | Other | . 41311-1 | Heartlake Pizzeria | 6190 | Bar 1 x 3 (Radio Handle, Phone Handset) | 1 | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 30374 | Bar 4L (Lightsaber Blade / Wand) | 1 | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 99207 | Bracket 1 x 2 - 2 x 2 Inverted | 1 | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 2453b | Brick 1 x 1 x 5 with Solid Stud | 4 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 4 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 3 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 3004 | Brick 1 x 2 | 1 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 3245b | Brick 1 x 2 x 2 with Inside Axle Holder | 2 | White | Bricks | . Lets create a general inventory_list as view and make the statement more readable and comparable with ANSI-syntax (separating filters/predicates from join conditions). . DROP VIEW IF EXISTS inventory_list; CREATE VIEW inventory_list AS SELECT s.set_num, s.name set_name, s.theme_id, s.num_parts, p.part_num, ip.quantity, p.name part_name, c.name color, pc.name part_cat FROM sets s INNER JOIN inventories i USING (set_num) INNER JOIN inventory_parts ip ON (i.id = ip.inventory_id) INNER JOIN parts p USING (part_num) INNER JOIN colors c ON (ip.color_id = c.id) INNER JOIN part_categories pc ON (p.part_cat_id = pc.id) WHERE i.version = 1 AND ip.is_spare = &#39;f&#39;; . Now we translate the view to Pandas. We see how the structure relates to sql. The parameter how defines the type of join (here: inner), on represents the USING clause whereas left_on and right_on stand for the SQL ON condition. . In SQL usually an optimizer defines based in rules or statistics the execution plan (the order in which the tables are accessed, combined and filtered). I’m not sure if Pandas follows a similar approache. To be safe, I assume the order and early column dropping might matter for performance and memory management. . df_inventory_list = df_sets[[&#39;set_num&#39;, &#39;name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;]] .merge( df_inventories[df_inventories[&#39;version&#39;] == 1][[&#39;id&#39;, &#39;set_num&#39;]], how = &#39;inner&#39;, on = &#39;set_num&#39; ) .merge( df_inventory_parts[df_inventory_parts[&#39;is_spare&#39;] == &#39;f&#39;][[&#39;inventory_id&#39;, &#39;part_num&#39;, &#39;color_id&#39;, &#39;quantity&#39;]], how = &#39;inner&#39;, left_on = &#39;id&#39;, right_on = &#39;inventory_id&#39; ) .merge( df_parts[[&#39;part_num&#39;, &#39;name&#39;, &#39;part_cat_id&#39;]], how = &#39;inner&#39;, on = &#39;part_num&#39; ) .merge( df_colors[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;color_id&#39;, right_on = &#39;id&#39; ) .merge( df_part_categories[[&#39;id&#39;, &#39;name&#39;]], how = &#39;inner&#39;, left_on = &#39;part_cat_id&#39;, right_on = &#39;id&#39; ) # remove some columns and use index as row number (reset_index) df_inventory_list = df_inventory_list.drop([&#39;id_x&#39;, &#39;inventory_id&#39;, &#39;color_id&#39;, &#39;part_cat_id&#39;, &#39;id_y&#39;, &#39;id&#39;], axis = 1).reset_index(drop = True) # rename columns df_inventory_list.columns = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;theme_id&#39;, &#39;num_parts&#39;, &#39;part_num&#39;, &#39;quantity&#39;, &#39;part_name&#39;, &#39;color&#39;, &#39;part_cat&#39;] . Lots of code here. So we better check if our Pandas code matches the results of our SQL code. . Select the inventory list for our example, write it to a DataFrame (df_test_from_sql) and compare the results. . df_test_from_sql &lt;&lt; SELECT il.* FROM inventory_list il WHERE il.set_num in (&#39;41311-1&#39;) ORDER BY il.part_name, il.set_num, il.color LIMIT 10; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # test df_test_from_df = df_inventory_list[df_inventory_list[&#39;set_num&#39;].isin([&#39;41311-1&#39;])].sort_values(by=[&#39;part_name&#39;, &#39;set_num&#39;, &#39;color&#39;]).head(10) df_test_from_df .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 507161 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 25269pr03 | 4 | 1/4 CIRCLE TILE 1X1 with Pizza Print | Tan | Tiles Printed | . 543292 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 32807 | 4 | BRICK 1X1X1 1/3, W/ ARCH | Red | Other | . 266022 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 6190 | 1 | Bar 1 x 3 (Radio Handle, Phone Handset) | Red | Bars, Ladders and Fences | . 273113 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 30374 | 1 | Bar 4L (Lightsaber Blade / Wand) | Light Bluish Gray | Bars, Ladders and Fences | . 306863 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 99207 | 1 | Bracket 1 x 2 - 2 x 2 Inverted | Black | Plates Special | . 47206 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 2453b | 4 | Brick 1 x 1 x 5 with Solid Stud | Tan | Bricks | . 50211 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 4 | Brick 1 x 2 | Light Bluish Gray | Bricks | . 45716 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 3 | Brick 1 x 2 | Tan | Bricks | . 16485 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3004 | 1 | Brick 1 x 2 | White | Bricks | . 22890 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3245b | 2 | Brick 1 x 2 x 2 with Inside Axle Holder | White | Bricks | . # assert equals if not USE_BIGQUERY: df_test_from_sql = df_test_from_sql.DataFrame() pd._testing.assert_frame_equal(df_test_from_sql, df_test_from_df.reset_index(drop = True)) . The results are equal as expected. . Since we are only interested in the red bricks we create a list of those missing parts. . SELECT * FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND part_cat like &#39;%Brick%&#39; AND color = &#39;Red&#39; ORDER BY il.color, il.part_name, il.set_num; . set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . We need to watchout for the brackets when combining filters in DataFrames. . df_missing_parts = df_inventory_list[(df_inventory_list[&#39;set_num&#39;] == &#39;41311-1&#39;) &amp; (df_inventory_list[&#39;part_cat&#39;].str.contains(&quot;Brick&quot;)) &amp; (df_inventory_list[&#39;color&#39;] == &#39;Red&#39;) ].sort_values(by=[&#39;color&#39;, &#39;part_name&#39;, &#39;set_num&#39;]).reset_index(drop = True) df_missing_parts .   set_num set_name theme_id num_parts part_num quantity part_name color part_cat . 0 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3039 | 1 | Slope 45° 2 x 2 | Red | Bricks Sloped | . 1 | 41311-1 | Heartlake Pizzeria | 494 | 287 | 3045 | 2 | Slope 45° 2 x 2 Double Convex | Red | Bricks Sloped | . :penguin: There we go, we are missing one 2x2 brick and tw0 2x2 double convex. . :hatched_chick: Yup, that’s the roof of the fireplace. I knew that before. . Conditional Joins and Aggregation (Almost done!) . Next we search for sets that contain the missing parts. The quantity of the parts in the found sets must be greater or equal the quantity of the missing parts. . In SQL it is done with an conditional join il.quantity &gt;= mp.quantity. . sets_with_missing_parts &lt;&lt; -- A list of missing parts WITH missing_parts AS ( SELECT il.set_name, il.part_num, il.part_name, il.color, il.quantity FROM inventory_list il WHERE il.set_num = &#39;41311-1&#39; AND il.part_cat like &#39;%Brick%&#39; AND il.color = &#39;Red&#39; ) -- Looking for set that contains as much of the missing parts as needed SELECT mp.set_name as searching_for_set, il.set_num, il.set_name, il.part_name, -- total number of parts per set il.num_parts, -- how many of the missing parts were found in the set COUNT(*) OVER (PARTITION BY il.set_num) AS matches_per_set FROM inventory_list il INNER JOIN missing_parts mp ON (il.part_num = mp.part_num AND il.color = mp.color -- searching for a set that contains at least as much parts as there are missing AND il.quantity &gt;= mp.quantity ) -- don&#39;t search in the Pizzeria set WHERE il.set_num &lt;&gt; &#39;41311-1&#39; -- prioritize sets with all the missing parts and as few parts as possible ORDER BY matches_per_set DESC, il.num_parts, il.set_num, il.part_name LIMIT 16 . Conditional Join . There is no intuitive way to do a conditional join on DataFrames. The easiest I’ve seen so far is a two step solution. As substitution for the SQL WITH-clause we can reuse df_missing_parts. . # 1. merge on the equal conditions df_sets_with_missing_parts = df_inventory_list.merge(df_missing_parts, how = &#39;inner&#39;, on = [&#39;part_num&#39;, &#39;color&#39;], suffixes = (&#39;_found&#39;, &#39;_missing&#39;)) # 2. apply filter for the qreater equals condition df_sets_with_missing_parts = df_sets_with_missing_parts[df_sets_with_missing_parts[&#39;quantity_found&#39;] &gt;= df_sets_with_missing_parts[&#39;quantity_missing&#39;]] # select columns cols = [&#39;set_num&#39;, &#39;set_name&#39;, &#39;part_name&#39;, &#39;num_parts&#39;] df_sets_with_missing_parts = df_sets_with_missing_parts[[&#39;set_name_missing&#39;] + [c + &#39;_found&#39; for c in cols]] df_sets_with_missing_parts.columns = [&#39;searching_for_set&#39;] + cols . Aggregation . In the next step the aggregation of the analytic function . COUNT(*) OVER (PARTITION BY il.set_num) matches_per_set . needs to be calculated. Hence the number of not-NaN values will be counted per SET_NUM group and assigned to each row in a new column (matches_per_set). . But before translating the analytic function, let’s have a look at a regular aggregation, first. Say, we simply want to count the entries per set_num on group level (without assigning the results back to the original group entries) and also sum up all parts of a group. Then the SQL would look something like this: . SELECT s.set_num, COUNT(*) AS matches_per_set SUM(s.num_parts) AS total_num_parts FROM ... WHERE ... GROUP BY s.set_num; . All selected columns must either be aggregated by a function (COUNT, SUM) or defined as a group (GROUP BY). The result is a two column list with the group set_num and the aggregations matches_per_set and total_num_part. . Now see how the counting is done with Pandas. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).count() .sort_values(&#39;set_num&#39;, ascending = False) # for sum and count: # df_sets_with_missing_parts.groupby([&#39;set_num&#39;]).agg([&#39;count&#39;, &#39;sum&#39;]) .   searching_for_set set_name part_name num_parts . set_num |   |   |   |   | . llca8-1 | 1 | 1 | 1 | 1 | . llca21-1 | 1 | 1 | 1 | 1 | . fruit1-1 | 1 | 1 | 1 | 1 | . MMMB026-1 | 1 | 1 | 1 | 1 | . MMMB003-1 | 1 | 1 | 1 | 1 | . … | … | … | … | … | . 10021-1 | 1 | 1 | 1 | 1 | . 088-1 | 1 | 1 | 1 | 1 | . 080-1 | 2 | 2 | 2 | 2 | . 066-1 | 1 | 1 | 1 | 1 | . 00-4 | 1 | 1 | 1 | 1 | . Wow, that’s different! The aggregation function is applied to every column independently and the group is set as row index. But it is also possible to define the aggregation function for each column explicitly like in SQL: . df_sets_with_missing_parts.groupby([&#39;set_num&#39;], as_index = False) .agg(matches_per_set = pd.NamedAgg(column = &quot;set_num&quot;, aggfunc = &quot;count&quot;), total_num_parts = pd.NamedAgg(column = &quot;num_parts&quot;, aggfunc = &quot;sum&quot;)) .   set_num matches_per_set total_num_parts . 0 | 00-4 | 1 | 126 | . 1 | 066-1 | 1 | 407 | . 2 | 080-1 | 2 | 1420 | . 3 | 088-1 | 1 | 615 | . 4 | 10021-1 | 1 | 974 | . … | … | … | … | . 463 | MMMB003-1 | 1 | 15 | . 464 | MMMB026-1 | 1 | 43 | . 465 | fruit1-1 | 1 | 8 | . 466 | llca21-1 | 1 | 42 | . 467 | llca8-1 | 1 | 58 | . This looks more familiar. With the as_index argument the group becomes a column (rather than a row index). . So, now we return to our initial task translating the COUNT(*) OVER(PARTITION BY) clause. One approach could be to join the results of the above aggregated DataFrame with the origanal DataFrame, like . df_sets_with_missing_parts.merge(my_agg_df, on = &#39;set_num&#39;) . A more common why is to use the transform() function: . # add aggregatiom df_sets_with_missing_parts[&#39;matches_per_set&#39;] = df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;].transform(&#39;count&#39;) df_sets_with_missing_parts.head(5) .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 00-4 | Weetabix Promotional Windmill | Slope 45° 2 x 2 | 126 | 1 | . 1 | Heartlake Pizzeria | 066-1 | Basic Building Set | Slope 45° 2 x 2 | 407 | 1 | . 2 | Heartlake Pizzeria | 080-1 | Basic Building Set with Train | Slope 45° 2 x 2 | 710 | 2 | . 3 | Heartlake Pizzeria | 088-1 | Super Set | Slope 45° 2 x 2 | 615 | 1 | . 4 | Heartlake Pizzeria | 10021-1 | U.S.S. Constellation | Slope 45° 2 x 2 | 974 | 1 | . Let’s elaborate the magic that’s happening. . df_sets_with_missing_parts.groupby([&#39;set_num&#39;])[&#39;part_name&#39;] . returns a GroupByDataFrame which contains the group names (from set_num) and all row/column indicies and values related to the groups. Here only one column [&#39;part_name&#39;] is selected. In the next step transform applies the given function (count) to each column individually but only with the values in the current group. Finaly the results are assigned to each row in the group as shown in Fig. 4. . . Fig. 4: Aggregation with transform . Now that we have gathered all the data we arange the results so that they can be compared to the SQL data: . # sort and pick top 16 df_sets_with_missing_parts = df_sets_with_missing_parts.sort_values([&#39;matches_per_set&#39;, &#39;num_parts&#39;, &#39;set_num&#39;, &#39;part_name&#39;], ascending = [False, True, True, True]).reset_index(drop = True).head(16) df_sets_with_missing_parts .   searching_for_set set_num set_name part_name num_parts matches_per_set . 0 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 1 | Heartlake Pizzeria | 199-1 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 2 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 | 41 | 2 | . 3 | Heartlake Pizzeria | 212-2 | Scooter | Slope 45° 2 x 2 Double Convex | 41 | 2 | . 4 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 | 58 | 2 | . 5 | Heartlake Pizzeria | 838-1 | Red Roof Bricks Parts Pack, 45 Degree | Slope 45° 2 x 2 Double Convex | 58 | 2 | . 6 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 | 59 | 2 | . 7 | Heartlake Pizzeria | 5151-1 | Roof Bricks, Red, 45 Degrees | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 8 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 | 59 | 2 | . 9 | Heartlake Pizzeria | 811-1 | Red Roof Bricks, Steep Pitch | Slope 45° 2 x 2 Double Convex | 59 | 2 | . 10 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 | 60 | 2 | . 11 | Heartlake Pizzeria | 663-1 | Hovercraft | Slope 45° 2 x 2 Double Convex | 60 | 2 | . 12 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 | 76 | 2 | . 13 | Heartlake Pizzeria | 336-1 | Fire Engine | Slope 45° 2 x 2 Double Convex | 76 | 2 | . 14 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 | 92 | 2 | . 15 | Heartlake Pizzeria | 6896-1 | Celestial Forager | Slope 45° 2 x 2 Double Convex | 92 | 2 | . # assert equals if not USE_BIGQUERY: sets_with_missing_parts = sets_with_missing_parts.DataFrame() pd._testing.assert_frame_equal(sets_with_missing_parts, df_sets_with_missing_parts) . The results are matching! . :penguin: We got it. We can buy the small Fire Engine to fix the roof of the fireplace. Now need for a new Pizzeria. :-) . :hatched_chick: (#@§?!*#) Are you sure your data is usefull for anything? . Recursion (Lost in trees?) . We solved the red brick problem. But since we have the data already open, let’s have a closer look at the Fire Engine, set number 336-1. . SELECT s.name AS set_name, s.year, t.id, t.name AS theme_name, t.parent_id FROM sets s, themes t WHERE t.id = s.theme_id AND s.set_num = &#39;336-1&#39;; . set_name year id theme_name parent_id . Fire Engine | 1968 | 376 | Fire | 373.0 | . The fire engine is quiet old (from 1968) and it belongs to the theme Fire. The themes table also includes as column called parent_id. This suggests that themes is a hierarchical structure. We can check this with an recursive WITH-clause in SQL. (*BQ: recursive WITH is not implemented in BIGQUERY. . WITH RECURSIVE hier(name, parent_id, level) AS ( -- init recursion SELECT t.name, t.parent_id, 0 AS level FROM themes t WHERE id = 376 UNION ALL -- recursive call SELECT t.name, t.parent_id, h.level +1 FROM themes t, hier h WHERE t.id = h.parent_id ) SELECT COUNT(1) OVER() - level AS level, name as theme, GROUP_CONCAT(name,&#39; --&gt; &#39;) over(order by level desc) path FROM hier ORDER BY level; . level theme path . 1 | Classic | Classic | . 2 | Vehicle | Classic –&gt; Vehicle | . 3 | Fire | Classic –&gt; Vehicle –&gt; Fire | . OK, that looks like a reasonable hierarchie. The path column includes the parents and grant parents of a theme. What if we want to reverse the order of the path. Unfortunately GROUP_CONCAT in SQLite doesn’t allow us to specify a sort order in the aggregation. It’s possible to add custom aggregation function in some databases. In SQLite we can compile application defined function or in Oracle we can define customized aggregation function even at runtime as types. . Quiet some steps need to be taken to make the database use costumized aggregation efficently, so we can use them like regulare aggregation and windowing function. In Oracle for instance we have to define: . initial values: total := 0; n := 0; . | calculation per iteration step: total := total + this_step_value; n := n + 1; . | deletion per iteration for windowing: total := total - removed_step_value; n := n - 1; . | merging for parallel execution: total := total_worker1 + total_worker2; n := n_worker1 + n_worker2; . | termination: my_avg := total / nullif(n-1, 0) . | Now, how are we gonna do this in Pandas? We start by traversing the hierarchie. . fire_engine_info = df_themes[df_themes[&#39;id&#39;] == 376].copy() fire_engine_info[&#39;level&#39;] = 0 parent_id = fire_engine_info.parent_id.values[0] lvl = 0 while not np.isnan(parent_id) and lvl &lt; 10: lvl+= 1 new_info = df_themes[df_themes[&#39;id&#39;] == parent_id].copy() new_info[&#39;level&#39;] = lvl parent_id = new_info.parent_id.values[0] fire_engine_info = fire_engine_info.append(new_info) fire_engine_info[&#39;grp&#39;]=0 fire_engine_info .   id name parent_id level grp . 375 | 376 | Fire | 373.0 | 0 | 0 | . 372 | 373 | Vehicle | 365.0 | 1 | 0 | . 364 | 365 | Classic | NaN | 2 | 0 | . On the one hand this is pretty need, since we can do what ever we want in a manually coded loop. On the other hand I doubt that it is very efficent when we have to deal with lots of data. But to be fair, Recursive WITH isn’t that fast either in SQL. . Finaly we consider how to do customized aggregation. We could do it in the loop above or we can rather use the library’s transform or apply functions. . We define a custom aggregation function cat_sorted and then use the apply function like this: . def cat_sorted(ser, df, val_col = None, sort_col = None): u=&#39; --&gt; &#39;.join(df[df.id.isin(ser)].sort_values(sort_col)[val_col].values) return [u] . fire_engine_info.apply(lambda x: cat_sorted(x, fire_engine_info, &#39;name&#39;, &#39;level&#39;)) .   id name parent_id level grp . 0 | Fire –&gt; Vehicle –&gt; Classic |   | Vehicle –&gt; Classic |   |   | . We can also apply rolling or windowing behaviour. . Note that a rolling representation or windowing on string values is not possible because Pandas only allows numeric values for those action. . fire_engine_info.rolling(10,min_periods=1)[&#39;level&#39;].apply(lambda x: sum(10**x), raw = False) . 375 1.0 372 11.0 364 111.0 Name: level, dtype: float64 . Now, we not only understand the numbers on the lego package but also have a better understandig of Pandas. . Summary (Got it!) . SQL stays my favourite language to access structured data arranged over many tables. Pandas shines when data already is gathered together and easily accessable (e.g. as csv file). There are alternatives to Pandas to build ml pipelines, such as Dask or CUDF. But learning Pandas is a good foundation to learn more of them. . Resources . To play with the examples: . Res. 1 Kaggle notebook: https://www.kaggle.com/joatom/a-handful-of-bricks-from-sql-to-pandas | Res. 2 Docker container: https://github.com/joatom/blog-resources/tree/main/handful_bricks | . References . Ref. 1 Pandas SQL comparison: https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html | Ref. 2 The Lego dataset: https://www.kaggle.com/rtatman/lego-database | .",
            "url": "https://joatom.github.io/ai_curious/sql/pandas/bigquery/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "relUrl": "/sql/pandas/bigquery/2020/12/12/a-handful-of-bricks-from-sql-to-pandas.html",
            "date": " • Dec 12, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "BigQuery-Geotab Intersection Congestion",
            "content": "This blog post contains some of my codings for the 2019 kaggle BigQuery-Geotab competition. . My submission scored 1st Place in the categorie BigQuery ML Models built in SQL. . The challange was to predict six measures for cars approaching intersections in four US cities. . My objective in the competition was to tryout BigQuery (BQ) including the basic ML features. Therefore the notebooks rely as much as possible on BQ. All features are generated in BQ with varying SQL-techniques. The prediction model is also build in BQ. . Additional resources can be found in my github repo. .",
            "url": "https://joatom.github.io/ai_curious/sql/bigquery/2020/01/05/bgml-geotab.html",
            "relUrl": "/sql/bigquery/2020/01/05/bgml-geotab.html",
            "date": " • Jan 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there! . I’m Johannes and this is my hobby blog. The content is limited to topics I’m curious in my spare time. Don’t expect to find here anything related to my job. . I’m happy when you leave a comment on blog posts you like. . Cheers, . Johannes . . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joatom.github.io/ai_curious/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joatom.github.io/ai_curious/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}